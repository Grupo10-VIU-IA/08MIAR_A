{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: [Jose Aurelio Bollas Taboada]\n",
    "*   Alumno 2: [Antonio Jose Bonafede Salas]\n",
    "*   Alumno 3: [Elvis David Pachacama Cabezas]\n",
    "*   Alumno 4: [Jose Fernando Sarmiento Sarmiento]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto SpaceInvaders\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbVRjvHCJ8UF"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de acciones disponibles: 6\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "print(f\"Número de acciones disponibles: {nb_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtariProcessorCell"
   },
   "source": [
    "#### Procesador Atari\n",
    "\n",
    "El procesador Atari se encarga de:\n",
    "1. Preprocesar las observaciones (redimensionar a 84x84 y convertir a escala de grises)\n",
    "2. Normalizar el estado batch (dividir por 255)\n",
    "3. Clipear las recompensas entre -1 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"Procesa cada observación: redimensiona a 84x84 y convierte a escala de grises\"\"\"\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # L = luminance (escala de grises)\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"Normaliza el batch de estados dividiendo por 255\"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Clipea las recompensas entre -1 y 1 para estabilizar el entrenamiento\"\"\"\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "---\n",
    "### **1. Implementación de la red neuronal**\n",
    "\n",
    "Se implementa una red neuronal convolucional (CNN) basada en la arquitectura descrita en el paper de Mnih et al. (2015) \"Human-level control through deep reinforcement learning\".\n",
    "\n",
    "**Arquitectura:**\n",
    "- Capa Permute: Reorganiza las dimensiones de entrada según el formato de Keras\n",
    "- Conv2D #1: 32 filtros, kernel 8x8, stride 4 → Extrae características de bajo nivel\n",
    "- Conv2D #2: 64 filtros, kernel 4x4, stride 2 → Extrae características de nivel medio\n",
    "- Conv2D #3: 64 filtros, kernel 3x3, stride 1 → Refina características\n",
    "- Dense #1: 512 neuronas → Capa completamente conectada\n",
    "- Dense #2: nb_actions neuronas, activación lineal → Salida Q-values para cada acción\n",
    "\n",
    "**Justificación:**\n",
    "- Las capas convolucionales permiten extraer características espaciales relevantes del juego\n",
    "- Los strides progresivamente más pequeños permiten capturar detalles a diferentes escalas\n",
    "- La capa densa de 512 neuronas permite combinar las características extraídas\n",
    "- La activación lineal final es apropiada para estimar Q-values (pueden ser negativos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de datos de imagen de Keras: channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construcción del modelo CNN siguiendo la arquitectura de Mnih et al. (2015)\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "\n",
    "print(f\"Formato de datos de imagen de Keras: {K.image_data_format()}\")\n",
    "\n",
    "# Reorganizar dimensiones según el formato de Keras\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # Formato: (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # Formato: (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_data_format.')\n",
    "\n",
    "# Primera capa convolucional: detecta características básicas (bordes, colores)\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Segunda capa convolucional: detecta patrones más complejos\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Tercera capa convolucional: refina las características detectadas\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Aplanar las características para las capas densas\n",
    "model.add(Flatten())\n",
    "\n",
    "# Capa completamente conectada: combina características\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Capa de salida: un Q-value por cada acción posible\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))  # Lineal porque los Q-values pueden ser negativos\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "---\n",
    "### **2. Implementación de la solución DQN**\n",
    "\n",
    "Se implementan los componentes principales del algoritmo DQN:\n",
    "\n",
    "**Memoria de Experiencia (Experience Replay):**\n",
    "- Tamaño: 1,000,000 transiciones\n",
    "- Window length: 4 frames (captura movimiento)\n",
    "- Permite romper correlaciones temporales y reutilizar experiencias\n",
    "\n",
    "**Política de Exploración:**\n",
    "- Epsilon-greedy con decaimiento lineal\n",
    "- ε inicial: 1.0 (exploración total)\n",
    "- ε final entrenamiento: 0.1 (10% exploración)\n",
    "- ε test: 0.05 (5% exploración)\n",
    "- Pasos de decaimiento: 1,000,000 (decae gradualmente)\n",
    "\n",
    "**Configuración del Agente DQN:**\n",
    "- Warmup: 50,000 pasos (acumula experiencias antes de entrenar)\n",
    "- Gamma (γ): 0.99 (factor de descuento, valora recompensas futuras)\n",
    "- Target model update: 10,000 pasos (actualiza la red objetivo)\n",
    "- Train interval: 4 pasos (entrena cada 4 acciones)\n",
    "- Optimizer: Adam con learning rate 0.00025\n",
    "- Batch size: 32 (por defecto en keras-rl)\n",
    "\n",
    "**Justificación de hiperparámetros:**\n",
    "- Learning rate bajo (0.00025): previene oscilaciones en el aprendizaje\n",
    "- Warmup alto (50,000): asegura suficiente diversidad en la memoria inicial\n",
    "- Target update (10,000): balance entre estabilidad y adaptación\n",
    "- Train interval (4): reduce correlación sin perder mucha información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONFIGURACIÓN DEL AGENTE DQN ===\n",
      "Memoria: 1,000,000 transiciones\n",
      "Window length: 4 frames\n",
      "Política: Epsilon-greedy con decaimiento lineal\n",
      "  - ε inicial: 1.0\n",
      "  - ε final: 0.1\n",
      "  - ε test: 0.05\n",
      "Warmup: 50,000 pasos\n",
      "Gamma (γ): 0.99\n",
      "Target update: cada 10,000 pasos\n",
      "Train interval: cada 4 acciones\n",
      "Learning rate: 0.00025\n"
     ]
    }
   ],
   "source": [
    "# 1. MEMORIA DE EXPERIENCIA (Experience Replay)\n",
    "# Almacena transiciones (s, a, r, s') para romper correlaciones temporales\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "# 2. PROCESADOR\n",
    "# Preprocesa observaciones, estados y recompensas\n",
    "processor = AtariProcessor()\n",
    "\n",
    "# 3. POLÍTICA DE EXPLORACIÓN\n",
    "# Epsilon-greedy con decaimiento lineal: empieza explorando (ε=1.0) \n",
    "# y gradualmente explota (ε=0.1)\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=1.,      # ε inicial: exploración total\n",
    "    value_min=.1,      # ε final: 10% exploración, 90% explotación\n",
    "    value_test=.05,    # ε test: 5% exploración durante evaluación\n",
    "    nb_steps=1000000   # Pasos para decaer de value_max a value_min\n",
    ")\n",
    "\n",
    "# 4. AGENTE DQN\n",
    "# Configuración del algoritmo Deep Q-Network\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    processor=processor,\n",
    "    nb_steps_warmup=50000,      # Pasos de warmup antes de entrenar\n",
    "    gamma=.99,                  # Factor de descuento (importancia futuro)\n",
    "    target_model_update=10000,  # Frecuencia de actualización de red objetivo\n",
    "    train_interval=4,           # Entrena cada 4 acciones\n",
    "    delta_clip=1,               # Clip del error TD para estabilidad,\n",
    "    enable_double_dqn=True,\n",
    "    enable_dueling_network=False\n",
    ")\n",
    "\n",
    "# 5. COMPILACIÓN\n",
    "# Optimizer Adam con learning rate bajo para estabilidad\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n",
    "\n",
    "print(\"\\n=== CONFIGURACIÓN DEL AGENTE DQN ===\")\n",
    "print(f\"Memoria: {memory.limit:,} transiciones\")\n",
    "print(f\"Window length: {WINDOW_LENGTH} frames\")\n",
    "print(f\"Política: Epsilon-greedy con decaimiento lineal\")\n",
    "print(f\"  - ε inicial: {policy.value_max}\")\n",
    "print(f\"  - ε final: {policy.value_min}\")\n",
    "print(f\"  - ε test: {policy.value_test}\")\n",
    "print(f\"Warmup: {dqn.nb_steps_warmup:,} pasos\")\n",
    "print(f\"Gamma (γ): {dqn.gamma}\")\n",
    "print(f\"Target update: cada {dqn.target_model_update:,} pasos\")\n",
    "print(f\"Train interval: cada {dqn.train_interval} acciones\")\n",
    "print(f\"Learning rate: 0.00025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrainingSection"
   },
   "source": [
    "---\n",
    "### **Entrenamiento del agente**\n",
    "\n",
    "**Configuración del entrenamiento:**\n",
    "- Pasos totales: 2,000,000 (suficiente para convergencia en Atari)\n",
    "- Log interval: cada 10,000 pasos (monitorizar progreso)\n",
    "- Checkpoints: cada 500,000 pasos (guardar progreso)\n",
    "- Visualización: desactivada (más rápido)\n",
    "\n",
    "**Métricas a observar durante el entrenamiento:**\n",
    "- episode_reward: recompensa total por episodio (objetivo: >20)\n",
    "- loss: error de predicción de Q-values\n",
    "- mae: error absoluto medio\n",
    "- mean_q: Q-value promedio (debe aumentar con el tiempo)\n",
    "- mean_eps: epsilon actual (debe decrecer)\n",
    "\n",
    "**Nota:** El entrenamiento puede dura varias horas (12-24h en GPU, 36 en CPU).\n",
    "Se recomienda usar GPU y guardar checkpoints para continuar si se interrumpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TrainingCell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INICIANDO ENTRENAMIENTO ===\n",
      "Pasos totales: 5,000,000\n",
      "Checkpoints se guardarán cada 500,000 pasos\n",
      "\n",
      "Para continuar desde un checkpoint:\n",
      "  dqn.load_weights('dqn_SpaceInvaders-v0_weights_XXXXX.h5f')\n",
      "\n",
      "Iniciando...\n",
      "\n",
      "Training for 5000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   79/10000 [..............................] - ETA: 19s - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bonaf\\.conda\\envs\\gym_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0131\n",
      "15 episodes - episode_reward: 8.333 [6.000, 12.000] - ale.lives: 2.052\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0129\n",
      "16 episodes - episode_reward: 8.062 [5.000, 11.000] - ale.lives: 2.072\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0134\n",
      "16 episodes - episode_reward: 8.500 [4.000, 14.000] - ale.lives: 2.072\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0159\n",
      "14 episodes - episode_reward: 11.429 [5.000, 23.000] - ale.lives: 2.065\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 20s 2ms/step - reward: 0.0131\n",
      "15 episodes - episode_reward: 8.133 [4.000, 17.000] - ale.lives: 2.150\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0134\n",
      "15 episodes - episode_reward: 9.533 [4.000, 21.000] - loss: 0.007 - mae: 0.020 - mean_q: 0.027 - mean_eps: 0.951 - ale.lives: 2.072\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 183s 18ms/step - reward: 0.0129\n",
      "14 episodes - episode_reward: 9.143 [3.000, 15.000] - loss: 0.006 - mae: 0.018 - mean_q: 0.024 - mean_eps: 0.942 - ale.lives: 2.135\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: 0.0129\n",
      "15 episodes - episode_reward: 8.667 [4.000, 18.000] - loss: 0.007 - mae: 0.030 - mean_q: 0.039 - mean_eps: 0.933 - ale.lives: 2.146\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: 0.0132\n",
      "16 episodes - episode_reward: 8.438 [2.000, 19.000] - loss: 0.007 - mae: 0.046 - mean_q: 0.059 - mean_eps: 0.924 - ale.lives: 2.073\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: 0.0141\n",
      "13 episodes - episode_reward: 10.615 [5.000, 21.000] - loss: 0.007 - mae: 0.057 - mean_q: 0.073 - mean_eps: 0.915 - ale.lives: 2.119\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 183s 18ms/step - reward: 0.0137\n",
      "14 episodes - episode_reward: 9.643 [5.000, 18.000] - loss: 0.006 - mae: 0.064 - mean_q: 0.082 - mean_eps: 0.906 - ale.lives: 2.047\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0163\n",
      "14 episodes - episode_reward: 11.643 [3.000, 29.000] - loss: 0.006 - mae: 0.073 - mean_q: 0.094 - mean_eps: 0.897 - ale.lives: 2.214\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0150\n",
      "15 episodes - episode_reward: 10.333 [6.000, 19.000] - loss: 0.006 - mae: 0.091 - mean_q: 0.117 - mean_eps: 0.888 - ale.lives: 2.079\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0142\n",
      "12 episodes - episode_reward: 11.583 [2.000, 19.000] - loss: 0.007 - mae: 0.112 - mean_q: 0.143 - mean_eps: 0.879 - ale.lives: 2.117\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: 0.0141\n",
      "15 episodes - episode_reward: 9.067 [3.000, 28.000] - loss: 0.007 - mae: 0.134 - mean_q: 0.169 - mean_eps: 0.870 - ale.lives: 2.079\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: 0.0131\n",
      "13 episodes - episode_reward: 9.923 [3.000, 14.000] - loss: 0.007 - mae: 0.148 - mean_q: 0.187 - mean_eps: 0.861 - ale.lives: 2.117\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 190s 19ms/step - reward: 0.0131\n",
      "14 episodes - episode_reward: 10.071 [3.000, 17.000] - loss: 0.007 - mae: 0.169 - mean_q: 0.213 - mean_eps: 0.852 - ale.lives: 1.965\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 192s 19ms/step - reward: 0.0145\n",
      "14 episodes - episode_reward: 10.357 [2.000, 18.000] - loss: 0.007 - mae: 0.183 - mean_q: 0.229 - mean_eps: 0.843 - ale.lives: 2.044\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 192s 19ms/step - reward: 0.0153\n",
      "16 episodes - episode_reward: 9.000 [2.000, 16.000] - loss: 0.007 - mae: 0.194 - mean_q: 0.243 - mean_eps: 0.834 - ale.lives: 2.199\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: 0.0138\n",
      "14 episodes - episode_reward: 9.857 [3.000, 18.000] - loss: 0.007 - mae: 0.214 - mean_q: 0.267 - mean_eps: 0.825 - ale.lives: 2.109\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 195s 19ms/step - reward: 0.0154\n",
      "13 episodes - episode_reward: 11.923 [3.000, 21.000] - loss: 0.007 - mae: 0.234 - mean_q: 0.294 - mean_eps: 0.816 - ale.lives: 2.144\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 196s 20ms/step - reward: 0.0157\n",
      "16 episodes - episode_reward: 10.125 [4.000, 22.000] - loss: 0.008 - mae: 0.249 - mean_q: 0.311 - mean_eps: 0.807 - ale.lives: 2.099\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 199s 20ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 10.571 [6.000, 16.000] - loss: 0.007 - mae: 0.252 - mean_q: 0.315 - mean_eps: 0.798 - ale.lives: 2.108\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 197s 20ms/step - reward: 0.0146\n",
      "15 episodes - episode_reward: 9.533 [5.000, 18.000] - loss: 0.008 - mae: 0.288 - mean_q: 0.359 - mean_eps: 0.789 - ale.lives: 2.061\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: 0.0174\n",
      "14 episodes - episode_reward: 13.286 [8.000, 28.000] - loss: 0.008 - mae: 0.321 - mean_q: 0.400 - mean_eps: 0.780 - ale.lives: 2.075\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: 0.0157\n",
      "15 episodes - episode_reward: 9.600 [4.000, 19.000] - loss: 0.008 - mae: 0.336 - mean_q: 0.418 - mean_eps: 0.771 - ale.lives: 2.022\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 203s 20ms/step - reward: 0.0164\n",
      "13 episodes - episode_reward: 13.692 [11.000, 19.000] - loss: 0.008 - mae: 0.347 - mean_q: 0.432 - mean_eps: 0.762 - ale.lives: 1.989\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 204s 20ms/step - reward: 0.0173\n",
      "15 episodes - episode_reward: 11.267 [3.000, 22.000] - loss: 0.008 - mae: 0.367 - mean_q: 0.457 - mean_eps: 0.753 - ale.lives: 2.031\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 206s 21ms/step - reward: 0.0167\n",
      "14 episodes - episode_reward: 11.286 [5.000, 18.000] - loss: 0.009 - mae: 0.404 - mean_q: 0.503 - mean_eps: 0.744 - ale.lives: 2.040\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: 0.0187\n",
      "15 episodes - episode_reward: 13.333 [8.000, 27.000] - loss: 0.009 - mae: 0.420 - mean_q: 0.522 - mean_eps: 0.735 - ale.lives: 2.087\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: 0.0185\n",
      "13 episodes - episode_reward: 13.692 [5.000, 23.000] - loss: 0.009 - mae: 0.442 - mean_q: 0.549 - mean_eps: 0.726 - ale.lives: 2.158\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0175\n",
      "13 episodes - episode_reward: 13.308 [6.000, 30.000] - loss: 0.009 - mae: 0.457 - mean_q: 0.567 - mean_eps: 0.717 - ale.lives: 2.083\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0184\n",
      "14 episodes - episode_reward: 13.000 [6.000, 24.000] - loss: 0.009 - mae: 0.481 - mean_q: 0.597 - mean_eps: 0.708 - ale.lives: 2.180\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0179\n",
      "15 episodes - episode_reward: 12.533 [5.000, 20.000] - loss: 0.010 - mae: 0.515 - mean_q: 0.638 - mean_eps: 0.699 - ale.lives: 2.002\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: 0.0184\n",
      "14 episodes - episode_reward: 13.143 [5.000, 23.000] - loss: 0.010 - mae: 0.544 - mean_q: 0.674 - mean_eps: 0.690 - ale.lives: 2.114\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0192\n",
      "14 episodes - episode_reward: 13.929 [5.000, 24.000] - loss: 0.009 - mae: 0.552 - mean_q: 0.683 - mean_eps: 0.681 - ale.lives: 2.146\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0185\n",
      "13 episodes - episode_reward: 13.923 [7.000, 26.000] - loss: 0.010 - mae: 0.584 - mean_q: 0.723 - mean_eps: 0.672 - ale.lives: 2.114\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0183\n",
      "13 episodes - episode_reward: 14.385 [6.000, 33.000] - loss: 0.010 - mae: 0.603 - mean_q: 0.746 - mean_eps: 0.663 - ale.lives: 2.165\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0171\n",
      "11 episodes - episode_reward: 14.818 [9.000, 26.000] - loss: 0.010 - mae: 0.636 - mean_q: 0.785 - mean_eps: 0.654 - ale.lives: 2.113\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0205\n",
      "10 episodes - episode_reward: 17.800 [8.000, 35.000] - loss: 0.011 - mae: 0.658 - mean_q: 0.812 - mean_eps: 0.645 - ale.lives: 2.097\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0182\n",
      "16 episodes - episode_reward: 12.812 [2.000, 44.000] - loss: 0.011 - mae: 0.684 - mean_q: 0.843 - mean_eps: 0.636 - ale.lives: 2.131\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0184\n",
      "15 episodes - episode_reward: 13.067 [5.000, 22.000] - loss: 0.011 - mae: 0.717 - mean_q: 0.884 - mean_eps: 0.627 - ale.lives: 2.263\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0189\n",
      "14 episodes - episode_reward: 13.500 [7.000, 24.000] - loss: 0.011 - mae: 0.740 - mean_q: 0.910 - mean_eps: 0.618 - ale.lives: 2.094\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0182\n",
      "14 episodes - episode_reward: 12.214 [5.000, 30.000] - loss: 0.012 - mae: 0.757 - mean_q: 0.931 - mean_eps: 0.609 - ale.lives: 2.111\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0191\n",
      "12 episodes - episode_reward: 15.833 [4.000, 34.000] - loss: 0.012 - mae: 0.784 - mean_q: 0.963 - mean_eps: 0.600 - ale.lives: 2.093\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 230s 23ms/step - reward: 0.0194\n",
      "14 episodes - episode_reward: 13.500 [7.000, 25.000] - loss: 0.011 - mae: 0.794 - mean_q: 0.975 - mean_eps: 0.591 - ale.lives: 2.242\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 231s 23ms/step - reward: 0.0172\n",
      "15 episodes - episode_reward: 12.067 [8.000, 17.000] - loss: 0.011 - mae: 0.805 - mean_q: 0.988 - mean_eps: 0.582 - ale.lives: 2.186\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 232s 23ms/step - reward: 0.0199\n",
      "14 episodes - episode_reward: 14.071 [8.000, 25.000] - loss: 0.012 - mae: 0.846 - mean_q: 1.039 - mean_eps: 0.573 - ale.lives: 2.015\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 234s 23ms/step - reward: 0.0196\n",
      "13 episodes - episode_reward: 14.615 [6.000, 36.000] - loss: 0.012 - mae: 0.888 - mean_q: 1.089 - mean_eps: 0.564 - ale.lives: 2.130\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0190\n",
      "12 episodes - episode_reward: 15.917 [9.000, 29.000] - loss: 0.013 - mae: 0.926 - mean_q: 1.134 - mean_eps: 0.555 - ale.lives: 2.044\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 238s 24ms/step - reward: 0.0186\n",
      "15 episodes - episode_reward: 13.333 [4.000, 30.000] - loss: 0.013 - mae: 0.928 - mean_q: 1.137 - mean_eps: 0.546 - ale.lives: 2.207\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 239s 24ms/step - reward: 0.0200\n",
      "13 episodes - episode_reward: 14.154 [4.000, 27.000] - loss: 0.013 - mae: 0.960 - mean_q: 1.175 - mean_eps: 0.537 - ale.lives: 2.126\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 239s 24ms/step - reward: 0.0190\n",
      "14 episodes - episode_reward: 13.929 [9.000, 22.000] - loss: 0.013 - mae: 0.969 - mean_q: 1.187 - mean_eps: 0.528 - ale.lives: 2.118\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 242s 24ms/step - reward: 0.0198\n",
      "13 episodes - episode_reward: 15.462 [7.000, 35.000] - loss: 0.013 - mae: 1.002 - mean_q: 1.226 - mean_eps: 0.519 - ale.lives: 2.163\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 241s 24ms/step - reward: 0.0202\n",
      "13 episodes - episode_reward: 16.000 [7.000, 34.000] - loss: 0.013 - mae: 1.019 - mean_q: 1.246 - mean_eps: 0.510 - ale.lives: 2.102\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 243s 24ms/step - reward: 0.0177\n",
      "14 episodes - episode_reward: 12.500 [3.000, 31.000] - loss: 0.013 - mae: 1.033 - mean_q: 1.261 - mean_eps: 0.501 - ale.lives: 2.059\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 244s 24ms/step - reward: 0.0194\n",
      "14 episodes - episode_reward: 14.071 [5.000, 24.000] - loss: 0.013 - mae: 1.072 - mean_q: 1.309 - mean_eps: 0.492 - ale.lives: 2.122\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 246s 25ms/step - reward: 0.0201\n",
      "13 episodes - episode_reward: 14.615 [8.000, 27.000] - loss: 0.013 - mae: 1.092 - mean_q: 1.333 - mean_eps: 0.483 - ale.lives: 2.102\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 247s 25ms/step - reward: 0.0182\n",
      "15 episodes - episode_reward: 12.733 [8.000, 22.000] - loss: 0.013 - mae: 1.095 - mean_q: 1.335 - mean_eps: 0.474 - ale.lives: 2.194\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: 0.0195\n",
      "13 episodes - episode_reward: 15.077 [6.000, 24.000] - loss: 0.013 - mae: 1.135 - mean_q: 1.383 - mean_eps: 0.465 - ale.lives: 2.040\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: 0.0196\n",
      "14 episodes - episode_reward: 13.857 [4.000, 25.000] - loss: 0.013 - mae: 1.166 - mean_q: 1.421 - mean_eps: 0.456 - ale.lives: 2.068\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 252s 25ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 17.583 [6.000, 34.000] - loss: 0.014 - mae: 1.178 - mean_q: 1.434 - mean_eps: 0.447 - ale.lives: 2.161\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: 0.0210\n",
      "13 episodes - episode_reward: 14.769 [8.000, 25.000] - loss: 0.014 - mae: 1.204 - mean_q: 1.466 - mean_eps: 0.438 - ale.lives: 2.214\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 255s 25ms/step - reward: 0.0190\n",
      "12 episodes - episode_reward: 16.500 [7.000, 32.000] - loss: 0.014 - mae: 1.221 - mean_q: 1.485 - mean_eps: 0.429 - ale.lives: 2.325\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 257s 26ms/step - reward: 0.0194\n",
      "13 episodes - episode_reward: 15.385 [7.000, 27.000] - loss: 0.014 - mae: 1.237 - mean_q: 1.504 - mean_eps: 0.420 - ale.lives: 2.146\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 258s 26ms/step - reward: 0.0202\n",
      "12 episodes - episode_reward: 15.583 [10.000, 27.000] - loss: 0.014 - mae: 1.246 - mean_q: 1.514 - mean_eps: 0.411 - ale.lives: 2.050\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0191\n",
      "14 episodes - episode_reward: 14.571 [5.000, 23.000] - loss: 0.014 - mae: 1.259 - mean_q: 1.529 - mean_eps: 0.402 - ale.lives: 2.251\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0199\n",
      "12 episodes - episode_reward: 16.333 [9.000, 23.000] - loss: 0.014 - mae: 1.303 - mean_q: 1.583 - mean_eps: 0.393 - ale.lives: 2.175\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0213\n",
      "14 episodes - episode_reward: 16.000 [8.000, 25.000] - loss: 0.014 - mae: 1.332 - mean_q: 1.617 - mean_eps: 0.384 - ale.lives: 2.143\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0197\n",
      "12 episodes - episode_reward: 16.083 [6.000, 35.000] - loss: 0.014 - mae: 1.348 - mean_q: 1.637 - mean_eps: 0.375 - ale.lives: 2.176\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0205\n",
      "13 episodes - episode_reward: 15.308 [5.000, 21.000] - loss: 0.015 - mae: 1.382 - mean_q: 1.676 - mean_eps: 0.366 - ale.lives: 2.142\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 17.500 [6.000, 29.000] - loss: 0.015 - mae: 1.417 - mean_q: 1.718 - mean_eps: 0.357 - ale.lives: 2.211\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 270s 27ms/step - reward: 0.0225\n",
      "13 episodes - episode_reward: 15.692 [7.000, 25.000] - loss: 0.014 - mae: 1.391 - mean_q: 1.688 - mean_eps: 0.348 - ale.lives: 2.240\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 270s 27ms/step - reward: 0.0208\n",
      "12 episodes - episode_reward: 20.083 [10.000, 34.000] - loss: 0.015 - mae: 1.415 - mean_q: 1.718 - mean_eps: 0.339 - ale.lives: 1.948\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 16.250 [9.000, 26.000] - loss: 0.015 - mae: 1.421 - mean_q: 1.725 - mean_eps: 0.330 - ale.lives: 2.089\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0209\n",
      "11 episodes - episode_reward: 20.727 [6.000, 32.000] - loss: 0.015 - mae: 1.468 - mean_q: 1.782 - mean_eps: 0.321 - ale.lives: 1.999\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0196\n",
      "13 episodes - episode_reward: 14.692 [8.000, 30.000] - loss: 0.015 - mae: 1.503 - mean_q: 1.822 - mean_eps: 0.312 - ale.lives: 2.241\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0202\n",
      "13 episodes - episode_reward: 15.462 [9.000, 32.000] - loss: 0.016 - mae: 1.522 - mean_q: 1.846 - mean_eps: 0.303 - ale.lives: 2.116\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0209\n",
      "15 episodes - episode_reward: 13.533 [4.000, 25.000] - loss: 0.016 - mae: 1.542 - mean_q: 1.870 - mean_eps: 0.294 - ale.lives: 2.129\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0192\n",
      "14 episodes - episode_reward: 14.286 [7.000, 26.000] - loss: 0.016 - mae: 1.537 - mean_q: 1.865 - mean_eps: 0.285 - ale.lives: 2.072\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 17.615 [6.000, 27.000] - loss: 0.016 - mae: 1.578 - mean_q: 1.916 - mean_eps: 0.276 - ale.lives: 2.147\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 15.308 [4.000, 26.000] - loss: 0.015 - mae: 1.574 - mean_q: 1.909 - mean_eps: 0.267 - ale.lives: 2.057\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: 0.0236\n",
      "13 episodes - episode_reward: 17.846 [9.000, 30.000] - loss: 0.016 - mae: 1.569 - mean_q: 1.903 - mean_eps: 0.258 - ale.lives: 1.987\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 285s 28ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 17.923 [7.000, 28.000] - loss: 0.015 - mae: 1.602 - mean_q: 1.943 - mean_eps: 0.249 - ale.lives: 2.129\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 286s 29ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.583 [10.000, 30.000] - loss: 0.016 - mae: 1.618 - mean_q: 1.962 - mean_eps: 0.240 - ale.lives: 2.051\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 289s 29ms/step - reward: 0.0229\n",
      "14 episodes - episode_reward: 16.429 [8.000, 27.000] - loss: 0.016 - mae: 1.658 - mean_q: 2.011 - mean_eps: 0.231 - ale.lives: 2.095\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 289s 29ms/step - reward: 0.0205\n",
      "14 episodes - episode_reward: 15.071 [7.000, 26.000] - loss: 0.016 - mae: 1.697 - mean_q: 2.058 - mean_eps: 0.222 - ale.lives: 2.083\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 289s 29ms/step - reward: 0.0221\n",
      "11 episodes - episode_reward: 18.182 [7.000, 32.000] - loss: 0.015 - mae: 1.711 - mean_q: 2.073 - mean_eps: 0.213 - ale.lives: 2.277\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 293s 29ms/step - reward: 0.0205\n",
      "13 episodes - episode_reward: 17.000 [6.000, 28.000] - loss: 0.016 - mae: 1.718 - mean_q: 2.081 - mean_eps: 0.204 - ale.lives: 2.222\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 292s 29ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.667 [7.000, 30.000] - loss: 0.016 - mae: 1.728 - mean_q: 2.093 - mean_eps: 0.195 - ale.lives: 2.194\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 294s 29ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 17.417 [8.000, 28.000] - loss: 0.016 - mae: 1.755 - mean_q: 2.127 - mean_eps: 0.186 - ale.lives: 2.024\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 292s 29ms/step - reward: 0.0198\n",
      "13 episodes - episode_reward: 14.308 [9.000, 24.000] - loss: 0.016 - mae: 1.768 - mean_q: 2.142 - mean_eps: 0.177 - ale.lives: 2.084\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 294s 29ms/step - reward: 0.0224\n",
      "14 episodes - episode_reward: 16.571 [6.000, 27.000] - loss: 0.017 - mae: 1.779 - mean_q: 2.156 - mean_eps: 0.168 - ale.lives: 2.252\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 305s 30ms/step - reward: 0.0210\n",
      "14 episodes - episode_reward: 14.929 [6.000, 28.000] - loss: 0.017 - mae: 1.791 - mean_q: 2.170 - mean_eps: 0.159 - ale.lives: 2.296\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0207\n",
      "11 episodes - episode_reward: 17.909 [10.000, 33.000] - loss: 0.017 - mae: 1.783 - mean_q: 2.159 - mean_eps: 0.150 - ale.lives: 2.104\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0202\n",
      "14 episodes - episode_reward: 15.429 [7.000, 22.000] - loss: 0.017 - mae: 1.800 - mean_q: 2.179 - mean_eps: 0.141 - ale.lives: 2.102\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0229\n",
      "13 episodes - episode_reward: 16.308 [9.000, 33.000] - loss: 0.016 - mae: 1.825 - mean_q: 2.211 - mean_eps: 0.132 - ale.lives: 2.053\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0179\n",
      "11 episodes - episode_reward: 17.455 [8.000, 31.000] - loss: 0.017 - mae: 1.834 - mean_q: 2.221 - mean_eps: 0.123 - ale.lives: 2.109\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0186\n",
      "14 episodes - episode_reward: 13.643 [3.000, 24.000] - loss: 0.017 - mae: 1.852 - mean_q: 2.242 - mean_eps: 0.114 - ale.lives: 2.099\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 17.500 [9.000, 25.000] - loss: 0.017 - mae: 1.898 - mean_q: 2.297 - mean_eps: 0.105 - ale.lives: 2.188\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 14.692 [3.000, 23.000] - loss: 0.018 - mae: 1.915 - mean_q: 2.317 - mean_eps: 0.100 - ale.lives: 2.048\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0200\n",
      "12 episodes - episode_reward: 15.750 [4.000, 29.000] - loss: 0.018 - mae: 1.944 - mean_q: 2.352 - mean_eps: 0.100 - ale.lives: 2.124\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0201\n",
      "14 episodes - episode_reward: 15.571 [6.000, 25.000] - loss: 0.018 - mae: 1.940 - mean_q: 2.346 - mean_eps: 0.100 - ale.lives: 2.115\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0198\n",
      "14 episodes - episode_reward: 13.714 [3.000, 26.000] - loss: 0.018 - mae: 1.926 - mean_q: 2.327 - mean_eps: 0.100 - ale.lives: 2.172\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0206\n",
      "12 episodes - episode_reward: 16.250 [10.000, 23.000] - loss: 0.018 - mae: 1.954 - mean_q: 2.360 - mean_eps: 0.100 - ale.lives: 1.978\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0175\n",
      "13 episodes - episode_reward: 13.462 [2.000, 31.000] - loss: 0.017 - mae: 1.942 - mean_q: 2.344 - mean_eps: 0.100 - ale.lives: 2.071\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0178\n",
      "13 episodes - episode_reward: 14.615 [4.000, 27.000] - loss: 0.017 - mae: 1.937 - mean_q: 2.338 - mean_eps: 0.100 - ale.lives: 2.179\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0209\n",
      "12 episodes - episode_reward: 17.500 [10.000, 26.000] - loss: 0.017 - mae: 1.933 - mean_q: 2.334 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0215\n",
      "11 episodes - episode_reward: 18.727 [7.000, 27.000] - loss: 0.017 - mae: 1.963 - mean_q: 2.368 - mean_eps: 0.100 - ale.lives: 2.223\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0191\n",
      "11 episodes - episode_reward: 17.364 [4.000, 28.000] - loss: 0.017 - mae: 1.994 - mean_q: 2.407 - mean_eps: 0.100 - ale.lives: 2.155\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0177\n",
      "14 episodes - episode_reward: 13.357 [3.000, 25.000] - loss: 0.017 - mae: 1.992 - mean_q: 2.404 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0181\n",
      "13 episodes - episode_reward: 14.154 [5.000, 22.000] - loss: 0.017 - mae: 2.048 - mean_q: 2.472 - mean_eps: 0.100 - ale.lives: 2.144\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0202\n",
      "10 episodes - episode_reward: 19.400 [8.000, 29.000] - loss: 0.017 - mae: 2.065 - mean_q: 2.492 - mean_eps: 0.100 - ale.lives: 2.009\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0209\n",
      "12 episodes - episode_reward: 17.333 [4.000, 30.000] - loss: 0.017 - mae: 2.070 - mean_q: 2.498 - mean_eps: 0.100 - ale.lives: 2.036\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0200\n",
      "12 episodes - episode_reward: 16.667 [6.000, 27.000] - loss: 0.018 - mae: 2.086 - mean_q: 2.517 - mean_eps: 0.100 - ale.lives: 2.108\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0177\n",
      "14 episodes - episode_reward: 12.929 [6.000, 22.000] - loss: 0.019 - mae: 2.086 - mean_q: 2.517 - mean_eps: 0.100 - ale.lives: 2.146\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0215\n",
      "11 episodes - episode_reward: 19.000 [8.000, 25.000] - loss: 0.018 - mae: 2.095 - mean_q: 2.530 - mean_eps: 0.100 - ale.lives: 2.148\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0199\n",
      "12 episodes - episode_reward: 16.750 [6.000, 31.000] - loss: 0.018 - mae: 2.103 - mean_q: 2.538 - mean_eps: 0.100 - ale.lives: 2.168\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0184\n",
      "14 episodes - episode_reward: 13.143 [6.000, 26.000] - loss: 0.018 - mae: 2.114 - mean_q: 2.553 - mean_eps: 0.100 - ale.lives: 2.148\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0201\n",
      "12 episodes - episode_reward: 16.583 [10.000, 27.000] - loss: 0.018 - mae: 2.110 - mean_q: 2.547 - mean_eps: 0.100 - ale.lives: 1.987\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0199\n",
      "12 episodes - episode_reward: 16.333 [8.000, 30.000] - loss: 0.018 - mae: 2.124 - mean_q: 2.564 - mean_eps: 0.100 - ale.lives: 2.047\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0196\n",
      "14 episodes - episode_reward: 15.000 [7.000, 27.000] - loss: 0.018 - mae: 2.161 - mean_q: 2.610 - mean_eps: 0.100 - ale.lives: 2.232\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0201\n",
      "13 episodes - episode_reward: 15.000 [7.000, 25.000] - loss: 0.019 - mae: 2.184 - mean_q: 2.637 - mean_eps: 0.100 - ale.lives: 2.211\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0190\n",
      "16 episodes - episode_reward: 12.250 [4.000, 22.000] - loss: 0.018 - mae: 2.186 - mean_q: 2.640 - mean_eps: 0.100 - ale.lives: 2.079\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 16.833 [3.000, 29.000] - loss: 0.018 - mae: 2.223 - mean_q: 2.686 - mean_eps: 0.100 - ale.lives: 2.228\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.846 [3.000, 29.000] - loss: 0.018 - mae: 2.245 - mean_q: 2.709 - mean_eps: 0.100 - ale.lives: 2.236\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0208\n",
      "12 episodes - episode_reward: 16.750 [6.000, 25.000] - loss: 0.018 - mae: 2.264 - mean_q: 2.733 - mean_eps: 0.100 - ale.lives: 2.020\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 16.231 [7.000, 32.000] - loss: 0.019 - mae: 2.303 - mean_q: 2.779 - mean_eps: 0.100 - ale.lives: 2.103\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0217\n",
      "14 episodes - episode_reward: 16.214 [5.000, 29.000] - loss: 0.019 - mae: 2.354 - mean_q: 2.842 - mean_eps: 0.100 - ale.lives: 1.974\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0196\n",
      "10 episodes - episode_reward: 18.900 [8.000, 32.000] - loss: 0.019 - mae: 2.353 - mean_q: 2.839 - mean_eps: 0.100 - ale.lives: 1.986\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 16.615 [7.000, 27.000] - loss: 0.019 - mae: 2.346 - mean_q: 2.829 - mean_eps: 0.100 - ale.lives: 2.012\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0200\n",
      "13 episodes - episode_reward: 14.000 [8.000, 23.000] - loss: 0.019 - mae: 2.378 - mean_q: 2.868 - mean_eps: 0.100 - ale.lives: 2.029\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0214\n",
      "13 episodes - episode_reward: 18.077 [10.000, 26.000] - loss: 0.019 - mae: 2.402 - mean_q: 2.898 - mean_eps: 0.100 - ale.lives: 1.984\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0199\n",
      "11 episodes - episode_reward: 17.000 [3.000, 31.000] - loss: 0.019 - mae: 2.400 - mean_q: 2.895 - mean_eps: 0.100 - ale.lives: 2.178\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0205\n",
      "12 episodes - episode_reward: 17.583 [8.000, 32.000] - loss: 0.019 - mae: 2.428 - mean_q: 2.928 - mean_eps: 0.100 - ale.lives: 2.135\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0186\n",
      "13 episodes - episode_reward: 13.000 [6.000, 23.000] - loss: 0.019 - mae: 2.429 - mean_q: 2.930 - mean_eps: 0.100 - ale.lives: 2.118\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0213\n",
      "14 episodes - episode_reward: 16.357 [6.000, 30.000] - loss: 0.019 - mae: 2.395 - mean_q: 2.889 - mean_eps: 0.100 - ale.lives: 2.107\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0203\n",
      "14 episodes - episode_reward: 14.786 [7.000, 21.000] - loss: 0.019 - mae: 2.403 - mean_q: 2.898 - mean_eps: 0.100 - ale.lives: 2.082\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0225\n",
      "13 episodes - episode_reward: 15.615 [6.000, 29.000] - loss: 0.019 - mae: 2.410 - mean_q: 2.907 - mean_eps: 0.100 - ale.lives: 1.988\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0204\n",
      "13 episodes - episode_reward: 17.231 [10.000, 27.000] - loss: 0.019 - mae: 2.431 - mean_q: 2.931 - mean_eps: 0.100 - ale.lives: 2.012\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0187\n",
      "12 episodes - episode_reward: 14.250 [6.000, 27.000] - loss: 0.019 - mae: 2.436 - mean_q: 2.937 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0205\n",
      "14 episodes - episode_reward: 15.857 [4.000, 26.000] - loss: 0.019 - mae: 2.438 - mean_q: 2.941 - mean_eps: 0.100 - ale.lives: 2.189\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0207\n",
      "14 episodes - episode_reward: 14.714 [6.000, 27.000] - loss: 0.019 - mae: 2.458 - mean_q: 2.965 - mean_eps: 0.100 - ale.lives: 2.057\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0201\n",
      "14 episodes - episode_reward: 14.714 [6.000, 28.000] - loss: 0.019 - mae: 2.468 - mean_q: 2.977 - mean_eps: 0.100 - ale.lives: 2.126\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0202\n",
      "11 episodes - episode_reward: 17.000 [10.000, 27.000] - loss: 0.020 - mae: 2.459 - mean_q: 2.966 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0225\n",
      "15 episodes - episode_reward: 16.000 [7.000, 24.000] - loss: 0.019 - mae: 2.458 - mean_q: 2.965 - mean_eps: 0.100 - ale.lives: 2.099\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 18.273 [2.000, 33.000] - loss: 0.019 - mae: 2.463 - mean_q: 2.969 - mean_eps: 0.100 - ale.lives: 2.072\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0201\n",
      "12 episodes - episode_reward: 16.917 [8.000, 24.000] - loss: 0.019 - mae: 2.447 - mean_q: 2.951 - mean_eps: 0.100 - ale.lives: 1.855\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0215\n",
      "13 episodes - episode_reward: 17.462 [12.000, 25.000] - loss: 0.019 - mae: 2.452 - mean_q: 2.955 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0199\n",
      "13 episodes - episode_reward: 14.462 [6.000, 24.000] - loss: 0.020 - mae: 2.452 - mean_q: 2.955 - mean_eps: 0.100 - ale.lives: 2.010\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0214\n",
      "14 episodes - episode_reward: 15.643 [6.000, 28.000] - loss: 0.019 - mae: 2.462 - mean_q: 2.967 - mean_eps: 0.100 - ale.lives: 2.078\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0187\n",
      "12 episodes - episode_reward: 15.333 [2.000, 31.000] - loss: 0.019 - mae: 2.475 - mean_q: 2.984 - mean_eps: 0.100 - ale.lives: 2.119\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0207\n",
      "13 episodes - episode_reward: 16.615 [9.000, 23.000] - loss: 0.019 - mae: 2.467 - mean_q: 2.972 - mean_eps: 0.100 - ale.lives: 2.034\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0228\n",
      "14 episodes - episode_reward: 16.071 [4.000, 31.000] - loss: 0.020 - mae: 2.483 - mean_q: 2.992 - mean_eps: 0.100 - ale.lives: 2.001\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 16.615 [9.000, 25.000] - loss: 0.019 - mae: 2.468 - mean_q: 2.975 - mean_eps: 0.100 - ale.lives: 1.983\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0223\n",
      "13 episodes - episode_reward: 17.846 [5.000, 25.000] - loss: 0.020 - mae: 2.483 - mean_q: 2.992 - mean_eps: 0.100 - ale.lives: 2.086\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0240\n",
      "11 episodes - episode_reward: 21.000 [9.000, 32.000] - loss: 0.018 - mae: 2.490 - mean_q: 3.002 - mean_eps: 0.100 - ale.lives: 2.085\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0242\n",
      "12 episodes - episode_reward: 19.667 [6.000, 33.000] - loss: 0.018 - mae: 2.480 - mean_q: 2.989 - mean_eps: 0.100 - ale.lives: 2.095\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0240\n",
      "11 episodes - episode_reward: 22.364 [8.000, 31.000] - loss: 0.018 - mae: 2.472 - mean_q: 2.979 - mean_eps: 0.100 - ale.lives: 2.005\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 20.364 [4.000, 33.000] - loss: 0.019 - mae: 2.487 - mean_q: 2.998 - mean_eps: 0.100 - ale.lives: 2.243\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0195\n",
      "11 episodes - episode_reward: 17.909 [10.000, 32.000] - loss: 0.019 - mae: 2.496 - mean_q: 3.008 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0232\n",
      "10 episodes - episode_reward: 24.300 [12.000, 37.000] - loss: 0.019 - mae: 2.490 - mean_q: 3.000 - mean_eps: 0.100 - ale.lives: 1.839\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0229\n",
      "11 episodes - episode_reward: 19.818 [5.000, 35.000] - loss: 0.019 - mae: 2.494 - mean_q: 3.005 - mean_eps: 0.100 - ale.lives: 2.211\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 18.000 [11.000, 29.000] - loss: 0.020 - mae: 2.509 - mean_q: 3.023 - mean_eps: 0.100 - ale.lives: 2.150\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0224\n",
      "11 episodes - episode_reward: 18.818 [6.000, 29.000] - loss: 0.020 - mae: 2.535 - mean_q: 3.055 - mean_eps: 0.100 - ale.lives: 2.098\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 19.818 [11.000, 28.000] - loss: 0.020 - mae: 2.539 - mean_q: 3.059 - mean_eps: 0.100 - ale.lives: 2.169\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0214\n",
      "10 episodes - episode_reward: 20.500 [12.000, 31.000] - loss: 0.020 - mae: 2.559 - mean_q: 3.083 - mean_eps: 0.100 - ale.lives: 1.980\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0205\n",
      "11 episodes - episode_reward: 19.636 [11.000, 32.000] - loss: 0.021 - mae: 2.514 - mean_q: 3.029 - mean_eps: 0.100 - ale.lives: 2.026\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0209\n",
      "14 episodes - episode_reward: 16.071 [10.000, 33.000] - loss: 0.019 - mae: 2.530 - mean_q: 3.049 - mean_eps: 0.100 - ale.lives: 2.140\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0204\n",
      "12 episodes - episode_reward: 16.333 [3.000, 23.000] - loss: 0.019 - mae: 2.533 - mean_q: 3.051 - mean_eps: 0.100 - ale.lives: 2.227\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0213\n",
      "11 episodes - episode_reward: 19.273 [11.000, 25.000] - loss: 0.019 - mae: 2.535 - mean_q: 3.055 - mean_eps: 0.100 - ale.lives: 2.217\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0206\n",
      "13 episodes - episode_reward: 15.692 [8.000, 24.000] - loss: 0.019 - mae: 2.527 - mean_q: 3.046 - mean_eps: 0.100 - ale.lives: 2.121\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 20.364 [15.000, 27.000] - loss: 0.019 - mae: 2.513 - mean_q: 3.029 - mean_eps: 0.100 - ale.lives: 1.917\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0232\n",
      "13 episodes - episode_reward: 17.154 [6.000, 26.000] - loss: 0.019 - mae: 2.530 - mean_q: 3.050 - mean_eps: 0.100 - ale.lives: 2.042\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0218\n",
      "14 episodes - episode_reward: 16.357 [8.000, 32.000] - loss: 0.018 - mae: 2.510 - mean_q: 3.026 - mean_eps: 0.100 - ale.lives: 1.970\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0209\n",
      "11 episodes - episode_reward: 19.182 [10.000, 26.000] - loss: 0.019 - mae: 2.535 - mean_q: 3.055 - mean_eps: 0.100 - ale.lives: 1.984\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0224\n",
      "13 episodes - episode_reward: 16.462 [9.000, 26.000] - loss: 0.018 - mae: 2.510 - mean_q: 3.025 - mean_eps: 0.100 - ale.lives: 2.017\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0213\n",
      "15 episodes - episode_reward: 14.467 [5.000, 25.000] - loss: 0.018 - mae: 2.527 - mean_q: 3.046 - mean_eps: 0.100 - ale.lives: 2.070\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0239\n",
      "13 episodes - episode_reward: 18.385 [7.000, 26.000] - loss: 0.018 - mae: 2.519 - mean_q: 3.036 - mean_eps: 0.100 - ale.lives: 2.065\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 17.083 [6.000, 29.000] - loss: 0.019 - mae: 2.519 - mean_q: 3.036 - mean_eps: 0.100 - ale.lives: 2.084\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0194\n",
      "12 episodes - episode_reward: 17.917 [13.000, 26.000] - loss: 0.018 - mae: 2.526 - mean_q: 3.045 - mean_eps: 0.100 - ale.lives: 2.013\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0251\n",
      "11 episodes - episode_reward: 21.273 [12.000, 32.000] - loss: 0.018 - mae: 2.556 - mean_q: 3.081 - mean_eps: 0.100 - ale.lives: 2.197\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0213\n",
      "14 episodes - episode_reward: 15.929 [9.000, 26.000] - loss: 0.018 - mae: 2.551 - mean_q: 3.074 - mean_eps: 0.100 - ale.lives: 2.146\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0217\n",
      "14 episodes - episode_reward: 15.500 [7.000, 29.000] - loss: 0.019 - mae: 2.581 - mean_q: 3.111 - mean_eps: 0.100 - ale.lives: 2.108\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0241\n",
      "11 episodes - episode_reward: 22.545 [12.000, 33.000] - loss: 0.019 - mae: 2.575 - mean_q: 3.103 - mean_eps: 0.100 - ale.lives: 2.079\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0249\n",
      "10 episodes - episode_reward: 23.000 [6.000, 31.000] - loss: 0.018 - mae: 2.586 - mean_q: 3.117 - mean_eps: 0.100 - ale.lives: 2.109\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 17.308 [6.000, 24.000] - loss: 0.019 - mae: 2.584 - mean_q: 3.114 - mean_eps: 0.100 - ale.lives: 2.128\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0256\n",
      "12 episodes - episode_reward: 21.833 [14.000, 33.000] - loss: 0.018 - mae: 2.573 - mean_q: 3.101 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 18.167 [10.000, 33.000] - loss: 0.019 - mae: 2.575 - mean_q: 3.104 - mean_eps: 0.100 - ale.lives: 2.016\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0232\n",
      "13 episodes - episode_reward: 18.462 [7.000, 26.000] - loss: 0.018 - mae: 2.580 - mean_q: 3.110 - mean_eps: 0.100 - ale.lives: 1.969\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 19.333 [8.000, 29.000] - loss: 0.018 - mae: 2.586 - mean_q: 3.115 - mean_eps: 0.100 - ale.lives: 1.996\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.667 [8.000, 32.000] - loss: 0.018 - mae: 2.588 - mean_q: 3.119 - mean_eps: 0.100 - ale.lives: 2.080\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 21.000 [8.000, 33.000] - loss: 0.019 - mae: 2.580 - mean_q: 3.109 - mean_eps: 0.100 - ale.lives: 2.273\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 19.833 [10.000, 26.000] - loss: 0.018 - mae: 2.569 - mean_q: 3.097 - mean_eps: 0.100 - ale.lives: 2.122\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 16.077 [8.000, 33.000] - loss: 0.019 - mae: 2.588 - mean_q: 3.119 - mean_eps: 0.100 - ale.lives: 2.158\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0252\n",
      "13 episodes - episode_reward: 19.615 [9.000, 29.000] - loss: 0.018 - mae: 2.595 - mean_q: 3.128 - mean_eps: 0.100 - ale.lives: 2.168\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 16.692 [7.000, 31.000] - loss: 0.018 - mae: 2.605 - mean_q: 3.139 - mean_eps: 0.100 - ale.lives: 2.075\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0225\n",
      "13 episodes - episode_reward: 16.692 [8.000, 32.000] - loss: 0.018 - mae: 2.592 - mean_q: 3.123 - mean_eps: 0.100 - ale.lives: 1.914\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0221\n",
      "16 episodes - episode_reward: 14.938 [7.000, 22.000] - loss: 0.018 - mae: 2.621 - mean_q: 3.159 - mean_eps: 0.100 - ale.lives: 2.092\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 16.538 [5.000, 29.000] - loss: 0.019 - mae: 2.618 - mean_q: 3.156 - mean_eps: 0.100 - ale.lives: 2.275\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0233\n",
      "13 episodes - episode_reward: 17.385 [9.000, 27.000] - loss: 0.018 - mae: 2.610 - mean_q: 3.146 - mean_eps: 0.100 - ale.lives: 2.082\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0206\n",
      "13 episodes - episode_reward: 15.923 [10.000, 26.000] - loss: 0.019 - mae: 2.630 - mean_q: 3.170 - mean_eps: 0.100 - ale.lives: 2.216\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 21.000 [14.000, 32.000] - loss: 0.018 - mae: 2.640 - mean_q: 3.181 - mean_eps: 0.100 - ale.lives: 2.188\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0233\n",
      "14 episodes - episode_reward: 16.143 [7.000, 26.000] - loss: 0.019 - mae: 2.663 - mean_q: 3.208 - mean_eps: 0.100 - ale.lives: 2.159\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 19.417 [8.000, 31.000] - loss: 0.019 - mae: 2.658 - mean_q: 3.202 - mean_eps: 0.100 - ale.lives: 2.170\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0219\n",
      "14 episodes - episode_reward: 15.000 [3.000, 30.000] - loss: 0.018 - mae: 2.633 - mean_q: 3.173 - mean_eps: 0.100 - ale.lives: 2.139\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 19.667 [8.000, 26.000] - loss: 0.018 - mae: 2.652 - mean_q: 3.195 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0239\n",
      "12 episodes - episode_reward: 20.000 [6.000, 34.000] - loss: 0.018 - mae: 2.635 - mean_q: 3.175 - mean_eps: 0.100 - ale.lives: 2.077\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 17.667 [8.000, 30.000] - loss: 0.018 - mae: 2.640 - mean_q: 3.181 - mean_eps: 0.100 - ale.lives: 2.287\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0211\n",
      "12 episodes - episode_reward: 18.500 [8.000, 33.000] - loss: 0.018 - mae: 2.633 - mean_q: 3.173 - mean_eps: 0.100 - ale.lives: 2.172\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 20.000 [6.000, 32.000] - loss: 0.018 - mae: 2.627 - mean_q: 3.165 - mean_eps: 0.100 - ale.lives: 2.065\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 20.364 [9.000, 35.000] - loss: 0.018 - mae: 2.618 - mean_q: 3.155 - mean_eps: 0.100 - ale.lives: 2.061\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0210\n",
      "14 episodes - episode_reward: 14.929 [8.000, 27.000] - loss: 0.018 - mae: 2.638 - mean_q: 3.178 - mean_eps: 0.100 - ale.lives: 2.038\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0210\n",
      "13 episodes - episode_reward: 16.077 [10.000, 32.000] - loss: 0.018 - mae: 2.628 - mean_q: 3.165 - mean_eps: 0.100 - ale.lives: 2.197\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 17.923 [8.000, 25.000] - loss: 0.017 - mae: 2.626 - mean_q: 3.164 - mean_eps: 0.100 - ale.lives: 2.142\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 17.333 [13.000, 25.000] - loss: 0.017 - mae: 2.638 - mean_q: 3.178 - mean_eps: 0.100 - ale.lives: 2.141\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 20.364 [6.000, 29.000] - loss: 0.018 - mae: 2.642 - mean_q: 3.184 - mean_eps: 0.100 - ale.lives: 2.021\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0244\n",
      "13 episodes - episode_reward: 19.846 [11.000, 28.000] - loss: 0.019 - mae: 2.653 - mean_q: 3.195 - mean_eps: 0.100 - ale.lives: 2.089\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0229\n",
      "13 episodes - episode_reward: 17.615 [9.000, 25.000] - loss: 0.018 - mae: 2.658 - mean_q: 3.202 - mean_eps: 0.100 - ale.lives: 2.229\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 17.333 [7.000, 32.000] - loss: 0.018 - mae: 2.666 - mean_q: 3.213 - mean_eps: 0.100 - ale.lives: 2.103\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0215\n",
      "13 episodes - episode_reward: 16.769 [3.000, 31.000] - loss: 0.018 - mae: 2.672 - mean_q: 3.221 - mean_eps: 0.100 - ale.lives: 2.074\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 17.750 [7.000, 28.000] - loss: 0.019 - mae: 2.693 - mean_q: 3.245 - mean_eps: 0.100 - ale.lives: 2.141\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0236\n",
      "13 episodes - episode_reward: 20.000 [11.000, 29.000] - loss: 0.019 - mae: 2.682 - mean_q: 3.231 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 17.417 [9.000, 26.000] - loss: 0.019 - mae: 2.663 - mean_q: 3.209 - mean_eps: 0.100 - ale.lives: 2.052\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0210\n",
      "13 episodes - episode_reward: 16.385 [8.000, 30.000] - loss: 0.018 - mae: 2.641 - mean_q: 3.182 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0217\n",
      "12 episodes - episode_reward: 16.833 [4.000, 26.000] - loss: 0.018 - mae: 2.639 - mean_q: 3.177 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0228\n",
      "14 episodes - episode_reward: 17.929 [9.000, 30.000] - loss: 0.018 - mae: 2.628 - mean_q: 3.165 - mean_eps: 0.100 - ale.lives: 2.007\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0194\n",
      "13 episodes - episode_reward: 15.462 [7.000, 26.000] - loss: 0.019 - mae: 2.631 - mean_q: 3.168 - mean_eps: 0.100 - ale.lives: 2.085\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 17.727 [8.000, 28.000] - loss: 0.018 - mae: 2.616 - mean_q: 3.151 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0227\n",
      "15 episodes - episode_reward: 15.800 [6.000, 30.000] - loss: 0.018 - mae: 2.606 - mean_q: 3.138 - mean_eps: 0.100 - ale.lives: 2.155\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0201\n",
      "12 episodes - episode_reward: 17.083 [9.000, 30.000] - loss: 0.018 - mae: 2.584 - mean_q: 3.112 - mean_eps: 0.100 - ale.lives: 2.264\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0224\n",
      "11 episodes - episode_reward: 20.636 [6.000, 34.000] - loss: 0.018 - mae: 2.595 - mean_q: 3.125 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0227\n",
      "10 episodes - episode_reward: 23.100 [9.000, 35.000] - loss: 0.018 - mae: 2.595 - mean_q: 3.124 - mean_eps: 0.100 - ale.lives: 2.124\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 18.000 [7.000, 31.000] - loss: 0.017 - mae: 2.589 - mean_q: 3.118 - mean_eps: 0.100 - ale.lives: 2.167\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 21.455 [10.000, 27.000] - loss: 0.017 - mae: 2.590 - mean_q: 3.120 - mean_eps: 0.100 - ale.lives: 2.240\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 17.250 [9.000, 28.000] - loss: 0.018 - mae: 2.606 - mean_q: 3.139 - mean_eps: 0.100 - ale.lives: 2.058\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0213\n",
      "11 episodes - episode_reward: 21.455 [4.000, 28.000] - loss: 0.018 - mae: 2.610 - mean_q: 3.142 - mean_eps: 0.100 - ale.lives: 1.984\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0235\n",
      "11 episodes - episode_reward: 19.182 [7.000, 32.000] - loss: 0.018 - mae: 2.608 - mean_q: 3.141 - mean_eps: 0.100 - ale.lives: 2.139\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 20.500 [8.000, 31.000] - loss: 0.018 - mae: 2.611 - mean_q: 3.145 - mean_eps: 0.100 - ale.lives: 2.149\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 17.583 [7.000, 32.000] - loss: 0.018 - mae: 2.602 - mean_q: 3.133 - mean_eps: 0.100 - ale.lives: 2.015\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 18.154 [9.000, 27.000] - loss: 0.018 - mae: 2.598 - mean_q: 3.128 - mean_eps: 0.100 - ale.lives: 1.984\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 19.545 [11.000, 27.000] - loss: 0.019 - mae: 2.617 - mean_q: 3.151 - mean_eps: 0.100 - ale.lives: 2.105\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 16.923 [11.000, 26.000] - loss: 0.018 - mae: 2.612 - mean_q: 3.145 - mean_eps: 0.100 - ale.lives: 2.076\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 15.769 [6.000, 23.000] - loss: 0.018 - mae: 2.604 - mean_q: 3.135 - mean_eps: 0.100 - ale.lives: 2.132\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0236\n",
      "13 episodes - episode_reward: 19.308 [4.000, 29.000] - loss: 0.018 - mae: 2.611 - mean_q: 3.146 - mean_eps: 0.100 - ale.lives: 1.927\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0230\n",
      "11 episodes - episode_reward: 19.545 [9.000, 32.000] - loss: 0.018 - mae: 2.590 - mean_q: 3.118 - mean_eps: 0.100 - ale.lives: 2.057\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 18.333 [8.000, 29.000] - loss: 0.017 - mae: 2.573 - mean_q: 3.097 - mean_eps: 0.100 - ale.lives: 2.126\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0201\n",
      "12 episodes - episode_reward: 16.250 [7.000, 31.000] - loss: 0.017 - mae: 2.546 - mean_q: 3.067 - mean_eps: 0.100 - ale.lives: 2.124\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0243\n",
      "13 episodes - episode_reward: 20.000 [13.000, 29.000] - loss: 0.017 - mae: 2.560 - mean_q: 3.081 - mean_eps: 0.100 - ale.lives: 2.013\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0233\n",
      "10 episodes - episode_reward: 22.400 [9.000, 35.000] - loss: 0.017 - mae: 2.563 - mean_q: 3.086 - mean_eps: 0.100 - ale.lives: 2.239\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 17.385 [7.000, 29.000] - loss: 0.018 - mae: 2.581 - mean_q: 3.108 - mean_eps: 0.100 - ale.lives: 2.053\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 18.083 [4.000, 29.000] - loss: 0.018 - mae: 2.553 - mean_q: 3.075 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 21.636 [12.000, 31.000] - loss: 0.017 - mae: 2.565 - mean_q: 3.090 - mean_eps: 0.100 - ale.lives: 2.225\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0178\n",
      "12 episodes - episode_reward: 15.083 [7.000, 24.000] - loss: 0.018 - mae: 2.568 - mean_q: 3.093 - mean_eps: 0.100 - ale.lives: 2.140\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0240\n",
      "13 episodes - episode_reward: 19.077 [8.000, 28.000] - loss: 0.017 - mae: 2.559 - mean_q: 3.082 - mean_eps: 0.100 - ale.lives: 2.045\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 17.154 [7.000, 29.000] - loss: 0.017 - mae: 2.564 - mean_q: 3.088 - mean_eps: 0.100 - ale.lives: 2.019\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0193\n",
      "13 episodes - episode_reward: 14.538 [4.000, 29.000] - loss: 0.018 - mae: 2.573 - mean_q: 3.098 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0204\n",
      "13 episodes - episode_reward: 16.077 [6.000, 33.000] - loss: 0.018 - mae: 2.597 - mean_q: 3.127 - mean_eps: 0.100 - ale.lives: 2.124\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0238\n",
      "12 episodes - episode_reward: 19.417 [7.000, 27.000] - loss: 0.018 - mae: 2.602 - mean_q: 3.134 - mean_eps: 0.100 - ale.lives: 2.144\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 18.500 [5.000, 30.000] - loss: 0.017 - mae: 2.591 - mean_q: 3.120 - mean_eps: 0.100 - ale.lives: 2.183\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 19.545 [8.000, 28.000] - loss: 0.017 - mae: 2.579 - mean_q: 3.106 - mean_eps: 0.100 - ale.lives: 2.211\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 18.846 [6.000, 33.000] - loss: 0.018 - mae: 2.571 - mean_q: 3.096 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0233\n",
      "10 episodes - episode_reward: 22.100 [15.000, 31.000] - loss: 0.017 - mae: 2.576 - mean_q: 3.102 - mean_eps: 0.100 - ale.lives: 2.144\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0232\n",
      "14 episodes - episode_reward: 17.071 [9.000, 29.000] - loss: 0.018 - mae: 2.563 - mean_q: 3.086 - mean_eps: 0.100 - ale.lives: 2.084\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 21.545 [11.000, 29.000] - loss: 0.018 - mae: 2.585 - mean_q: 3.113 - mean_eps: 0.100 - ale.lives: 2.196\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 17.769 [6.000, 29.000] - loss: 0.017 - mae: 2.589 - mean_q: 3.118 - mean_eps: 0.100 - ale.lives: 1.957\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0214\n",
      "10 episodes - episode_reward: 22.200 [12.000, 33.000] - loss: 0.017 - mae: 2.580 - mean_q: 3.106 - mean_eps: 0.100 - ale.lives: 2.107\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 20.364 [7.000, 31.000] - loss: 0.017 - mae: 2.596 - mean_q: 3.125 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0217\n",
      "13 episodes - episode_reward: 15.231 [3.000, 27.000] - loss: 0.017 - mae: 2.592 - mean_q: 3.122 - mean_eps: 0.100 - ale.lives: 2.002\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 19.500 [7.000, 31.000] - loss: 0.017 - mae: 2.588 - mean_q: 3.116 - mean_eps: 0.100 - ale.lives: 2.052\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0214\n",
      "14 episodes - episode_reward: 15.929 [5.000, 28.000] - loss: 0.017 - mae: 2.591 - mean_q: 3.119 - mean_eps: 0.100 - ale.lives: 2.015\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0223\n",
      "14 episodes - episode_reward: 16.000 [8.000, 29.000] - loss: 0.017 - mae: 2.585 - mean_q: 3.113 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 17.417 [4.000, 32.000] - loss: 0.017 - mae: 2.597 - mean_q: 3.128 - mean_eps: 0.100 - ale.lives: 1.919\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0208\n",
      "13 episodes - episode_reward: 16.462 [7.000, 26.000] - loss: 0.017 - mae: 2.589 - mean_q: 3.117 - mean_eps: 0.100 - ale.lives: 1.974\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0207\n",
      "10 episodes - episode_reward: 18.700 [10.000, 32.000] - loss: 0.017 - mae: 2.593 - mean_q: 3.122 - mean_eps: 0.100 - ale.lives: 2.196\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0215\n",
      "14 episodes - episode_reward: 16.286 [3.000, 27.000] - loss: 0.016 - mae: 2.601 - mean_q: 3.133 - mean_eps: 0.100 - ale.lives: 2.053\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0213\n",
      "15 episodes - episode_reward: 14.667 [6.000, 29.000] - loss: 0.017 - mae: 2.625 - mean_q: 3.160 - mean_eps: 0.100 - ale.lives: 2.029\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 15.846 [8.000, 32.000] - loss: 0.017 - mae: 2.622 - mean_q: 3.159 - mean_eps: 0.100 - ale.lives: 2.044\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0238\n",
      "13 episodes - episode_reward: 18.308 [9.000, 30.000] - loss: 0.017 - mae: 2.637 - mean_q: 3.176 - mean_eps: 0.100 - ale.lives: 2.070\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0224\n",
      "11 episodes - episode_reward: 19.818 [5.000, 32.000] - loss: 0.017 - mae: 2.650 - mean_q: 3.192 - mean_eps: 0.100 - ale.lives: 1.987\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0214\n",
      "14 episodes - episode_reward: 16.286 [5.000, 28.000] - loss: 0.018 - mae: 2.675 - mean_q: 3.222 - mean_eps: 0.100 - ale.lives: 2.091\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.583 [5.000, 31.000] - loss: 0.019 - mae: 2.681 - mean_q: 3.228 - mean_eps: 0.100 - ale.lives: 1.975\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0215\n",
      "14 episodes - episode_reward: 14.286 [4.000, 27.000] - loss: 0.018 - mae: 2.676 - mean_q: 3.223 - mean_eps: 0.100 - ale.lives: 2.000\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0202\n",
      "12 episodes - episode_reward: 16.917 [8.000, 25.000] - loss: 0.018 - mae: 2.659 - mean_q: 3.202 - mean_eps: 0.100 - ale.lives: 2.223\n",
      "\n",
      "Interval 285 (2840000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0225\n",
      "12 episodes - episode_reward: 19.417 [6.000, 28.000] - loss: 0.017 - mae: 2.654 - mean_q: 3.196 - mean_eps: 0.100 - ale.lives: 2.046\n",
      "\n",
      "Interval 286 (2850000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0234\n",
      "12 episodes - episode_reward: 19.750 [8.000, 29.000] - loss: 0.018 - mae: 2.666 - mean_q: 3.210 - mean_eps: 0.100 - ale.lives: 1.906\n",
      "\n",
      "Interval 287 (2860000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0247\n",
      "13 episodes - episode_reward: 19.308 [5.000, 32.000] - loss: 0.018 - mae: 2.657 - mean_q: 3.199 - mean_eps: 0.100 - ale.lives: 1.934\n",
      "\n",
      "Interval 288 (2870000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0205\n",
      "14 episodes - episode_reward: 14.714 [6.000, 28.000] - loss: 0.018 - mae: 2.657 - mean_q: 3.199 - mean_eps: 0.100 - ale.lives: 2.057\n",
      "\n",
      "Interval 289 (2880000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0214\n",
      "13 episodes - episode_reward: 15.769 [6.000, 27.000] - loss: 0.019 - mae: 2.635 - mean_q: 3.172 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 290 (2890000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0214\n",
      "13 episodes - episode_reward: 16.154 [8.000, 28.000] - loss: 0.018 - mae: 2.624 - mean_q: 3.159 - mean_eps: 0.100 - ale.lives: 2.120\n",
      "\n",
      "Interval 291 (2900000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 17.333 [6.000, 28.000] - loss: 0.018 - mae: 2.613 - mean_q: 3.146 - mean_eps: 0.100 - ale.lives: 2.085\n",
      "\n",
      "Interval 292 (2910000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0208\n",
      "14 episodes - episode_reward: 15.786 [7.000, 28.000] - loss: 0.018 - mae: 2.623 - mean_q: 3.157 - mean_eps: 0.100 - ale.lives: 2.023\n",
      "\n",
      "Interval 293 (2920000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.667 [7.000, 34.000] - loss: 0.017 - mae: 2.613 - mean_q: 3.145 - mean_eps: 0.100 - ale.lives: 2.219\n",
      "\n",
      "Interval 294 (2930000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 17.167 [8.000, 27.000] - loss: 0.017 - mae: 2.603 - mean_q: 3.133 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 295 (2940000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 17.769 [10.000, 27.000] - loss: 0.017 - mae: 2.624 - mean_q: 3.158 - mean_eps: 0.100 - ale.lives: 2.097\n",
      "\n",
      "Interval 296 (2950000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0219\n",
      "10 episodes - episode_reward: 22.300 [15.000, 32.000] - loss: 0.017 - mae: 2.631 - mean_q: 3.166 - mean_eps: 0.100 - ale.lives: 2.078\n",
      "\n",
      "Interval 297 (2960000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 16.667 [5.000, 29.000] - loss: 0.017 - mae: 2.629 - mean_q: 3.164 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 298 (2970000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 19.667 [7.000, 32.000] - loss: 0.017 - mae: 2.621 - mean_q: 3.154 - mean_eps: 0.100 - ale.lives: 2.070\n",
      "\n",
      "Interval 299 (2980000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 17.000 [8.000, 31.000] - loss: 0.017 - mae: 2.615 - mean_q: 3.147 - mean_eps: 0.100 - ale.lives: 1.953\n",
      "\n",
      "Interval 300 (2990000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0221\n",
      "13 episodes - episode_reward: 17.000 [5.000, 26.000] - loss: 0.017 - mae: 2.626 - mean_q: 3.161 - mean_eps: 0.100 - ale.lives: 2.002\n",
      "\n",
      "Interval 301 (3000000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0203\n",
      "11 episodes - episode_reward: 18.364 [9.000, 31.000] - loss: 0.017 - mae: 2.621 - mean_q: 3.155 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 302 (3010000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.500 [5.000, 33.000] - loss: 0.017 - mae: 2.629 - mean_q: 3.164 - mean_eps: 0.100 - ale.lives: 2.139\n",
      "\n",
      "Interval 303 (3020000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 18.583 [3.000, 33.000] - loss: 0.017 - mae: 2.614 - mean_q: 3.146 - mean_eps: 0.100 - ale.lives: 2.110\n",
      "\n",
      "Interval 304 (3030000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0188\n",
      "12 episodes - episode_reward: 15.500 [9.000, 24.000] - loss: 0.017 - mae: 2.605 - mean_q: 3.135 - mean_eps: 0.100 - ale.lives: 2.185\n",
      "\n",
      "Interval 305 (3040000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 15.692 [6.000, 26.000] - loss: 0.017 - mae: 2.601 - mean_q: 3.130 - mean_eps: 0.100 - ale.lives: 1.980\n",
      "\n",
      "Interval 306 (3050000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0211\n",
      "12 episodes - episode_reward: 18.500 [8.000, 28.000] - loss: 0.017 - mae: 2.612 - mean_q: 3.144 - mean_eps: 0.100 - ale.lives: 2.136\n",
      "\n",
      "Interval 307 (3060000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0230\n",
      "12 episodes - episode_reward: 17.583 [5.000, 31.000] - loss: 0.017 - mae: 2.623 - mean_q: 3.158 - mean_eps: 0.100 - ale.lives: 1.940\n",
      "\n",
      "Interval 308 (3070000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 18.250 [6.000, 35.000] - loss: 0.017 - mae: 2.618 - mean_q: 3.150 - mean_eps: 0.100 - ale.lives: 2.119\n",
      "\n",
      "Interval 309 (3080000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0204\n",
      "13 episodes - episode_reward: 16.154 [3.000, 29.000] - loss: 0.017 - mae: 2.612 - mean_q: 3.144 - mean_eps: 0.100 - ale.lives: 2.060\n",
      "\n",
      "Interval 310 (3090000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0200\n",
      "14 episodes - episode_reward: 13.214 [6.000, 19.000] - loss: 0.016 - mae: 2.624 - mean_q: 3.159 - mean_eps: 0.100 - ale.lives: 2.120\n",
      "\n",
      "Interval 311 (3100000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0221\n",
      "13 episodes - episode_reward: 18.385 [7.000, 31.000] - loss: 0.016 - mae: 2.636 - mean_q: 3.172 - mean_eps: 0.100 - ale.lives: 2.138\n",
      "\n",
      "Interval 312 (3110000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0235\n",
      "12 episodes - episode_reward: 18.667 [6.000, 33.000] - loss: 0.016 - mae: 2.644 - mean_q: 3.182 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 313 (3120000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 19.727 [4.000, 27.000] - loss: 0.017 - mae: 2.646 - mean_q: 3.185 - mean_eps: 0.100 - ale.lives: 2.117\n",
      "\n",
      "Interval 314 (3130000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0206\n",
      "10 episodes - episode_reward: 21.900 [7.000, 32.000] - loss: 0.017 - mae: 2.645 - mean_q: 3.183 - mean_eps: 0.100 - ale.lives: 2.231\n",
      "\n",
      "Interval 315 (3140000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0221\n",
      "11 episodes - episode_reward: 19.455 [8.000, 26.000] - loss: 0.017 - mae: 2.649 - mean_q: 3.189 - mean_eps: 0.100 - ale.lives: 2.064\n",
      "\n",
      "Interval 316 (3150000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 18.417 [7.000, 31.000] - loss: 0.017 - mae: 2.641 - mean_q: 3.179 - mean_eps: 0.100 - ale.lives: 2.098\n",
      "\n",
      "Interval 317 (3160000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0210\n",
      "11 episodes - episode_reward: 17.364 [9.000, 29.000] - loss: 0.017 - mae: 2.642 - mean_q: 3.179 - mean_eps: 0.100 - ale.lives: 2.031\n",
      "\n",
      "Interval 318 (3170000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0248\n",
      "12 episodes - episode_reward: 21.417 [11.000, 31.000] - loss: 0.015 - mae: 2.631 - mean_q: 3.168 - mean_eps: 0.100 - ale.lives: 2.125\n",
      "\n",
      "Interval 319 (3180000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 17.750 [6.000, 27.000] - loss: 0.016 - mae: 2.620 - mean_q: 3.154 - mean_eps: 0.100 - ale.lives: 2.062\n",
      "\n",
      "Interval 320 (3190000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0215\n",
      "13 episodes - episode_reward: 16.846 [7.000, 26.000] - loss: 0.017 - mae: 2.641 - mean_q: 3.179 - mean_eps: 0.100 - ale.lives: 2.092\n",
      "\n",
      "Interval 321 (3200000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0207\n",
      "13 episodes - episode_reward: 16.692 [6.000, 31.000] - loss: 0.017 - mae: 2.648 - mean_q: 3.188 - mean_eps: 0.100 - ale.lives: 2.090\n",
      "\n",
      "Interval 322 (3210000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0196\n",
      "14 episodes - episode_reward: 13.357 [5.000, 26.000] - loss: 0.017 - mae: 2.650 - mean_q: 3.190 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 323 (3220000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0252\n",
      "13 episodes - episode_reward: 19.846 [10.000, 29.000] - loss: 0.017 - mae: 2.641 - mean_q: 3.180 - mean_eps: 0.100 - ale.lives: 2.173\n",
      "\n",
      "Interval 324 (3230000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0206\n",
      "13 episodes - episode_reward: 16.000 [4.000, 25.000] - loss: 0.017 - mae: 2.627 - mean_q: 3.163 - mean_eps: 0.100 - ale.lives: 2.054\n",
      "\n",
      "Interval 325 (3240000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 17.077 [4.000, 29.000] - loss: 0.017 - mae: 2.641 - mean_q: 3.180 - mean_eps: 0.100 - ale.lives: 2.097\n",
      "\n",
      "Interval 326 (3250000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 17.917 [6.000, 30.000] - loss: 0.018 - mae: 2.647 - mean_q: 3.186 - mean_eps: 0.100 - ale.lives: 2.189\n",
      "\n",
      "Interval 327 (3260000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0196\n",
      "12 episodes - episode_reward: 15.167 [6.000, 27.000] - loss: 0.018 - mae: 2.665 - mean_q: 3.209 - mean_eps: 0.100 - ale.lives: 2.090\n",
      "\n",
      "Interval 328 (3270000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0200\n",
      "12 episodes - episode_reward: 18.917 [6.000, 30.000] - loss: 0.017 - mae: 2.678 - mean_q: 3.225 - mean_eps: 0.100 - ale.lives: 2.239\n",
      "\n",
      "Interval 329 (3280000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 17.615 [10.000, 28.000] - loss: 0.018 - mae: 2.666 - mean_q: 3.210 - mean_eps: 0.100 - ale.lives: 2.031\n",
      "\n",
      "Interval 330 (3290000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 16.750 [4.000, 29.000] - loss: 0.018 - mae: 2.672 - mean_q: 3.217 - mean_eps: 0.100 - ale.lives: 1.967\n",
      "\n",
      "Interval 331 (3300000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0193\n",
      "13 episodes - episode_reward: 16.154 [3.000, 24.000] - loss: 0.018 - mae: 2.674 - mean_q: 3.220 - mean_eps: 0.100 - ale.lives: 2.070\n",
      "\n",
      "Interval 332 (3310000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 20.091 [4.000, 30.000] - loss: 0.017 - mae: 2.670 - mean_q: 3.215 - mean_eps: 0.100 - ale.lives: 2.085\n",
      "\n",
      "Interval 333 (3320000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 16.308 [6.000, 26.000] - loss: 0.017 - mae: 2.663 - mean_q: 3.206 - mean_eps: 0.100 - ale.lives: 2.098\n",
      "\n",
      "Interval 334 (3330000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0214\n",
      "14 episodes - episode_reward: 16.357 [9.000, 31.000] - loss: 0.018 - mae: 2.669 - mean_q: 3.212 - mean_eps: 0.100 - ale.lives: 2.073\n",
      "\n",
      "Interval 335 (3340000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 15.692 [6.000, 26.000] - loss: 0.018 - mae: 2.670 - mean_q: 3.214 - mean_eps: 0.100 - ale.lives: 2.150\n",
      "\n",
      "Interval 336 (3350000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 17.308 [8.000, 29.000] - loss: 0.018 - mae: 2.677 - mean_q: 3.222 - mean_eps: 0.100 - ale.lives: 2.126\n",
      "\n",
      "Interval 337 (3360000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 19.250 [14.000, 28.000] - loss: 0.017 - mae: 2.669 - mean_q: 3.212 - mean_eps: 0.100 - ale.lives: 2.096\n",
      "\n",
      "Interval 338 (3370000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0214\n",
      "14 episodes - episode_reward: 14.500 [7.000, 26.000] - loss: 0.018 - mae: 2.647 - mean_q: 3.188 - mean_eps: 0.100 - ale.lives: 2.019\n",
      "\n",
      "Interval 339 (3380000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 19.538 [11.000, 28.000] - loss: 0.018 - mae: 2.645 - mean_q: 3.184 - mean_eps: 0.100 - ale.lives: 1.814\n",
      "\n",
      "Interval 340 (3390000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 16.077 [8.000, 26.000] - loss: 0.018 - mae: 2.636 - mean_q: 3.174 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 341 (3400000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0226\n",
      "14 episodes - episode_reward: 16.643 [8.000, 24.000] - loss: 0.018 - mae: 2.640 - mean_q: 3.179 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 342 (3410000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0225\n",
      "13 episodes - episode_reward: 16.308 [7.000, 27.000] - loss: 0.018 - mae: 2.641 - mean_q: 3.178 - mean_eps: 0.100 - ale.lives: 2.014\n",
      "\n",
      "Interval 343 (3420000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.583 [9.000, 34.000] - loss: 0.018 - mae: 2.638 - mean_q: 3.174 - mean_eps: 0.100 - ale.lives: 1.968\n",
      "\n",
      "Interval 344 (3430000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0208\n",
      "11 episodes - episode_reward: 19.273 [8.000, 31.000] - loss: 0.017 - mae: 2.644 - mean_q: 3.183 - mean_eps: 0.100 - ale.lives: 1.976\n",
      "\n",
      "Interval 345 (3440000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0240\n",
      "11 episodes - episode_reward: 21.273 [12.000, 31.000] - loss: 0.017 - mae: 2.661 - mean_q: 3.203 - mean_eps: 0.100 - ale.lives: 1.965\n",
      "\n",
      "Interval 346 (3450000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0222\n",
      "11 episodes - episode_reward: 20.545 [8.000, 29.000] - loss: 0.017 - mae: 2.665 - mean_q: 3.207 - mean_eps: 0.100 - ale.lives: 2.304\n",
      "\n",
      "Interval 347 (3460000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 20.000 [5.000, 31.000] - loss: 0.017 - mae: 2.656 - mean_q: 3.197 - mean_eps: 0.100 - ale.lives: 2.078\n",
      "\n",
      "Interval 348 (3470000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0228\n",
      "10 episodes - episode_reward: 20.200 [8.000, 30.000] - loss: 0.017 - mae: 2.642 - mean_q: 3.180 - mean_eps: 0.100 - ale.lives: 2.071\n",
      "\n",
      "Interval 349 (3480000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0240\n",
      "13 episodes - episode_reward: 18.923 [7.000, 32.000] - loss: 0.017 - mae: 2.645 - mean_q: 3.183 - mean_eps: 0.100 - ale.lives: 2.056\n",
      "\n",
      "Interval 350 (3490000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0205\n",
      "12 episodes - episode_reward: 17.667 [9.000, 28.000] - loss: 0.018 - mae: 2.640 - mean_q: 3.177 - mean_eps: 0.100 - ale.lives: 1.992\n",
      "\n",
      "Interval 351 (3500000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0255\n",
      "11 episodes - episode_reward: 23.818 [11.000, 33.000] - loss: 0.017 - mae: 2.643 - mean_q: 3.181 - mean_eps: 0.100 - ale.lives: 1.984\n",
      "\n",
      "Interval 352 (3510000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0222\n",
      "11 episodes - episode_reward: 19.364 [3.000, 32.000] - loss: 0.018 - mae: 2.653 - mean_q: 3.193 - mean_eps: 0.100 - ale.lives: 1.983\n",
      "\n",
      "Interval 353 (3520000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0199\n",
      "11 episodes - episode_reward: 17.818 [3.000, 35.000] - loss: 0.018 - mae: 2.651 - mean_q: 3.190 - mean_eps: 0.100 - ale.lives: 1.911\n",
      "\n",
      "Interval 354 (3530000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 19.667 [10.000, 32.000] - loss: 0.018 - mae: 2.628 - mean_q: 3.162 - mean_eps: 0.100 - ale.lives: 2.041\n",
      "\n",
      "Interval 355 (3540000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0205\n",
      "13 episodes - episode_reward: 16.154 [7.000, 30.000] - loss: 0.017 - mae: 2.649 - mean_q: 3.188 - mean_eps: 0.100 - ale.lives: 2.208\n",
      "\n",
      "Interval 356 (3550000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0237\n",
      "13 episodes - episode_reward: 18.077 [9.000, 27.000] - loss: 0.017 - mae: 2.652 - mean_q: 3.191 - mean_eps: 0.100 - ale.lives: 2.139\n",
      "\n",
      "Interval 357 (3560000 steps performed)\n",
      "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0208\n",
      "14 episodes - episode_reward: 15.357 [4.000, 25.000] - loss: 0.017 - mae: 2.644 - mean_q: 3.182 - mean_eps: 0.100 - ale.lives: 2.106\n",
      "\n",
      "Interval 358 (3570000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0222\n",
      "11 episodes - episode_reward: 17.818 [4.000, 31.000] - loss: 0.017 - mae: 2.654 - mean_q: 3.193 - mean_eps: 0.100 - ale.lives: 1.991\n",
      "\n",
      "Interval 359 (3580000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 20.250 [10.000, 28.000] - loss: 0.017 - mae: 2.656 - mean_q: 3.196 - mean_eps: 0.100 - ale.lives: 2.057\n",
      "\n",
      "Interval 360 (3590000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0224\n",
      "13 episodes - episode_reward: 17.308 [4.000, 32.000] - loss: 0.017 - mae: 2.654 - mean_q: 3.195 - mean_eps: 0.100 - ale.lives: 2.151\n",
      "\n",
      "Interval 361 (3600000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0232\n",
      "10 episodes - episode_reward: 20.800 [9.000, 28.000] - loss: 0.017 - mae: 2.658 - mean_q: 3.199 - mean_eps: 0.100 - ale.lives: 1.992\n",
      "\n",
      "Interval 362 (3610000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 20.727 [9.000, 30.000] - loss: 0.017 - mae: 2.644 - mean_q: 3.182 - mean_eps: 0.100 - ale.lives: 2.208\n",
      "\n",
      "Interval 363 (3620000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0225\n",
      "14 episodes - episode_reward: 17.143 [7.000, 33.000] - loss: 0.017 - mae: 2.645 - mean_q: 3.183 - mean_eps: 0.100 - ale.lives: 1.993\n",
      "\n",
      "Interval 364 (3630000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0209\n",
      "9 episodes - episode_reward: 23.222 [15.000, 31.000] - loss: 0.018 - mae: 2.665 - mean_q: 3.208 - mean_eps: 0.100 - ale.lives: 1.955\n",
      "\n",
      "Interval 365 (3640000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 19.091 [7.000, 32.000] - loss: 0.017 - mae: 2.655 - mean_q: 3.195 - mean_eps: 0.100 - ale.lives: 2.229\n",
      "\n",
      "Interval 366 (3650000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 17.462 [7.000, 27.000] - loss: 0.017 - mae: 2.647 - mean_q: 3.185 - mean_eps: 0.100 - ale.lives: 2.008\n",
      "\n",
      "Interval 367 (3660000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0238\n",
      "11 episodes - episode_reward: 20.273 [12.000, 28.000] - loss: 0.017 - mae: 2.645 - mean_q: 3.184 - mean_eps: 0.100 - ale.lives: 1.975\n",
      "\n",
      "Interval 368 (3670000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 18.667 [8.000, 34.000] - loss: 0.017 - mae: 2.629 - mean_q: 3.165 - mean_eps: 0.100 - ale.lives: 2.103\n",
      "\n",
      "Interval 369 (3680000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0237\n",
      "13 episodes - episode_reward: 17.615 [8.000, 26.000] - loss: 0.017 - mae: 2.634 - mean_q: 3.170 - mean_eps: 0.100 - ale.lives: 2.103\n",
      "\n",
      "Interval 370 (3690000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 20.364 [6.000, 32.000] - loss: 0.017 - mae: 2.622 - mean_q: 3.155 - mean_eps: 0.100 - ale.lives: 1.905\n",
      "\n",
      "Interval 371 (3700000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 17.083 [8.000, 25.000] - loss: 0.017 - mae: 2.643 - mean_q: 3.182 - mean_eps: 0.100 - ale.lives: 1.965\n",
      "\n",
      "Interval 372 (3710000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0212\n",
      "11 episodes - episode_reward: 20.545 [8.000, 32.000] - loss: 0.016 - mae: 2.623 - mean_q: 3.157 - mean_eps: 0.100 - ale.lives: 2.069\n",
      "\n",
      "Interval 373 (3720000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0234\n",
      "12 episodes - episode_reward: 19.500 [10.000, 31.000] - loss: 0.016 - mae: 2.624 - mean_q: 3.159 - mean_eps: 0.100 - ale.lives: 1.975\n",
      "\n",
      "Interval 374 (3730000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 20.000 [4.000, 30.000] - loss: 0.017 - mae: 2.630 - mean_q: 3.166 - mean_eps: 0.100 - ale.lives: 2.253\n",
      "\n",
      "Interval 375 (3740000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 21.455 [10.000, 30.000] - loss: 0.017 - mae: 2.621 - mean_q: 3.155 - mean_eps: 0.100 - ale.lives: 2.207\n",
      "\n",
      "Interval 376 (3750000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 20.273 [7.000, 31.000] - loss: 0.016 - mae: 2.620 - mean_q: 3.154 - mean_eps: 0.100 - ale.lives: 1.994\n",
      "\n",
      "Interval 377 (3760000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0226\n",
      "12 episodes - episode_reward: 18.167 [8.000, 34.000] - loss: 0.017 - mae: 2.597 - mean_q: 3.127 - mean_eps: 0.100 - ale.lives: 2.102\n",
      "\n",
      "Interval 378 (3770000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0223\n",
      "9 episodes - episode_reward: 22.778 [12.000, 35.000] - loss: 0.017 - mae: 2.595 - mean_q: 3.124 - mean_eps: 0.100 - ale.lives: 2.063\n",
      "\n",
      "Interval 379 (3780000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 20.455 [7.000, 31.000] - loss: 0.017 - mae: 2.609 - mean_q: 3.141 - mean_eps: 0.100 - ale.lives: 2.127\n",
      "\n",
      "Interval 380 (3790000 steps performed)\n",
      "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0197\n",
      "12 episodes - episode_reward: 16.750 [5.000, 29.000] - loss: 0.016 - mae: 2.585 - mean_q: 3.112 - mean_eps: 0.100 - ale.lives: 2.090\n",
      "\n",
      "Interval 381 (3800000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0208\n",
      "13 episodes - episode_reward: 16.154 [4.000, 28.000] - loss: 0.017 - mae: 2.578 - mean_q: 3.102 - mean_eps: 0.100 - ale.lives: 1.974\n",
      "\n",
      "Interval 382 (3810000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.417 [10.000, 33.000] - loss: 0.017 - mae: 2.573 - mean_q: 3.097 - mean_eps: 0.100 - ale.lives: 2.104\n",
      "\n",
      "Interval 383 (3820000 steps performed)\n",
      "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0217\n",
      "13 episodes - episode_reward: 16.462 [9.000, 31.000] - loss: 0.017 - mae: 2.591 - mean_q: 3.119 - mean_eps: 0.100 - ale.lives: 2.171\n",
      "\n",
      "Interval 384 (3830000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0211\n",
      "13 episodes - episode_reward: 16.385 [8.000, 27.000] - loss: 0.017 - mae: 2.584 - mean_q: 3.109 - mean_eps: 0.100 - ale.lives: 2.020\n",
      "\n",
      "Interval 385 (3840000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0199\n",
      "13 episodes - episode_reward: 15.538 [9.000, 23.000] - loss: 0.016 - mae: 2.586 - mean_q: 3.113 - mean_eps: 0.100 - ale.lives: 2.184\n",
      "\n",
      "Interval 386 (3850000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0209\n",
      "12 episodes - episode_reward: 17.667 [10.000, 29.000] - loss: 0.016 - mae: 2.581 - mean_q: 3.107 - mean_eps: 0.100 - ale.lives: 2.296\n",
      "\n",
      "Interval 387 (3860000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 16.167 [4.000, 22.000] - loss: 0.018 - mae: 2.579 - mean_q: 3.104 - mean_eps: 0.100 - ale.lives: 2.166\n",
      "\n",
      "Interval 388 (3870000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0203\n",
      "14 episodes - episode_reward: 15.357 [4.000, 27.000] - loss: 0.017 - mae: 2.573 - mean_q: 3.096 - mean_eps: 0.100 - ale.lives: 2.108\n",
      "\n",
      "Interval 389 (3880000 steps performed)\n",
      "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0231\n",
      "11 episodes - episode_reward: 22.091 [9.000, 34.000] - loss: 0.017 - mae: 2.583 - mean_q: 3.109 - mean_eps: 0.100 - ale.lives: 2.277\n",
      "\n",
      "Interval 390 (3890000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0205\n",
      "12 episodes - episode_reward: 17.083 [9.000, 31.000] - loss: 0.017 - mae: 2.571 - mean_q: 3.095 - mean_eps: 0.100 - ale.lives: 2.122\n",
      "\n",
      "Interval 391 (3900000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0207\n",
      "9 episodes - episode_reward: 21.778 [10.000, 32.000] - loss: 0.017 - mae: 2.555 - mean_q: 3.076 - mean_eps: 0.100 - ale.lives: 1.854\n",
      "\n",
      "Interval 392 (3910000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0202\n",
      "12 episodes - episode_reward: 16.667 [9.000, 24.000] - loss: 0.017 - mae: 2.551 - mean_q: 3.070 - mean_eps: 0.100 - ale.lives: 2.141\n",
      "\n",
      "Interval 393 (3920000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 17.250 [9.000, 30.000] - loss: 0.017 - mae: 2.558 - mean_q: 3.078 - mean_eps: 0.100 - ale.lives: 2.164\n",
      "\n",
      "Interval 394 (3930000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0197\n",
      "13 episodes - episode_reward: 16.077 [8.000, 24.000] - loss: 0.017 - mae: 2.559 - mean_q: 3.080 - mean_eps: 0.100 - ale.lives: 2.084\n",
      "\n",
      "Interval 395 (3940000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0229\n",
      "11 episodes - episode_reward: 18.909 [7.000, 31.000] - loss: 0.018 - mae: 2.557 - mean_q: 3.077 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 396 (3950000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0218\n",
      "12 episodes - episode_reward: 18.833 [7.000, 28.000] - loss: 0.018 - mae: 2.553 - mean_q: 3.072 - mean_eps: 0.100 - ale.lives: 2.072\n",
      "\n",
      "Interval 397 (3960000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 19.250 [6.000, 31.000] - loss: 0.017 - mae: 2.569 - mean_q: 3.091 - mean_eps: 0.100 - ale.lives: 2.086\n",
      "\n",
      "Interval 398 (3970000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 19.273 [5.000, 30.000] - loss: 0.017 - mae: 2.570 - mean_q: 3.094 - mean_eps: 0.100 - ale.lives: 2.097\n",
      "\n",
      "Interval 399 (3980000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0229\n",
      "11 episodes - episode_reward: 21.182 [9.000, 34.000] - loss: 0.017 - mae: 2.574 - mean_q: 3.100 - mean_eps: 0.100 - ale.lives: 2.258\n",
      "\n",
      "Interval 400 (3990000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0218\n",
      "13 episodes - episode_reward: 17.615 [10.000, 28.000] - loss: 0.017 - mae: 2.587 - mean_q: 3.114 - mean_eps: 0.100 - ale.lives: 2.153\n",
      "\n",
      "Interval 401 (4000000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 18.167 [9.000, 30.000] - loss: 0.017 - mae: 2.595 - mean_q: 3.124 - mean_eps: 0.100 - ale.lives: 2.046\n",
      "\n",
      "Interval 402 (4010000 steps performed)\n",
      "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 16.385 [9.000, 23.000] - loss: 0.017 - mae: 2.603 - mean_q: 3.134 - mean_eps: 0.100 - ale.lives: 2.028\n",
      "\n",
      "Interval 403 (4020000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0203\n",
      "12 episodes - episode_reward: 16.833 [5.000, 30.000] - loss: 0.017 - mae: 2.610 - mean_q: 3.142 - mean_eps: 0.100 - ale.lives: 1.980\n",
      "\n",
      "Interval 404 (4030000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0188\n",
      "14 episodes - episode_reward: 13.786 [7.000, 25.000] - loss: 0.017 - mae: 2.592 - mean_q: 3.119 - mean_eps: 0.100 - ale.lives: 2.101\n",
      "\n",
      "Interval 405 (4040000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0238\n",
      "10 episodes - episode_reward: 21.500 [12.000, 32.000] - loss: 0.016 - mae: 2.578 - mean_q: 3.102 - mean_eps: 0.100 - ale.lives: 1.971\n",
      "\n",
      "Interval 406 (4050000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0209\n",
      "12 episodes - episode_reward: 19.167 [6.000, 33.000] - loss: 0.016 - mae: 2.572 - mean_q: 3.096 - mean_eps: 0.100 - ale.lives: 1.973\n",
      "\n",
      "Interval 407 (4060000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0202\n",
      "11 episodes - episode_reward: 17.636 [4.000, 32.000] - loss: 0.017 - mae: 2.578 - mean_q: 3.104 - mean_eps: 0.100 - ale.lives: 2.128\n",
      "\n",
      "Interval 408 (4070000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0220\n",
      "11 episodes - episode_reward: 19.909 [7.000, 31.000] - loss: 0.017 - mae: 2.570 - mean_q: 3.095 - mean_eps: 0.100 - ale.lives: 2.214\n",
      "\n",
      "Interval 409 (4080000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0216\n",
      "10 episodes - episode_reward: 21.500 [11.000, 34.000] - loss: 0.017 - mae: 2.540 - mean_q: 3.058 - mean_eps: 0.100 - ale.lives: 2.232\n",
      "\n",
      "Interval 410 (4090000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 20.273 [10.000, 29.000] - loss: 0.017 - mae: 2.538 - mean_q: 3.056 - mean_eps: 0.100 - ale.lives: 2.041\n",
      "\n",
      "Interval 411 (4100000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0213\n",
      "11 episodes - episode_reward: 17.091 [12.000, 25.000] - loss: 0.017 - mae: 2.545 - mean_q: 3.064 - mean_eps: 0.100 - ale.lives: 1.997\n",
      "\n",
      "Interval 412 (4110000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0200\n",
      "14 episodes - episode_reward: 16.429 [10.000, 32.000] - loss: 0.018 - mae: 2.562 - mean_q: 3.084 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 413 (4120000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 16.417 [8.000, 25.000] - loss: 0.017 - mae: 2.542 - mean_q: 3.060 - mean_eps: 0.100 - ale.lives: 2.149\n",
      "\n",
      "Interval 414 (4130000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0219\n",
      "10 episodes - episode_reward: 22.100 [10.000, 30.000] - loss: 0.017 - mae: 2.572 - mean_q: 3.095 - mean_eps: 0.100 - ale.lives: 2.210\n",
      "\n",
      "Interval 415 (4140000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 18.615 [9.000, 34.000] - loss: 0.017 - mae: 2.561 - mean_q: 3.083 - mean_eps: 0.100 - ale.lives: 2.114\n",
      "\n",
      "Interval 416 (4150000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 15.692 [6.000, 28.000] - loss: 0.017 - mae: 2.552 - mean_q: 3.072 - mean_eps: 0.100 - ale.lives: 2.069\n",
      "\n",
      "Interval 417 (4160000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 20.500 [8.000, 28.000] - loss: 0.017 - mae: 2.543 - mean_q: 3.061 - mean_eps: 0.100 - ale.lives: 2.084\n",
      "\n",
      "Interval 418 (4170000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 17.917 [11.000, 28.000] - loss: 0.016 - mae: 2.550 - mean_q: 3.070 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 419 (4180000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0235\n",
      "12 episodes - episode_reward: 19.750 [7.000, 29.000] - loss: 0.017 - mae: 2.564 - mean_q: 3.087 - mean_eps: 0.100 - ale.lives: 2.121\n",
      "\n",
      "Interval 420 (4190000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0206\n",
      "12 episodes - episode_reward: 16.417 [8.000, 27.000] - loss: 0.017 - mae: 2.572 - mean_q: 3.096 - mean_eps: 0.100 - ale.lives: 2.132\n",
      "\n",
      "Interval 421 (4200000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0205\n",
      "11 episodes - episode_reward: 19.727 [9.000, 35.000] - loss: 0.017 - mae: 2.577 - mean_q: 3.102 - mean_eps: 0.100 - ale.lives: 2.102\n",
      "\n",
      "Interval 422 (4210000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0206\n",
      "11 episodes - episode_reward: 18.727 [7.000, 28.000] - loss: 0.017 - mae: 2.576 - mean_q: 3.100 - mean_eps: 0.100 - ale.lives: 2.153\n",
      "\n",
      "Interval 423 (4220000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0204\n",
      "12 episodes - episode_reward: 15.750 [6.000, 27.000] - loss: 0.017 - mae: 2.574 - mean_q: 3.097 - mean_eps: 0.100 - ale.lives: 2.081\n",
      "\n",
      "Interval 424 (4230000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 19.545 [12.000, 36.000] - loss: 0.016 - mae: 2.577 - mean_q: 3.101 - mean_eps: 0.100 - ale.lives: 2.115\n",
      "\n",
      "Interval 425 (4240000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0209\n",
      "12 episodes - episode_reward: 18.083 [7.000, 32.000] - loss: 0.016 - mae: 2.552 - mean_q: 3.071 - mean_eps: 0.100 - ale.lives: 2.035\n",
      "\n",
      "Interval 426 (4250000 steps performed)\n",
      "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 18.167 [10.000, 30.000] - loss: 0.016 - mae: 2.553 - mean_q: 3.073 - mean_eps: 0.100 - ale.lives: 2.015\n",
      "\n",
      "Interval 427 (4260000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 16.077 [6.000, 26.000] - loss: 0.017 - mae: 2.576 - mean_q: 3.101 - mean_eps: 0.100 - ale.lives: 2.172\n",
      "\n",
      "Interval 428 (4270000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0188\n",
      "12 episodes - episode_reward: 14.500 [3.000, 29.000] - loss: 0.017 - mae: 2.593 - mean_q: 3.121 - mean_eps: 0.100 - ale.lives: 1.939\n",
      "\n",
      "Interval 429 (4280000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0207\n",
      "15 episodes - episode_reward: 14.267 [5.000, 26.000] - loss: 0.016 - mae: 2.581 - mean_q: 3.106 - mean_eps: 0.100 - ale.lives: 2.013\n",
      "\n",
      "Interval 430 (4290000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0226\n",
      "12 episodes - episode_reward: 18.917 [9.000, 28.000] - loss: 0.017 - mae: 2.586 - mean_q: 3.112 - mean_eps: 0.100 - ale.lives: 2.040\n",
      "\n",
      "Interval 431 (4300000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 16.462 [5.000, 24.000] - loss: 0.017 - mae: 2.581 - mean_q: 3.106 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 432 (4310000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 16.000 [6.000, 24.000] - loss: 0.017 - mae: 2.601 - mean_q: 3.131 - mean_eps: 0.100 - ale.lives: 2.054\n",
      "\n",
      "Interval 433 (4320000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 19.273 [10.000, 32.000] - loss: 0.017 - mae: 2.571 - mean_q: 3.093 - mean_eps: 0.100 - ale.lives: 2.036\n",
      "\n",
      "Interval 434 (4330000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0217\n",
      "12 episodes - episode_reward: 19.167 [10.000, 29.000] - loss: 0.017 - mae: 2.575 - mean_q: 3.098 - mean_eps: 0.100 - ale.lives: 2.225\n",
      "\n",
      "Interval 435 (4340000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 22.727 [10.000, 33.000] - loss: 0.017 - mae: 2.580 - mean_q: 3.104 - mean_eps: 0.100 - ale.lives: 2.292\n",
      "\n",
      "Interval 436 (4350000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 16.750 [8.000, 28.000] - loss: 0.017 - mae: 2.558 - mean_q: 3.078 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 437 (4360000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 19.182 [5.000, 25.000] - loss: 0.017 - mae: 2.543 - mean_q: 3.059 - mean_eps: 0.100 - ale.lives: 2.224\n",
      "\n",
      "Interval 438 (4370000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0207\n",
      "12 episodes - episode_reward: 19.333 [8.000, 29.000] - loss: 0.017 - mae: 2.525 - mean_q: 3.039 - mean_eps: 0.100 - ale.lives: 2.066\n",
      "\n",
      "Interval 439 (4380000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0217\n",
      "10 episodes - episode_reward: 19.000 [8.000, 33.000] - loss: 0.016 - mae: 2.521 - mean_q: 3.034 - mean_eps: 0.100 - ale.lives: 1.967\n",
      "\n",
      "Interval 440 (4390000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0223\n",
      "13 episodes - episode_reward: 19.231 [9.000, 28.000] - loss: 0.016 - mae: 2.517 - mean_q: 3.030 - mean_eps: 0.100 - ale.lives: 2.173\n",
      "\n",
      "Interval 441 (4400000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0232\n",
      "10 episodes - episode_reward: 21.400 [6.000, 32.000] - loss: 0.015 - mae: 2.508 - mean_q: 3.020 - mean_eps: 0.100 - ale.lives: 2.087\n",
      "\n",
      "Interval 442 (4410000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0198\n",
      "12 episodes - episode_reward: 17.833 [6.000, 26.000] - loss: 0.016 - mae: 2.516 - mean_q: 3.027 - mean_eps: 0.100 - ale.lives: 2.009\n",
      "\n",
      "Interval 443 (4420000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 16.833 [6.000, 24.000] - loss: 0.016 - mae: 2.502 - mean_q: 3.010 - mean_eps: 0.100 - ale.lives: 1.928\n",
      "\n",
      "Interval 444 (4430000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0193\n",
      "11 episodes - episode_reward: 18.455 [7.000, 34.000] - loss: 0.016 - mae: 2.508 - mean_q: 3.016 - mean_eps: 0.100 - ale.lives: 1.793\n",
      "\n",
      "Interval 445 (4440000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0200\n",
      "12 episodes - episode_reward: 16.833 [7.000, 24.000] - loss: 0.016 - mae: 2.496 - mean_q: 3.003 - mean_eps: 0.100 - ale.lives: 2.093\n",
      "\n",
      "Interval 446 (4450000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0205\n",
      "11 episodes - episode_reward: 18.727 [10.000, 29.000] - loss: 0.016 - mae: 2.500 - mean_q: 3.008 - mean_eps: 0.100 - ale.lives: 1.976\n",
      "\n",
      "Interval 447 (4460000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 18.417 [7.000, 30.000] - loss: 0.016 - mae: 2.527 - mean_q: 3.042 - mean_eps: 0.100 - ale.lives: 2.168\n",
      "\n",
      "Interval 448 (4470000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0198\n",
      "12 episodes - episode_reward: 15.750 [4.000, 28.000] - loss: 0.016 - mae: 2.526 - mean_q: 3.040 - mean_eps: 0.100 - ale.lives: 1.852\n",
      "\n",
      "Interval 449 (4480000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0214\n",
      "10 episodes - episode_reward: 21.500 [11.000, 33.000] - loss: 0.016 - mae: 2.530 - mean_q: 3.045 - mean_eps: 0.100 - ale.lives: 2.171\n",
      "\n",
      "Interval 450 (4490000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0217\n",
      "13 episodes - episode_reward: 16.692 [6.000, 26.000] - loss: 0.017 - mae: 2.520 - mean_q: 3.032 - mean_eps: 0.100 - ale.lives: 2.081\n",
      "\n",
      "Interval 451 (4500000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 19.909 [7.000, 33.000] - loss: 0.017 - mae: 2.524 - mean_q: 3.038 - mean_eps: 0.100 - ale.lives: 2.238\n",
      "\n",
      "Interval 452 (4510000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0215\n",
      "11 episodes - episode_reward: 19.909 [6.000, 29.000] - loss: 0.017 - mae: 2.527 - mean_q: 3.041 - mean_eps: 0.100 - ale.lives: 2.222\n",
      "\n",
      "Interval 453 (4520000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0206\n",
      "11 episodes - episode_reward: 16.818 [4.000, 35.000] - loss: 0.016 - mae: 2.536 - mean_q: 3.051 - mean_eps: 0.100 - ale.lives: 2.159\n",
      "\n",
      "Interval 454 (4530000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0200\n",
      "11 episodes - episode_reward: 20.182 [12.000, 33.000] - loss: 0.016 - mae: 2.530 - mean_q: 3.045 - mean_eps: 0.100 - ale.lives: 2.100\n",
      "\n",
      "Interval 455 (4540000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0218\n",
      "13 episodes - episode_reward: 16.692 [9.000, 24.000] - loss: 0.016 - mae: 2.536 - mean_q: 3.053 - mean_eps: 0.100 - ale.lives: 2.053\n",
      "\n",
      "Interval 456 (4550000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0214\n",
      "11 episodes - episode_reward: 19.000 [5.000, 30.000] - loss: 0.016 - mae: 2.537 - mean_q: 3.053 - mean_eps: 0.100 - ale.lives: 1.995\n",
      "\n",
      "Interval 457 (4560000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0207\n",
      "12 episodes - episode_reward: 17.333 [7.000, 24.000] - loss: 0.016 - mae: 2.525 - mean_q: 3.039 - mean_eps: 0.100 - ale.lives: 1.987\n",
      "\n",
      "Interval 458 (4570000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0218\n",
      "12 episodes - episode_reward: 17.417 [7.000, 29.000] - loss: 0.016 - mae: 2.521 - mean_q: 3.035 - mean_eps: 0.100 - ale.lives: 2.022\n",
      "\n",
      "Interval 459 (4580000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 17.417 [8.000, 30.000] - loss: 0.016 - mae: 2.537 - mean_q: 3.054 - mean_eps: 0.100 - ale.lives: 1.997\n",
      "\n",
      "Interval 460 (4590000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 22.727 [12.000, 31.000] - loss: 0.016 - mae: 2.533 - mean_q: 3.050 - mean_eps: 0.100 - ale.lives: 2.106\n",
      "\n",
      "Interval 461 (4600000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 20.455 [12.000, 31.000] - loss: 0.016 - mae: 2.523 - mean_q: 3.038 - mean_eps: 0.100 - ale.lives: 1.871\n",
      "\n",
      "Interval 462 (4610000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0199\n",
      "12 episodes - episode_reward: 17.583 [6.000, 27.000] - loss: 0.016 - mae: 2.520 - mean_q: 3.035 - mean_eps: 0.100 - ale.lives: 2.015\n",
      "\n",
      "Interval 463 (4620000 steps performed)\n",
      "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 15.538 [6.000, 24.000] - loss: 0.016 - mae: 2.520 - mean_q: 3.034 - mean_eps: 0.100 - ale.lives: 2.111\n",
      "\n",
      "Interval 464 (4630000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 16.000 [8.000, 24.000] - loss: 0.016 - mae: 2.520 - mean_q: 3.034 - mean_eps: 0.100 - ale.lives: 2.033\n",
      "\n",
      "Interval 465 (4640000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 20.727 [13.000, 33.000] - loss: 0.016 - mae: 2.506 - mean_q: 3.017 - mean_eps: 0.100 - ale.lives: 2.183\n",
      "\n",
      "Interval 466 (4650000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 19.818 [8.000, 29.000] - loss: 0.016 - mae: 2.519 - mean_q: 3.032 - mean_eps: 0.100 - ale.lives: 1.975\n",
      "\n",
      "Interval 467 (4660000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0215\n",
      "13 episodes - episode_reward: 15.846 [3.000, 33.000] - loss: 0.016 - mae: 2.533 - mean_q: 3.048 - mean_eps: 0.100 - ale.lives: 2.159\n",
      "\n",
      "Interval 468 (4670000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0196\n",
      "12 episodes - episode_reward: 17.000 [8.000, 29.000] - loss: 0.016 - mae: 2.533 - mean_q: 3.048 - mean_eps: 0.100 - ale.lives: 2.009\n",
      "\n",
      "Interval 469 (4680000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0230\n",
      "12 episodes - episode_reward: 18.667 [4.000, 28.000] - loss: 0.015 - mae: 2.528 - mean_q: 3.044 - mean_eps: 0.100 - ale.lives: 2.143\n",
      "\n",
      "Interval 470 (4690000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0195\n",
      "12 episodes - episode_reward: 18.000 [4.000, 30.000] - loss: 0.016 - mae: 2.542 - mean_q: 3.059 - mean_eps: 0.100 - ale.lives: 2.008\n",
      "\n",
      "Interval 471 (4700000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 17.917 [10.000, 30.000] - loss: 0.015 - mae: 2.560 - mean_q: 3.081 - mean_eps: 0.100 - ale.lives: 1.943\n",
      "\n",
      "Interval 472 (4710000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0247\n",
      "11 episodes - episode_reward: 20.273 [8.000, 28.000] - loss: 0.015 - mae: 2.559 - mean_q: 3.081 - mean_eps: 0.100 - ale.lives: 2.075\n",
      "\n",
      "Interval 473 (4720000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 18.154 [8.000, 31.000] - loss: 0.016 - mae: 2.556 - mean_q: 3.078 - mean_eps: 0.100 - ale.lives: 2.163\n",
      "\n",
      "Interval 474 (4730000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0208\n",
      "13 episodes - episode_reward: 14.923 [6.000, 23.000] - loss: 0.016 - mae: 2.549 - mean_q: 3.069 - mean_eps: 0.100 - ale.lives: 2.023\n",
      "\n",
      "Interval 475 (4740000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0208\n",
      "13 episodes - episode_reward: 16.692 [7.000, 26.000] - loss: 0.016 - mae: 2.543 - mean_q: 3.060 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 476 (4750000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0208\n",
      "10 episodes - episode_reward: 20.100 [10.000, 34.000] - loss: 0.015 - mae: 2.520 - mean_q: 3.034 - mean_eps: 0.100 - ale.lives: 1.965\n",
      "\n",
      "Interval 477 (4760000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0214\n",
      "11 episodes - episode_reward: 19.455 [4.000, 35.000] - loss: 0.016 - mae: 2.527 - mean_q: 3.042 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 478 (4770000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0229\n",
      "12 episodes - episode_reward: 18.500 [8.000, 36.000] - loss: 0.016 - mae: 2.530 - mean_q: 3.046 - mean_eps: 0.100 - ale.lives: 2.196\n",
      "\n",
      "Interval 479 (4780000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0235\n",
      "11 episodes - episode_reward: 22.545 [7.000, 31.000] - loss: 0.016 - mae: 2.526 - mean_q: 3.041 - mean_eps: 0.100 - ale.lives: 2.054\n",
      "\n",
      "Interval 480 (4790000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 18.727 [1.000, 33.000] - loss: 0.015 - mae: 2.538 - mean_q: 3.055 - mean_eps: 0.100 - ale.lives: 2.082\n",
      "\n",
      "Interval 481 (4800000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 20.083 [11.000, 35.000] - loss: 0.016 - mae: 2.545 - mean_q: 3.064 - mean_eps: 0.100 - ale.lives: 1.944\n",
      "\n",
      "Interval 482 (4810000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0208\n",
      "12 episodes - episode_reward: 17.167 [8.000, 27.000] - loss: 0.016 - mae: 2.545 - mean_q: 3.063 - mean_eps: 0.100 - ale.lives: 2.186\n",
      "\n",
      "Interval 483 (4820000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 18.250 [11.000, 29.000] - loss: 0.016 - mae: 2.533 - mean_q: 3.049 - mean_eps: 0.100 - ale.lives: 2.004\n",
      "\n",
      "Interval 484 (4830000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0235\n",
      "11 episodes - episode_reward: 20.636 [10.000, 32.000] - loss: 0.016 - mae: 2.529 - mean_q: 3.044 - mean_eps: 0.100 - ale.lives: 1.977\n",
      "\n",
      "Interval 485 (4840000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 18.500 [5.000, 35.000] - loss: 0.015 - mae: 2.522 - mean_q: 3.036 - mean_eps: 0.100 - ale.lives: 2.059\n",
      "\n",
      "Interval 486 (4850000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0219\n",
      "10 episodes - episode_reward: 19.500 [9.000, 33.000] - loss: 0.016 - mae: 2.523 - mean_q: 3.038 - mean_eps: 0.100 - ale.lives: 2.109\n",
      "\n",
      "Interval 487 (4860000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0209\n",
      "11 episodes - episode_reward: 20.727 [13.000, 31.000] - loss: 0.015 - mae: 2.527 - mean_q: 3.042 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 488 (4870000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0238\n",
      "11 episodes - episode_reward: 21.636 [16.000, 27.000] - loss: 0.017 - mae: 2.528 - mean_q: 3.043 - mean_eps: 0.100 - ale.lives: 2.007\n",
      "\n",
      "Interval 489 (4880000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0212\n",
      "11 episodes - episode_reward: 19.273 [13.000, 26.000] - loss: 0.015 - mae: 2.516 - mean_q: 3.030 - mean_eps: 0.100 - ale.lives: 2.058\n",
      "\n",
      "Interval 490 (4890000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0212\n",
      "14 episodes - episode_reward: 14.929 [6.000, 32.000] - loss: 0.016 - mae: 2.513 - mean_q: 3.026 - mean_eps: 0.100 - ale.lives: 2.052\n",
      "\n",
      "Interval 491 (4900000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0228\n",
      "12 episodes - episode_reward: 19.083 [12.000, 35.000] - loss: 0.016 - mae: 2.491 - mean_q: 2.999 - mean_eps: 0.100 - ale.lives: 2.000\n",
      "\n",
      "Interval 492 (4910000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0226\n",
      "11 episodes - episode_reward: 19.545 [11.000, 29.000] - loss: 0.015 - mae: 2.503 - mean_q: 3.013 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 493 (4920000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0204\n",
      "11 episodes - episode_reward: 20.182 [8.000, 27.000] - loss: 0.015 - mae: 2.510 - mean_q: 3.022 - mean_eps: 0.100 - ale.lives: 2.067\n",
      "\n",
      "Interval 494 (4930000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0229\n",
      "9 episodes - episode_reward: 23.778 [13.000, 33.000] - loss: 0.015 - mae: 2.498 - mean_q: 3.008 - mean_eps: 0.100 - ale.lives: 2.221\n",
      "\n",
      "Interval 495 (4940000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0231\n",
      "12 episodes - episode_reward: 18.833 [7.000, 30.000] - loss: 0.016 - mae: 2.494 - mean_q: 3.003 - mean_eps: 0.100 - ale.lives: 2.204\n",
      "\n",
      "Interval 496 (4950000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 17.385 [5.000, 31.000] - loss: 0.016 - mae: 2.495 - mean_q: 3.004 - mean_eps: 0.100 - ale.lives: 2.239\n",
      "\n",
      "Interval 497 (4960000 steps performed)\n",
      "10000/10000 [==============================] - 325s 33ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 18.833 [8.000, 31.000] - loss: 0.015 - mae: 2.489 - mean_q: 2.997 - mean_eps: 0.100 - ale.lives: 2.010\n",
      "\n",
      "Interval 498 (4970000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0200\n",
      "12 episodes - episode_reward: 17.000 [9.000, 32.000] - loss: 0.015 - mae: 2.506 - mean_q: 3.017 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 499 (4980000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0213\n",
      "11 episodes - episode_reward: 18.818 [7.000, 28.000] - loss: 0.016 - mae: 2.498 - mean_q: 3.007 - mean_eps: 0.100 - ale.lives: 2.032\n",
      "\n",
      "Interval 500 (4990000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0225\n",
      "done, took 152256.117 seconds\n",
      "\n",
      "=== ENTRENAMIENTO COMPLETADO ===\n",
      "Pesos guardados en: dqn_SpaceInvaders-v0_weights.h5f\n"
     ]
    }
   ],
   "source": [
    "# Configuración de callbacks para guardar progreso\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "\n",
    "callbacks = [\n",
    "    ModelIntervalCheckpoint(checkpoint_weights_filename, interval=500000),\n",
    "    FileLogger(log_filename, interval=100)\n",
    "]\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"\\n=== INICIANDO ENTRENAMIENTO ===\")\n",
    "print(f\"Pasos totales: 5,000,000\")\n",
    "print(f\"Checkpoints se guardarán cada 500,000 pasos\")\n",
    "print(\"\\nPara continuar desde un checkpoint:\")\n",
    "print(\"  dqn.load_weights('dqn_SpaceInvaders-v0_weights_XXXXX.h5f')\")\n",
    "print(\"\\nIniciando...\\n\")\n",
    "\n",
    "history = dqn.fit(\n",
    "    env,\n",
    "    callbacks=callbacks,\n",
    "    nb_steps=5000000,\n",
    "    log_interval=10000,\n",
    "    visualize=False\n",
    ")\n",
    "\n",
    "# Guardar pesos finales\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "print(f\"\\n=== ENTRENAMIENTO COMPLETADO ===\")\n",
    "print(f\"Pesos guardados en: {weights_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoadWeightsSection"
   },
   "source": [
    "---\n",
    "### **Cargar pesos pre-entrenados**\n",
    "\n",
    "Al tener el modelo entrenado, permite cargar los pesos aquí en lugar de entrenar desde cero.\n",
    "Esto es útil para:\n",
    "- Continuar el entrenamiento desde un checkpoint\n",
    "- Evaluar un modelo ya entrenado\n",
    "- Comparar diferentes versiones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoadWeightsCell"
   },
   "outputs": [],
   "source": [
    "# Descomentar para cargar pesos pre-entrenados\n",
    "weights_filename = 'dqn_SpaceInvaders-v0_weights.h5f'\n",
    "dqn.load_weights(weights_filename)\n",
    "print(f\"Pesos cargados desde: {weights_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TestingSection"
   },
   "source": [
    "---\n",
    "### **Evaluación del modelo (Testing)**\n",
    "\n",
    "Esta celda evalúa el rendimiento del agente entrenado en 100 episodios de test.\n",
    "\n",
    "**Objetivo:** Alcanzar **más de 20 puntos durante más de 100 episodios consecutivos**\n",
    "\n",
    "**Criterio de éxito:**\n",
    "- Ejecutar 100 episodios de evaluación\n",
    "- Contar la racha máxima de episodios consecutivos con recompensa >20\n",
    "- Si racha_máxima >= 100 → Objetivo alcanzado ✅\n",
    "\n",
    "**Interpretación de resultados:**\n",
    "- < 50 consecutivos: El agente necesita más entrenamiento\n",
    "- 50-99 consecutivos: El agente está cerca, continuar entrenamiento\n",
    "- >= 100 consecutivos: ¡Objetivo conseguido!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OHYryKd1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUACIÓN DEL MODELO ===\n",
      "Ejecutando 100 episodios de test...\n",
      "\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 726\n",
      "Episode 2: reward: 25.000, steps: 1136\n",
      "Episode 3: reward: 19.000, steps: 1039\n",
      "Episode 4: reward: 10.000, steps: 588\n",
      "Episode 5: reward: 17.000, steps: 838\n",
      "Episode 6: reward: 24.000, steps: 1205\n",
      "Episode 7: reward: 13.000, steps: 767\n",
      "Episode 8: reward: 9.000, steps: 739\n",
      "Episode 9: reward: 14.000, steps: 930\n",
      "Episode 10: reward: 6.000, steps: 442\n",
      "Episode 11: reward: 13.000, steps: 598\n",
      "Episode 12: reward: 26.000, steps: 1263\n",
      "Episode 13: reward: 29.000, steps: 1356\n",
      "Episode 14: reward: 24.000, steps: 1085\n",
      "Episode 15: reward: 20.000, steps: 1048\n",
      "Episode 16: reward: 11.000, steps: 614\n",
      "Episode 17: reward: 26.000, steps: 1154\n",
      "Episode 18: reward: 27.000, steps: 1231\n",
      "Episode 19: reward: 7.000, steps: 567\n",
      "Episode 20: reward: 28.000, steps: 1287\n",
      "Episode 21: reward: 23.000, steps: 776\n",
      "Episode 22: reward: 22.000, steps: 1037\n",
      "Episode 23: reward: 20.000, steps: 932\n",
      "Episode 24: reward: 9.000, steps: 505\n",
      "Episode 25: reward: 18.000, steps: 1233\n",
      "Episode 26: reward: 8.000, steps: 662\n",
      "Episode 27: reward: 25.000, steps: 1256\n",
      "Episode 28: reward: 28.000, steps: 1144\n",
      "Episode 29: reward: 9.000, steps: 689\n",
      "Episode 30: reward: 11.000, steps: 685\n",
      "Episode 31: reward: 12.000, steps: 716\n",
      "Episode 32: reward: 13.000, steps: 676\n",
      "Episode 33: reward: 6.000, steps: 385\n",
      "Episode 34: reward: 11.000, steps: 539\n",
      "Episode 35: reward: 24.000, steps: 1000\n",
      "Episode 36: reward: 29.000, steps: 1392\n",
      "Episode 37: reward: 16.000, steps: 945\n",
      "Episode 38: reward: 8.000, steps: 506\n",
      "Episode 39: reward: 11.000, steps: 621\n",
      "Episode 40: reward: 21.000, steps: 1154\n",
      "Episode 41: reward: 10.000, steps: 697\n",
      "Episode 42: reward: 6.000, steps: 518\n",
      "Episode 43: reward: 20.000, steps: 1017\n",
      "Episode 44: reward: 9.000, steps: 638\n",
      "Episode 45: reward: 27.000, steps: 1334\n",
      "Episode 46: reward: 29.000, steps: 1698\n",
      "Episode 47: reward: 17.000, steps: 931\n",
      "Episode 48: reward: 19.000, steps: 1005\n",
      "Episode 49: reward: 13.000, steps: 677\n",
      "Episode 50: reward: 20.000, steps: 1035\n",
      "Episode 51: reward: 11.000, steps: 653\n",
      "Episode 52: reward: 11.000, steps: 556\n",
      "Episode 53: reward: 15.000, steps: 920\n",
      "Episode 54: reward: 10.000, steps: 590\n",
      "Episode 55: reward: 24.000, steps: 1306\n",
      "Episode 56: reward: 28.000, steps: 1521\n",
      "Episode 57: reward: 23.000, steps: 1096\n",
      "Episode 58: reward: 20.000, steps: 867\n",
      "Episode 59: reward: 12.000, steps: 648\n",
      "Episode 60: reward: 9.000, steps: 524\n",
      "Episode 61: reward: 11.000, steps: 630\n",
      "Episode 62: reward: 20.000, steps: 1050\n",
      "Episode 63: reward: 16.000, steps: 712\n",
      "Episode 64: reward: 17.000, steps: 833\n",
      "Episode 65: reward: 9.000, steps: 484\n",
      "Episode 66: reward: 22.000, steps: 1065\n",
      "Episode 67: reward: 15.000, steps: 793\n",
      "Episode 68: reward: 21.000, steps: 1461\n",
      "Episode 69: reward: 17.000, steps: 701\n",
      "Episode 70: reward: 9.000, steps: 594\n",
      "Episode 71: reward: 13.000, steps: 651\n",
      "Episode 72: reward: 11.000, steps: 565\n",
      "Episode 73: reward: 6.000, steps: 492\n",
      "Episode 74: reward: 12.000, steps: 784\n",
      "Episode 75: reward: 13.000, steps: 856\n",
      "Episode 76: reward: 14.000, steps: 718\n",
      "Episode 77: reward: 15.000, steps: 733\n",
      "Episode 78: reward: 17.000, steps: 700\n",
      "Episode 79: reward: 15.000, steps: 911\n",
      "Episode 80: reward: 8.000, steps: 522\n",
      "Episode 81: reward: 12.000, steps: 652\n",
      "Episode 82: reward: 9.000, steps: 630\n",
      "Episode 83: reward: 16.000, steps: 802\n",
      "Episode 84: reward: 25.000, steps: 1044\n",
      "Episode 85: reward: 11.000, steps: 649\n",
      "Episode 86: reward: 13.000, steps: 940\n",
      "Episode 87: reward: 15.000, steps: 955\n",
      "Episode 88: reward: 12.000, steps: 934\n",
      "Episode 89: reward: 23.000, steps: 1123\n",
      "Episode 90: reward: 12.000, steps: 796\n",
      "Episode 91: reward: 14.000, steps: 763\n",
      "Episode 92: reward: 9.000, steps: 620\n",
      "Episode 93: reward: 24.000, steps: 1021\n",
      "Episode 94: reward: 15.000, steps: 1033\n",
      "Episode 95: reward: 26.000, steps: 1423\n",
      "Episode 96: reward: 22.000, steps: 1224\n",
      "Episode 97: reward: 10.000, steps: 518\n",
      "Episode 98: reward: 9.000, steps: 544\n",
      "Episode 99: reward: 18.000, steps: 918\n",
      "Episode 100: reward: 14.000, steps: 786\n",
      "\n",
      "=== RESULTADOS ===\n",
      "Recompensa media: 16.10\n",
      "Recompensa mínima: 6.00\n",
      "Recompensa máxima: 29.00\n",
      "Desviación estándar: 6.47\n",
      "\n",
      "Episodios con >20.0 puntos: 27/100\n",
      "Máximo de episodios consecutivos >20.0: 3\n",
      "\n",
      "==================================================\n",
      "❌ OBJETIVO NO ALCANZADO\n",
      "Máximo consecutivo: 3/100 episodios\n",
      "\n",
      "Sugerencias:\n",
      "  - Entrenar por más pasos (recomendado: 4-5M)\n",
      "  - Continuar entrenamiento desde checkpoint actual\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EVALUACIÓN DEL MODELO ===\")\n",
    "print(\"Ejecutando 100 episodios de test...\\n\")\n",
    "\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "test_results = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "\n",
    "rewards = test_results.history['episode_reward']\n",
    "mean_reward = np.mean(rewards)\n",
    "min_reward = np.min(rewards)\n",
    "max_reward = np.max(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "threshold = 20.0\n",
    "consecutive = 0\n",
    "max_consecutive = 0\n",
    "\n",
    "for reward in rewards:\n",
    "    if reward > threshold:\n",
    "        consecutive += 1\n",
    "        max_consecutive = max(max_consecutive, consecutive)\n",
    "    else:\n",
    "        consecutive = 0\n",
    "\n",
    "episodios_exitosos = sum(r > threshold for r in rewards)\n",
    "\n",
    "print(\"\\n=== RESULTADOS ===\")\n",
    "print(f\"Recompensa media: {mean_reward:.2f}\")\n",
    "print(f\"Recompensa mínima: {min_reward:.2f}\")\n",
    "print(f\"Recompensa máxima: {max_reward:.2f}\")\n",
    "print(f\"Desviación estándar: {std_reward:.2f}\")\n",
    "print(f\"\\nEpisodios con >{threshold} puntos: {episodios_exitosos}/100\")\n",
    "print(f\"Máximo de episodios consecutivos >{threshold}: {max_consecutive}\")\n",
    "\n",
    "objetivo_alcanzado = max_consecutive >= 100\n",
    "print(f\"\\n{'='*50}\")\n",
    "if objetivo_alcanzado:\n",
    "    print(f\"✅ OBJETIVO ALCANZADO\")\n",
    "    print(f\"El agente logró >20 puntos durante {max_consecutive} episodios consecutivos\")\n",
    "else:\n",
    "    print(f\"❌ OBJETIVO NO ALCANZADO\")\n",
    "    print(f\"Máximo consecutivo: {max_consecutive}/100 episodios\")\n",
    "    print(f\"\\nSugerencias:\")\n",
    "    print(f\"  - Entrenar por más pasos (recomendado: 4-5M)\")\n",
    "    print(f\"  - Continuar entrenamiento desde checkpoint actual\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VisualizationSection"
   },
   "source": [
    "---\n",
    "### **Visualización del agente (solo local)**\n",
    "\n",
    "Esta celda permite visualizar el agente jugando.\n",
    "**Nota:** Solo funciona en entorno local (puesto en False dado que al ejecutar el Kernel se reinicializa), no en Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VisualizationCell"
   },
   "outputs": [],
   "source": [
    "# Visualización (solo funciona en local)\n",
    "if not IN_COLAB:\n",
    "    print(\"Visualizando 3 episodios...\")\n",
    "    dqn.test(env, nb_episodes=3, visualize=False)\n",
    "else:\n",
    "    print(\"La visualización no está disponible en Google Colab.\")\n",
    "    print(\"Para ver al agente jugar, ejecuta este notebook en local.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "---\n",
    "### **3. Justificación de los parámetros seleccionados y de los resultados obtenidos**\n",
    "\n",
    "#### **3.1. Arquitectura de la Red Neuronal**\n",
    "\n",
    "**Decisión:** Se utilizó la arquitectura CNN descrita en el paper seminal de DQN (Mnih et al., 2015).\n",
    "\n",
    "**Justificación:**\n",
    "- **Capas Convolucionales:** Son ideales para procesar imágenes, ya que preservan la estructura espacial y detectan patrones locales como bordes, objetos y movimientos.\n",
    "- **Estructura progresiva:** Los filtros van de 32→64→64, permitiendo detectar características desde simples (bordes) hasta complejas (naves, disparos).\n",
    "- **Strides decrecientes:** (4→2→1) capturan información a diferentes escalas, desde una vista general hasta detalles finos.\n",
    "- **Capa densa de 512:** Suficientemente grande para combinar todas las características extraídas sin sobreajustar.\n",
    "\n",
    "**Alternativas consideradas:**\n",
    "- Redes más profundas (ResNet): Descartadas por mayor coste computacional sin mejora significativa en Atari.\n",
    "- Menos filtros: Podría reducir capacidad de aprendizaje.\n",
    "- Más filtros: Aumentaría tiempo de entrenamiento sin beneficio claro.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.2. Hiperparámetros del Agente DQN**\n",
    "\n",
    "##### **Learning Rate: 0.00025**\n",
    "- **Justificación:** Valor bajo que asegura convergencia estable. DQN es sensible a learning rates altos que causan oscilaciones.\n",
    "- **Efecto:** Aprendizaje más lento pero más estable.\n",
    "- **Alternativas:** 0.0001 (más estable, más lento) o 0.0005 (más rápido, más inestable).\n",
    "\n",
    "##### **Epsilon-greedy con decaimiento lineal (1.0 → 0.1)**\n",
    "- **Justificación:** Balance exploración-explotación crucial en RL.\n",
    "  - Inicio (ε=1.0): Exploración total, el agente descubre el entorno.\n",
    "  - Final (ε=0.1): Mayormente explota conocimiento, pero mantiene 10% exploración para descubrir mejores estrategias.\n",
    "- **Decaimiento en 1M pasos:** Permite suficiente tiempo de exploración inicial.\n",
    "- **ε test (0.05):** Pequeña exploración en evaluación para manejar situaciones nuevas.\n",
    "\n",
    "##### **Memoria: 1,000,000 transiciones**\n",
    "- **Justificación:** \n",
    "  - Rompe correlación temporal entre experiencias consecutivas.\n",
    "  - Permite reutilizar experiencias pasadas (sample efficiency).\n",
    "  - 1M es suficiente para juegos Atari (balance memoria/diversidad).\n",
    "- **Alternativas:** Memorias más pequeñas (500K) funcionan pero con menor estabilidad.\n",
    "\n",
    "##### **Target Network Update: cada 10,000 pasos**\n",
    "- **Justificación:**\n",
    "  - La red objetivo (target network) proporciona valores Q estables para calcular el error.\n",
    "  - 10K pasos: balance entre estabilidad (updates poco frecuentes) y adaptación (updates no muy espaciados).\n",
    "- **Efecto:** Reduce oscilaciones en el aprendizaje.\n",
    "\n",
    "##### **Warmup: 50,000 pasos**\n",
    "- **Justificación:**\n",
    "  - Acumula experiencias diversas antes de entrenar.\n",
    "  - Previene sobreajuste inicial a experiencias limitadas.\n",
    "  - 50K pasos proporcionan suficiente diversidad inicial.\n",
    "\n",
    "##### **Gamma (γ): 0.99**\n",
    "- **Justificación:**\n",
    "  - Factor de descuento que valora recompensas futuras.\n",
    "  - 0.99 es estándar para tareas con horizonte largo (Atari tiene episodios largos).\n",
    "  - Valores más bajos (0.9) harían al agente miope, valores más altos (0.999) podrían causar inestabilidad.\n",
    "\n",
    "##### **Train Interval: cada 4 acciones**\n",
    "- **Justificación:**\n",
    "  - Reduce correlación entre actualizaciones sucesivas.\n",
    "  - Balance entre eficiencia computacional y frecuencia de actualización.\n",
    "  - Valor estándar en implementaciones DQN para Atari.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.3. Análisis de Resultados**\n",
    "\n",
    "**Escribir aquí el análisis después de entrenar:**\n",
    "\n",
    "```\n",
    "RECOMPENSA MEDIA OBTENIDA: [COMPLETAR]\n",
    "\n",
    "ANÁLISIS:\n",
    "1. Evolución del aprendizaje:\n",
    "   - ¿Cómo evolucionó la recompensa durante el entrenamiento?\n",
    "   - ¿Hubo momentos de mejora/estancamiento?\n",
    "   - ¿Cuándo empezó a converger?\n",
    "\n",
    "2. Comportamiento del agente:\n",
    "   - ¿Qué estrategias aprendió? (ej: esquivar, disparar, protegerse)\n",
    "   - ¿Hay comportamientos subóptimos?\n",
    "\n",
    "3. Métricas observadas:\n",
    "   - Evolución de mean_q (debería aumentar)\n",
    "   - Evolución de loss (debería estabilizarse)\n",
    "   - Epsilon decay (de 1.0 a 0.1)\n",
    "\n",
    "4. Comparación con objetivo:\n",
    "   - ¿Se alcanzó el umbral de 20 puntos?\n",
    "   - Si no: ¿Qué ajustes propondrías?\n",
    "   - Si sí: ¿Qué factores fueron clave?\n",
    "\n",
    "5. Posibles mejoras:\n",
    "   - Double DQN (reduce sobreestimación de Q-values)\n",
    "   - Dueling DQN (separa valor del estado y ventaja de acciones)\n",
    "   - Prioritized Experience Replay (muestrea experiencias importantes más frecuentemente)\n",
    "   - Entrenar por más pasos (2-3M pasos)\n",
    "   - Ajustar epsilon decay (más lento/rápido)\n",
    "   - Modificar arquitectura (más capas, diferentes filtros)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.4. Conclusiones**\n",
    "\n",
    "```\n",
    "[COMPLETAR DESPUÉS DEL ENTRENAMIENTO]\n",
    "\n",
    "Resumen:\n",
    "- Objetivo conseguido: SÍ/NO\n",
    "- Principal desafío: [Describir]\n",
    "- Lecciones aprendidas: [Describir]\n",
    "- Trabajo futuro: [Describir posibles extensiones]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.5. Referencias**\n",
    "\n",
    "1. Mnih, V., et al. (2015). \"Human-level control through deep reinforcement learning.\" Nature, 518(7540), 529-533.\n",
    "2. Van Hasselt, H., Guez, A., & Silver, D. (2016). \"Deep reinforcement learning with double Q-learning.\" AAAI.\n",
    "3. Wang, Z., et al. (2016). \"Dueling network architectures for deep reinforcement learning.\" ICML.\n",
    "4. Schaul, T., et al. (2015). \"Prioritized experience replay.\" ICLR.\n",
    "5. keras-rl2 documentation: https://github.com/wau/keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MetricsSection"
   },
   "source": [
    "---\n",
    "## **ANEXO: Análisis de Métricas del Entrenamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar este código para visualizar el progreso del test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Extraer datos disponibles\n",
    "episodes = list(range(1, len(test_results.history['episode_reward']) + 1))\n",
    "rewards = test_results.history['episode_reward']\n",
    "steps = test_results.history['nb_steps']\n",
    "\n",
    "# Calcular estadísticas\n",
    "episodios_exitosos = sum(r > 20 for r in rewards)\n",
    "tasa_exito = (episodios_exitosos / len(rewards)) * 100\n",
    "\n",
    "# Crear subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Recompensas por Episodio (Test)', \n",
    "                    'Steps por Episodio',\n",
    "                    'Distribución de Recompensas', \n",
    "                    'Estadísticas del Test'),\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"histogram\"}, {\"type\": \"table\"}]]  # ← CAMBIO: tabla en lugar de scatter\n",
    ")\n",
    "\n",
    "# 1. Recompensas por episodio\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=episodes, y=rewards, mode='lines+markers', \n",
    "               name='Recompensa', line=dict(color='blue', width=2),\n",
    "               marker=dict(size=10)),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=20, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=\"Objetivo (20)\", row=1, col=1)\n",
    "fig.add_hline(y=np.mean(rewards), line_dash=\"dot\", line_color=\"green\",\n",
    "              annotation_text=f\"Media ({np.mean(rewards):.2f})\", row=1, col=1)\n",
    "\n",
    "# 2. Steps por episodio\n",
    "fig.add_trace(\n",
    "    go.Bar(x=episodes, y=steps, name='Steps', marker_color='orange',\n",
    "           showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Distribución (histograma)\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rewards, nbinsx=8, name='Distribución',\n",
    "                 marker_color='purple', showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Tabla de estadísticas (SOLUCIÓN)\n",
    "fig.add_trace(\n",
    "    go.Table(\n",
    "        header=dict(\n",
    "            values=['<b>Métrica</b>', '<b>Valor</b>'],\n",
    "            fill_color='lightblue',\n",
    "            align='left',\n",
    "            font=dict(size=14, color='black')\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                ['Episodios evaluados', 'Recompensa media', 'Recompensa máxima', \n",
    "                 'Recompensa mínima', 'Desviación estándar', 'Mediana',\n",
    "                 'Steps promedio', '', 'Episodios >20 pts', 'Tasa de éxito', \n",
    "                 '', '<b>Objetivo (μ>20)</b>'],\n",
    "                [f'{len(rewards)}', f'{np.mean(rewards):.2f}', f'{np.max(rewards):.2f}',\n",
    "                 f'{np.min(rewards):.2f}', f'{np.std(rewards):.2f}', f'{np.median(rewards):.2f}',\n",
    "                 f'{np.mean(steps):.0f}', '', f'{episodios_exitosos}/{len(rewards)}', \n",
    "                 f'{tasa_exito:.0f}%', '',\n",
    "                 f'<b>{\"✅ ALCANZADO\" if np.mean(rewards) > 20 else \"❌ NO ALCANZADO\"}</b>']\n",
    "            ],\n",
    "            fill_color=[['white', 'lightgray']*6],\n",
    "            align='left',\n",
    "            font=dict(size=12),\n",
    "            height=30\n",
    "        )\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Actualizar layouts\n",
    "fig.update_xaxes(title_text=\"Episodio\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Episodio\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Recompensa\", row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Recompensa\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Steps\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Frecuencia\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, width=1200,\n",
    "    showlegend=False,\n",
    "    title_text=\"<b>Resultados del Test - DQN SpaceInvaders</b>\",\n",
    "    title_font_size=20\n",
    ")\n",
    "\n",
    "# Guardar\n",
    "fig.write_html('test_results.html')\n",
    "fig.write_image('test_results.png', width=1200, height=800)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Visualización guardada:\")\n",
    "print(\"  - test_results.html (interactivo)\")\n",
    "print(\"  - test_results.png (imagen)\")\n",
    "\n",
    "# Resumen en consola\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANÁLISIS DETALLADO DEL TEST\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n Recompensas por episodio:\")\n",
    "for i, r in enumerate(rewards, 1):\n",
    "    emoji = \"🟢\" if r > 20 else \"🟡\" if r > 10 else \"🔴\"\n",
    "    print(f\"  {emoji} Episodio {i:2d}: {r:5.2f} puntos\")\n",
    "\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\"Estadísticas agregadas:\")\n",
    "print(f\"{'─'*60}\")\n",
    "print(f\"  Recompensa media:      {np.mean(rewards):6.2f}\")\n",
    "print(f\"  Recompensa mediana:    {np.median(rewards):6.2f}\")\n",
    "print(f\"  Recompensa máxima:     {np.max(rewards):6.2f}\")\n",
    "print(f\"  Recompensa mínima:     {np.min(rewards):6.2f}\")\n",
    "print(f\"  Desviación estándar:   {np.std(rewards):6.2f}\")\n",
    "print(f\"  Steps promedio:        {np.mean(steps):6.0f}\")\n",
    "\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\" Evaluación de objetivo:\")\n",
    "print(f\"{'─'*60}\")\n",
    "print(f\"  Episodios >20 puntos:  {episodios_exitosos}/{len(rewards)} ({tasa_exito:.0f}%)\")\n",
    "print(f\"  Media vs objetivo:     {np.mean(rewards):.2f} vs 20.00\")\n",
    "print(f\"  Estado:                {'✅ ALCANZADO' if np.mean(rewards) > 20 else '❌ NO ALCANZADO'}\")\n",
    "\n",
    "if np.mean(rewards) < 20:\n",
    "    deficit = 20 - np.mean(rewards)\n",
    "    print(f\"\\n El modelo necesita mejorar {deficit:.2f} puntos para alcanzar el objetivo\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_results.png\" alt=\"Estadisticas del modelo\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar este código para visualizar el progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuar Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el modelo con los últimos pesos\n",
    "dqn.load_weights('dqn_SpaceInvaders-v0_weights.h5f')\n",
    "print(\" Pesos cargados\")\n",
    "\n",
    "# 2. Continuar entrenando más steps\n",
    "dqn.fit(\n",
    "    env, \n",
    "    nb_steps=500000,  # 500k steps adicionales\n",
    "    visualize=False, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 3. Los nuevos episodios se añadirán automáticamente al log\n",
    "print(\" Entrenamiento continuado completado!\")\n",
    "\n",
    "# 4. Evaluar\n",
    "test_results = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(f\"Media: {np.mean(test_results.history['episode_reward']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (gym)",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
