{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: [Jose Aurelio Bollas Taboada]\n",
    "*   Alumno 2: [Antonio Jose Bonafede Salas]\n",
    "*   Alumno 3: [Elvis David Pachacama Cabezas]\n",
    "*   Alumno 4: [Jose Fernando Sarmiento Sarmiento]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto SpaceInvaders\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.ipynb_checkpoints', '08MIAR_pg_ac.ipynb', 'checkpoint', 'CONFIG_ANTI_COLAPSO_PG.py', 'continuar entrenamiento.zip', 'Corrida 0', 'Corrida 1', 'Corrida 1 PG', 'Corrida 2', 'Corrida 3', 'Corrida 4', 'Corrida 5', 'dqn_SpaceInvaders-v0_log.json', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_3000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_3000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_3500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_3500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_4000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_4000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_4500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_4500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_5000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_5000000.h5f.index', 'MEJORAS_PG_SPACEINVADERS_CODIGO.py', 'ppo_spaceinvaders v0.1.pt', 'ppo_spaceinvaders V0.2.pt', 'ppo_spaceinvaders.pt', 'ppo_spaceinvaders_v2.pt', 'Proyecto_DDQN_SpaceInvaders.ipynb', 'Proyecto_DNDQN_SpaceInvaders.ipynb', 'Proyecto_DQN_SpaceInvaders.ipynb', 'Proyecto_PG_SpaceInvaders V5.0.ipynb', 'Proyecto_PG_SpaceInvaders V6.0.ipynb', 'Proyecto_PPO_SpaceInvaders - V0.2.ipynb', 'Proyecto_PPO_SpaceInvaders - V0.3.ipynb', 'Proyecto_PPO_SpaceInvaders - V1.0 - 5M.ipynb', 'Proyecto_PPO_SpaceInvaders - V1.0.ipynb', 'Proyecto_PPO_SpaceInvaders - V2.0 - 5M.ipynb', 'Proyecto_practico_SpaceInvaders - copia.ipynb', 'Proyecto_practico_SpaceInvaders - Exp1.ipynb', 'Proyecto_practico_SpaceInvaders - Exp2.ipynb', 'Proyecto_practico_SpaceInvaders - Exp2DUE.ipynb', 'Proyecto_practico_SpaceInvaders v0.ipynb', 'Proyecto_practico_SpaceInvaders v1.ipynb', 'Proyecto_practico_SpaceInvaders.ipynb', 'Proyecto_práctico.ipynb', 'Recomendaciones Chatgpt.txt', 'Recomendaciones Claude.txt', 'SOLUCION_1_epsilon_0.py', 'SOLUCION_DEFINITIVA_2M_steps.py', 'spaceinvaders_ppo_episode.gif', 'test_results.html', 'test_results.png', 'training_metrics.html', 'training_metrics.png', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbVRjvHCJ8UF"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de acciones disponibles: 6\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "print(f\"Número de acciones disponibles: {nb_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtariProcessorCell"
   },
   "source": [
    "#### Procesador Atari\n",
    "\n",
    "El procesador Atari se encarga de:\n",
    "1. Preprocesar las observaciones (redimensionar a 84x84 y convertir a escala de grises)\n",
    "2. Normalizar el estado batch (dividir por 255)\n",
    "3. Clipear las recompensas entre -1 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"Procesa cada observación: redimensiona a 84x84 y convierte a escala de grises\"\"\"\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # L = luminance (escala de grises)\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"Normaliza el batch de estados dividiendo por 255\"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Clipea las recompensas entre -1 y 1 para estabilizar el entrenamiento\"\"\"\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "---\n",
    "### **1. Implementación de la red neuronal**\n",
    "\n",
    "Se implementa una red neuronal convolucional (CNN) basada en la arquitectura descrita en el paper de Mnih et al. (2015) \"Human-level control through deep reinforcement learning\".\n",
    "\n",
    "**Arquitectura:**\n",
    "- Capa Permute: Reorganiza las dimensiones de entrada según el formato de Keras\n",
    "- Conv2D #1: 32 filtros, kernel 8x8, stride 4 → Extrae características de bajo nivel\n",
    "- Conv2D #2: 64 filtros, kernel 4x4, stride 2 → Extrae características de nivel medio\n",
    "- Conv2D #3: 64 filtros, kernel 3x3, stride 1 → Refina características\n",
    "- Dense #1: 512 neuronas → Capa completamente conectada\n",
    "- Dense #2: nb_actions neuronas, activación lineal → Salida Q-values para cada acción\n",
    "\n",
    "**Justificación:**\n",
    "- Las capas convolucionales permiten extraer características espaciales relevantes del juego\n",
    "- Los strides progresivamente más pequeños permiten capturar detalles a diferentes escalas\n",
    "- La capa densa de 512 neuronas permite combinar las características extraídas\n",
    "- La activación lineal final es apropiada para estimar Q-values (pueden ser negativos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de datos de imagen de Keras: channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construcción del modelo CNN siguiendo la arquitectura de Mnih et al. (2015)\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "\n",
    "print(f\"Formato de datos de imagen de Keras: {K.image_data_format()}\")\n",
    "\n",
    "# Reorganizar dimensiones según el formato de Keras\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # Formato: (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # Formato: (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_data_format.')\n",
    "\n",
    "# Primera capa convolucional: detecta características básicas (bordes, colores)\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Segunda capa convolucional: detecta patrones más complejos\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Tercera capa convolucional: refina las características detectadas\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Aplanar las características para las capas densas\n",
    "model.add(Flatten())\n",
    "\n",
    "# Capa completamente conectada: combina características\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Capa de salida: un Q-value por cada acción posible\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))  # Lineal porque los Q-values pueden ser negativos\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "---\n",
    "### **2. Implementación de la solución DQN**\n",
    "\n",
    "Se implementan los componentes principales del algoritmo DQN:\n",
    "\n",
    "**Memoria de Experiencia (Experience Replay):**\n",
    "- Tamaño: 1,000,000 transiciones\n",
    "- Window length: 4 frames (captura movimiento)\n",
    "- Permite romper correlaciones temporales y reutilizar experiencias\n",
    "\n",
    "**Política de Exploración:**\n",
    "- Epsilon-greedy con decaimiento lineal\n",
    "- ε inicial: 1.0 (exploración total)\n",
    "- ε final entrenamiento: 0.1 (10% exploración)\n",
    "- ε test: 0.05 (5% exploración)\n",
    "- Pasos de decaimiento: 1,000,000 (decae gradualmente)\n",
    "\n",
    "**Configuración del Agente DQN:**\n",
    "- Warmup: 50,000 pasos (acumula experiencias antes de entrenar)\n",
    "- Gamma (γ): 0.99 (factor de descuento, valora recompensas futuras)\n",
    "- Target model update: 10,000 pasos (actualiza la red objetivo)\n",
    "- Train interval: 4 pasos (entrena cada 4 acciones)\n",
    "- Optimizer: Adam con learning rate 0.00025\n",
    "- Batch size: 32 (por defecto en keras-rl)\n",
    "\n",
    "**Justificación de hiperparámetros:**\n",
    "- Learning rate bajo (0.00025): previene oscilaciones en el aprendizaje\n",
    "- Warmup alto (50,000): asegura suficiente diversidad en la memoria inicial\n",
    "- Target update (10,000): balance entre estabilidad y adaptación\n",
    "- Train interval (4): reduce correlación sin perder mucha información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONFIGURACIÓN DEL AGENTE DQN ===\n",
      "Memoria: 1,000,000 transiciones\n",
      "Window length: 4 frames\n",
      "Política: Epsilon-greedy con decaimiento lineal\n",
      "  - ε inicial: 1.0\n",
      "  - ε final: 0.1\n",
      "  - ε test: 0.05\n",
      "Warmup: 50,000 pasos\n",
      "Gamma (γ): 0.99\n",
      "Target update: cada 10,000 pasos\n",
      "Train interval: cada 4 acciones\n",
      "Learning rate: 0.00025\n"
     ]
    }
   ],
   "source": [
    "# 1. MEMORIA DE EXPERIENCIA (Experience Replay)\n",
    "# Almacena transiciones (s, a, r, s') para romper correlaciones temporales\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "# 2. PROCESADOR\n",
    "# Preprocesa observaciones, estados y recompensas\n",
    "processor = AtariProcessor()\n",
    "\n",
    "# 3. POLÍTICA DE EXPLORACIÓN\n",
    "# Epsilon-greedy con decaimiento lineal: empieza explorando (ε=1.0) \n",
    "# y gradualmente explota (ε=0.1)\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=1.,      # ε inicial: exploración total\n",
    "    value_min=.1,      # ε final: 10% exploración, 90% explotación\n",
    "    value_test=.05,    # ε test: 5% exploración durante evaluación\n",
    "    nb_steps=1000000   # Pasos para decaer de value_max a value_min\n",
    ")\n",
    "\n",
    "# 4. AGENTE DQN\n",
    "# Configuración del algoritmo Deep Q-Network\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    processor=processor,\n",
    "    nb_steps_warmup=50000,      # Pasos de warmup antes de entrenar\n",
    "    gamma=.99,                  # Factor de descuento (importancia futuro)\n",
    "    target_model_update=10000,  # Frecuencia de actualización de red objetivo\n",
    "    train_interval=4,           # Entrena cada 4 acciones\n",
    "    delta_clip=1,               # Clip del error TD para estabilidad,\n",
    "    enable_double_dqn=False,\n",
    "    enable_dueling_network=True\n",
    ")\n",
    "\n",
    "# 5. COMPILACIÓN\n",
    "# Optimizer Adam con learning rate bajo para estabilidad\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n",
    "\n",
    "print(\"\\n=== CONFIGURACIÓN DEL AGENTE DQN ===\")\n",
    "print(f\"Memoria: {memory.limit:,} transiciones\")\n",
    "print(f\"Window length: {WINDOW_LENGTH} frames\")\n",
    "print(f\"Política: Epsilon-greedy con decaimiento lineal\")\n",
    "print(f\"  - ε inicial: {policy.value_max}\")\n",
    "print(f\"  - ε final: {policy.value_min}\")\n",
    "print(f\"  - ε test: {policy.value_test}\")\n",
    "print(f\"Warmup: {dqn.nb_steps_warmup:,} pasos\")\n",
    "print(f\"Gamma (γ): {dqn.gamma}\")\n",
    "print(f\"Target update: cada {dqn.target_model_update:,} pasos\")\n",
    "print(f\"Train interval: cada {dqn.train_interval} acciones\")\n",
    "print(f\"Learning rate: 0.00025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrainingSection"
   },
   "source": [
    "---\n",
    "### **Entrenamiento del agente**\n",
    "\n",
    "**Configuración del entrenamiento:**\n",
    "- Pasos totales: 2,000,000 (suficiente para convergencia en Atari)\n",
    "- Log interval: cada 10,000 pasos (monitorizar progreso)\n",
    "- Checkpoints: cada 500,000 pasos (guardar progreso)\n",
    "- Visualización: desactivada (más rápido)\n",
    "\n",
    "**Métricas a observar durante el entrenamiento:**\n",
    "- episode_reward: recompensa total por episodio (objetivo: >20)\n",
    "- loss: error de predicción de Q-values\n",
    "- mae: error absoluto medio\n",
    "- mean_q: Q-value promedio (debe aumentar con el tiempo)\n",
    "- mean_eps: epsilon actual (debe decrecer)\n",
    "\n",
    "**Nota:** El entrenamiento puede dura varias horas (12-24h en GPU, 36 en CPU).\n",
    "Se recomienda usar GPU y guardar checkpoints para continuar si se interrumpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TrainingCell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INICIANDO ENTRENAMIENTO ===\n",
      "Pasos totales: 5,000,000\n",
      "Checkpoints se guardarán cada 500,000 pasos\n",
      "\n",
      "Para continuar desde un checkpoint:\n",
      "  dqn.load_weights('dqn_SpaceInvaders-v0_weights_XXXXX.h5f')\n",
      "\n",
      "Iniciando...\n",
      "\n",
      "Training for 5000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   52/10000 [..............................] - ETA: 20s - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bonaf\\.conda\\envs\\gym_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0113\n",
      "14 episodes - episode_reward: 8.071 [1.000, 21.000] - ale.lives: 2.073\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0127\n",
      "15 episodes - episode_reward: 8.467 [3.000, 19.000] - ale.lives: 2.092\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0136\n",
      "15 episodes - episode_reward: 9.000 [3.000, 20.000] - ale.lives: 2.127\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0142\n",
      "12 episodes - episode_reward: 10.917 [1.000, 20.000] - ale.lives: 2.101\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0132\n",
      "15 episodes - episode_reward: 9.267 [4.000, 16.000] - ale.lives: 1.946\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 165s 17ms/step - reward: 0.0119\n",
      "14 episodes - episode_reward: 8.071 [3.000, 17.000] - loss: 0.007 - mae: 0.023 - mean_q: 0.027 - mean_eps: 0.951 - ale.lives: 2.047\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 166s 17ms/step - reward: 0.0139\n",
      "16 episodes - episode_reward: 9.375 [5.000, 16.000] - loss: 0.007 - mae: 0.033 - mean_q: 0.040 - mean_eps: 0.942 - ale.lives: 2.130\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 167s 17ms/step - reward: 0.0146\n",
      "11 episodes - episode_reward: 12.909 [6.000, 22.000] - loss: 0.006 - mae: 0.053 - mean_q: 0.067 - mean_eps: 0.933 - ale.lives: 2.240\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 168s 17ms/step - reward: 0.0140\n",
      "14 episodes - episode_reward: 9.929 [4.000, 29.000] - loss: 0.006 - mae: 0.072 - mean_q: 0.092 - mean_eps: 0.924 - ale.lives: 2.186\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 168s 17ms/step - reward: 0.0151\n",
      "14 episodes - episode_reward: 10.857 [6.000, 22.000] - loss: 0.006 - mae: 0.081 - mean_q: 0.102 - mean_eps: 0.915 - ale.lives: 2.082\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 169s 17ms/step - reward: 0.0140\n",
      "12 episodes - episode_reward: 11.500 [4.000, 24.000] - loss: 0.007 - mae: 0.109 - mean_q: 0.136 - mean_eps: 0.906 - ale.lives: 2.117\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0145\n",
      "13 episodes - episode_reward: 10.846 [4.000, 20.000] - loss: 0.007 - mae: 0.134 - mean_q: 0.166 - mean_eps: 0.897 - ale.lives: 2.144\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0143\n",
      "14 episodes - episode_reward: 10.643 [5.000, 32.000] - loss: 0.007 - mae: 0.144 - mean_q: 0.178 - mean_eps: 0.888 - ale.lives: 2.102\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0149\n",
      "14 episodes - episode_reward: 10.857 [2.000, 25.000] - loss: 0.007 - mae: 0.168 - mean_q: 0.208 - mean_eps: 0.879 - ale.lives: 2.084\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0138\n",
      "15 episodes - episode_reward: 8.733 [3.000, 21.000] - loss: 0.007 - mae: 0.195 - mean_q: 0.241 - mean_eps: 0.870 - ale.lives: 2.029\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0145\n",
      "15 episodes - episode_reward: 9.733 [5.000, 21.000] - loss: 0.007 - mae: 0.223 - mean_q: 0.274 - mean_eps: 0.861 - ale.lives: 2.092\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0130\n",
      "13 episodes - episode_reward: 9.615 [1.000, 19.000] - loss: 0.007 - mae: 0.243 - mean_q: 0.298 - mean_eps: 0.852 - ale.lives: 2.085\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0145\n",
      "15 episodes - episode_reward: 10.467 [4.000, 18.000] - loss: 0.007 - mae: 0.271 - mean_q: 0.332 - mean_eps: 0.843 - ale.lives: 2.033\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 10.786 [5.000, 20.000] - loss: 0.007 - mae: 0.301 - mean_q: 0.368 - mean_eps: 0.834 - ale.lives: 2.146\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0149\n",
      "14 episodes - episode_reward: 10.286 [5.000, 21.000] - loss: 0.007 - mae: 0.312 - mean_q: 0.381 - mean_eps: 0.825 - ale.lives: 2.052\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0145\n",
      "17 episodes - episode_reward: 8.647 [5.000, 15.000] - loss: 0.007 - mae: 0.339 - mean_q: 0.414 - mean_eps: 0.816 - ale.lives: 2.082\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0147\n",
      "16 episodes - episode_reward: 9.000 [3.000, 21.000] - loss: 0.007 - mae: 0.354 - mean_q: 0.432 - mean_eps: 0.807 - ale.lives: 2.127\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0148\n",
      "16 episodes - episode_reward: 9.938 [5.000, 21.000] - loss: 0.008 - mae: 0.374 - mean_q: 0.456 - mean_eps: 0.798 - ale.lives: 2.023\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0155\n",
      "16 episodes - episode_reward: 9.500 [4.000, 20.000] - loss: 0.008 - mae: 0.390 - mean_q: 0.476 - mean_eps: 0.789 - ale.lives: 2.073\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0170\n",
      "14 episodes - episode_reward: 11.786 [4.000, 22.000] - loss: 0.008 - mae: 0.416 - mean_q: 0.506 - mean_eps: 0.780 - ale.lives: 2.013\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 182s 18ms/step - reward: 0.0167\n",
      "12 episodes - episode_reward: 14.000 [6.000, 29.000] - loss: 0.008 - mae: 0.433 - mean_q: 0.528 - mean_eps: 0.771 - ale.lives: 2.135\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: 0.0160\n",
      "17 episodes - episode_reward: 9.824 [5.000, 25.000] - loss: 0.009 - mae: 0.468 - mean_q: 0.570 - mean_eps: 0.762 - ale.lives: 2.035\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: 0.0173\n",
      "15 episodes - episode_reward: 11.533 [7.000, 24.000] - loss: 0.009 - mae: 0.483 - mean_q: 0.590 - mean_eps: 0.753 - ale.lives: 2.054\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0157\n",
      "14 episodes - episode_reward: 10.571 [3.000, 18.000] - loss: 0.008 - mae: 0.517 - mean_q: 0.631 - mean_eps: 0.744 - ale.lives: 2.087\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: 0.0163\n",
      "14 episodes - episode_reward: 11.857 [6.000, 21.000] - loss: 0.009 - mae: 0.541 - mean_q: 0.660 - mean_eps: 0.735 - ale.lives: 1.997\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: 0.0168\n",
      "14 episodes - episode_reward: 11.571 [2.000, 22.000] - loss: 0.009 - mae: 0.549 - mean_q: 0.670 - mean_eps: 0.726 - ale.lives: 2.057\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 189s 19ms/step - reward: 0.0166\n",
      "15 episodes - episode_reward: 11.200 [4.000, 19.000] - loss: 0.009 - mae: 0.567 - mean_q: 0.692 - mean_eps: 0.717 - ale.lives: 2.083\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 189s 19ms/step - reward: 0.0181\n",
      "15 episodes - episode_reward: 12.467 [5.000, 23.000] - loss: 0.009 - mae: 0.596 - mean_q: 0.726 - mean_eps: 0.708 - ale.lives: 2.045\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 190s 19ms/step - reward: 0.0187\n",
      "15 episodes - episode_reward: 12.400 [5.000, 25.000] - loss: 0.010 - mae: 0.631 - mean_q: 0.769 - mean_eps: 0.699 - ale.lives: 2.019\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 191s 19ms/step - reward: 0.0176\n",
      "14 episodes - episode_reward: 12.286 [2.000, 25.000] - loss: 0.009 - mae: 0.657 - mean_q: 0.801 - mean_eps: 0.690 - ale.lives: 2.153\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 191s 19ms/step - reward: 0.0175\n",
      "13 episodes - episode_reward: 13.000 [4.000, 35.000] - loss: 0.009 - mae: 0.674 - mean_q: 0.821 - mean_eps: 0.681 - ale.lives: 2.142\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: 0.0197\n",
      "16 episodes - episode_reward: 12.188 [5.000, 25.000] - loss: 0.009 - mae: 0.703 - mean_q: 0.857 - mean_eps: 0.672 - ale.lives: 2.211\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: 0.0195\n",
      "15 episodes - episode_reward: 13.933 [9.000, 21.000] - loss: 0.010 - mae: 0.738 - mean_q: 0.899 - mean_eps: 0.663 - ale.lives: 2.151\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 197s 20ms/step - reward: 0.0183\n",
      "13 episodes - episode_reward: 12.769 [7.000, 23.000] - loss: 0.010 - mae: 0.777 - mean_q: 0.945 - mean_eps: 0.654 - ale.lives: 2.037\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 198s 20ms/step - reward: 0.0199\n",
      "15 episodes - episode_reward: 13.933 [3.000, 23.000] - loss: 0.010 - mae: 0.811 - mean_q: 0.987 - mean_eps: 0.645 - ale.lives: 2.227\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 200s 20ms/step - reward: 0.0196\n",
      "15 episodes - episode_reward: 13.733 [8.000, 26.000] - loss: 0.011 - mae: 0.834 - mean_q: 1.016 - mean_eps: 0.636 - ale.lives: 2.051\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: 0.0192\n",
      "14 episodes - episode_reward: 13.571 [7.000, 22.000] - loss: 0.010 - mae: 0.857 - mean_q: 1.044 - mean_eps: 0.627 - ale.lives: 2.162\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 202s 20ms/step - reward: 0.0211\n",
      "14 episodes - episode_reward: 14.643 [6.000, 31.000] - loss: 0.011 - mae: 0.889 - mean_q: 1.081 - mean_eps: 0.618 - ale.lives: 2.142\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 202s 20ms/step - reward: 0.0186\n",
      "14 episodes - episode_reward: 13.000 [8.000, 23.000] - loss: 0.010 - mae: 0.899 - mean_q: 1.092 - mean_eps: 0.609 - ale.lives: 2.219\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 204s 20ms/step - reward: 0.0202\n",
      "15 episodes - episode_reward: 13.600 [8.000, 25.000] - loss: 0.010 - mae: 0.918 - mean_q: 1.116 - mean_eps: 0.600 - ale.lives: 2.136\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 205s 21ms/step - reward: 0.0199\n",
      "14 episodes - episode_reward: 14.143 [7.000, 24.000] - loss: 0.011 - mae: 0.941 - mean_q: 1.143 - mean_eps: 0.591 - ale.lives: 2.200\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 205s 21ms/step - reward: 0.0217\n",
      "13 episodes - episode_reward: 17.231 [5.000, 27.000] - loss: 0.011 - mae: 0.961 - mean_q: 1.168 - mean_eps: 0.582 - ale.lives: 2.006\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 205s 20ms/step - reward: 0.0200\n",
      "13 episodes - episode_reward: 15.462 [8.000, 29.000] - loss: 0.011 - mae: 0.995 - mean_q: 1.208 - mean_eps: 0.573 - ale.lives: 2.251\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: 0.0192\n",
      "15 episodes - episode_reward: 12.467 [6.000, 20.000] - loss: 0.011 - mae: 1.023 - mean_q: 1.243 - mean_eps: 0.564 - ale.lives: 2.118\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: 0.0206\n",
      "14 episodes - episode_reward: 15.214 [8.000, 23.000] - loss: 0.011 - mae: 1.042 - mean_q: 1.265 - mean_eps: 0.555 - ale.lives: 2.274\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: 0.0192\n",
      "15 episodes - episode_reward: 12.400 [4.000, 25.000] - loss: 0.012 - mae: 1.066 - mean_q: 1.293 - mean_eps: 0.546 - ale.lives: 2.233\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0205\n",
      "13 episodes - episode_reward: 16.077 [9.000, 32.000] - loss: 0.011 - mae: 1.081 - mean_q: 1.311 - mean_eps: 0.537 - ale.lives: 2.272\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0207\n",
      "14 episodes - episode_reward: 14.000 [6.000, 25.000] - loss: 0.011 - mae: 1.106 - mean_q: 1.341 - mean_eps: 0.528 - ale.lives: 2.196\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0207\n",
      "13 episodes - episode_reward: 15.923 [10.000, 27.000] - loss: 0.011 - mae: 1.140 - mean_q: 1.382 - mean_eps: 0.519 - ale.lives: 2.076\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0187\n",
      "14 episodes - episode_reward: 12.786 [7.000, 23.000] - loss: 0.012 - mae: 1.168 - mean_q: 1.416 - mean_eps: 0.510 - ale.lives: 2.150\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 17.538 [7.000, 35.000] - loss: 0.012 - mae: 1.206 - mean_q: 1.461 - mean_eps: 0.501 - ale.lives: 2.236\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0198\n",
      "15 episodes - episode_reward: 12.400 [8.000, 20.000] - loss: 0.013 - mae: 1.240 - mean_q: 1.502 - mean_eps: 0.492 - ale.lives: 2.143\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0212\n",
      "14 episodes - episode_reward: 17.000 [10.000, 30.000] - loss: 0.013 - mae: 1.254 - mean_q: 1.519 - mean_eps: 0.483 - ale.lives: 2.116\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: 0.0198\n",
      "13 episodes - episode_reward: 14.615 [10.000, 33.000] - loss: 0.012 - mae: 1.282 - mean_q: 1.553 - mean_eps: 0.474 - ale.lives: 2.122\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 15.769 [7.000, 31.000] - loss: 0.013 - mae: 1.316 - mean_q: 1.593 - mean_eps: 0.465 - ale.lives: 2.067\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0210\n",
      "15 episodes - episode_reward: 14.200 [8.000, 27.000] - loss: 0.012 - mae: 1.335 - mean_q: 1.616 - mean_eps: 0.456 - ale.lives: 2.089\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0201\n",
      "16 episodes - episode_reward: 12.562 [4.000, 23.000] - loss: 0.013 - mae: 1.366 - mean_q: 1.653 - mean_eps: 0.447 - ale.lives: 2.138\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.583 [12.000, 26.000] - loss: 0.013 - mae: 1.386 - mean_q: 1.676 - mean_eps: 0.438 - ale.lives: 2.122\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0202\n",
      "12 episodes - episode_reward: 17.583 [6.000, 35.000] - loss: 0.013 - mae: 1.428 - mean_q: 1.726 - mean_eps: 0.429 - ale.lives: 2.344\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 18.818 [10.000, 32.000] - loss: 0.013 - mae: 1.438 - mean_q: 1.737 - mean_eps: 0.420 - ale.lives: 2.260\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0219\n",
      "14 episodes - episode_reward: 15.929 [6.000, 25.000] - loss: 0.013 - mae: 1.447 - mean_q: 1.748 - mean_eps: 0.411 - ale.lives: 2.045\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0220\n",
      "11 episodes - episode_reward: 19.545 [11.000, 31.000] - loss: 0.013 - mae: 1.467 - mean_q: 1.771 - mean_eps: 0.402 - ale.lives: 2.109\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0230\n",
      "13 episodes - episode_reward: 17.692 [8.000, 30.000] - loss: 0.014 - mae: 1.489 - mean_q: 1.797 - mean_eps: 0.393 - ale.lives: 2.223\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 231s 23ms/step - reward: 0.0225\n",
      "12 episodes - episode_reward: 18.083 [10.000, 34.000] - loss: 0.013 - mae: 1.493 - mean_q: 1.803 - mean_eps: 0.384 - ale.lives: 2.128\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 233s 23ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 16.846 [9.000, 30.000] - loss: 0.014 - mae: 1.517 - mean_q: 1.831 - mean_eps: 0.375 - ale.lives: 2.048\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 233s 23ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.917 [6.000, 28.000] - loss: 0.014 - mae: 1.539 - mean_q: 1.857 - mean_eps: 0.366 - ale.lives: 2.179\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 234s 23ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.917 [10.000, 34.000] - loss: 0.014 - mae: 1.568 - mean_q: 1.892 - mean_eps: 0.357 - ale.lives: 2.049\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 238s 24ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.083 [12.000, 32.000] - loss: 0.014 - mae: 1.595 - mean_q: 1.923 - mean_eps: 0.348 - ale.lives: 1.976\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 238s 24ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 18.000 [8.000, 28.000] - loss: 0.014 - mae: 1.605 - mean_q: 1.936 - mean_eps: 0.339 - ale.lives: 1.994\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 240s 24ms/step - reward: 0.0216\n",
      "14 episodes - episode_reward: 15.571 [7.000, 24.000] - loss: 0.014 - mae: 1.633 - mean_q: 1.970 - mean_eps: 0.330 - ale.lives: 1.934\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 240s 24ms/step - reward: 0.0214\n",
      "13 episodes - episode_reward: 15.692 [7.000, 30.000] - loss: 0.014 - mae: 1.646 - mean_q: 1.984 - mean_eps: 0.321 - ale.lives: 2.060\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 243s 24ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 18.818 [9.000, 30.000] - loss: 0.015 - mae: 1.660 - mean_q: 2.002 - mean_eps: 0.312 - ale.lives: 1.990\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 244s 24ms/step - reward: 0.0228\n",
      "12 episodes - episode_reward: 19.500 [9.000, 27.000] - loss: 0.014 - mae: 1.679 - mean_q: 2.025 - mean_eps: 0.303 - ale.lives: 2.142\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 246s 25ms/step - reward: 0.0219\n",
      "13 episodes - episode_reward: 16.077 [9.000, 21.000] - loss: 0.015 - mae: 1.703 - mean_q: 2.053 - mean_eps: 0.294 - ale.lives: 2.047\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 248s 25ms/step - reward: 0.0213\n",
      "16 episodes - episode_reward: 13.000 [7.000, 25.000] - loss: 0.015 - mae: 1.737 - mean_q: 2.094 - mean_eps: 0.285 - ale.lives: 2.196\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 251s 25ms/step - reward: 0.0238\n",
      "13 episodes - episode_reward: 19.154 [9.000, 29.000] - loss: 0.016 - mae: 1.749 - mean_q: 2.108 - mean_eps: 0.276 - ale.lives: 2.113\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 19.583 [11.000, 32.000] - loss: 0.015 - mae: 1.769 - mean_q: 2.133 - mean_eps: 0.267 - ale.lives: 2.023\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: 0.0221\n",
      "11 episodes - episode_reward: 18.182 [8.000, 35.000] - loss: 0.016 - mae: 1.784 - mean_q: 2.152 - mean_eps: 0.258 - ale.lives: 2.123\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: 0.0232\n",
      "12 episodes - episode_reward: 21.500 [10.000, 33.000] - loss: 0.016 - mae: 1.807 - mean_q: 2.179 - mean_eps: 0.249 - ale.lives: 2.194\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 252s 25ms/step - reward: 0.0220\n",
      "14 episodes - episode_reward: 14.071 [7.000, 20.000] - loss: 0.015 - mae: 1.820 - mean_q: 2.193 - mean_eps: 0.240 - ale.lives: 2.167\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 254s 25ms/step - reward: 0.0230\n",
      "15 episodes - episode_reward: 15.733 [8.000, 25.000] - loss: 0.016 - mae: 1.824 - mean_q: 2.197 - mean_eps: 0.231 - ale.lives: 1.980\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: 0.0231\n",
      "12 episodes - episode_reward: 19.167 [12.000, 30.000] - loss: 0.016 - mae: 1.846 - mean_q: 2.224 - mean_eps: 0.222 - ale.lives: 2.012\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0214\n",
      "13 episodes - episode_reward: 17.231 [10.000, 31.000] - loss: 0.016 - mae: 1.850 - mean_q: 2.228 - mean_eps: 0.213 - ale.lives: 1.970\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 18.833 [7.000, 32.000] - loss: 0.016 - mae: 1.861 - mean_q: 2.242 - mean_eps: 0.204 - ale.lives: 2.058\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: 0.0200\n",
      "12 episodes - episode_reward: 15.833 [7.000, 32.000] - loss: 0.016 - mae: 1.870 - mean_q: 2.254 - mean_eps: 0.195 - ale.lives: 1.970\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: 0.0207\n",
      "14 episodes - episode_reward: 15.786 [6.000, 26.000] - loss: 0.016 - mae: 1.888 - mean_q: 2.274 - mean_eps: 0.186 - ale.lives: 2.084\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: 0.0223\n",
      "13 episodes - episode_reward: 15.308 [7.000, 30.000] - loss: 0.016 - mae: 1.891 - mean_q: 2.278 - mean_eps: 0.177 - ale.lives: 2.112\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 18.583 [5.000, 34.000] - loss: 0.016 - mae: 1.906 - mean_q: 2.296 - mean_eps: 0.168 - ale.lives: 2.089\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0225\n",
      "14 episodes - episode_reward: 16.214 [4.000, 30.000] - loss: 0.017 - mae: 1.917 - mean_q: 2.307 - mean_eps: 0.159 - ale.lives: 1.938\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0239\n",
      "14 episodes - episode_reward: 17.429 [9.000, 28.000] - loss: 0.017 - mae: 1.939 - mean_q: 2.334 - mean_eps: 0.150 - ale.lives: 2.088\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 270s 27ms/step - reward: 0.0210\n",
      "11 episodes - episode_reward: 18.091 [10.000, 31.000] - loss: 0.017 - mae: 1.956 - mean_q: 2.355 - mean_eps: 0.141 - ale.lives: 2.200\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0217\n",
      "14 episodes - episode_reward: 15.357 [4.000, 23.000] - loss: 0.017 - mae: 1.993 - mean_q: 2.398 - mean_eps: 0.132 - ale.lives: 2.045\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 270s 27ms/step - reward: 0.0202\n",
      "13 episodes - episode_reward: 16.154 [5.000, 24.000] - loss: 0.017 - mae: 1.996 - mean_q: 2.401 - mean_eps: 0.123 - ale.lives: 2.041\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 22.364 [8.000, 34.000] - loss: 0.016 - mae: 2.016 - mean_q: 2.425 - mean_eps: 0.114 - ale.lives: 2.135\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.0212\n",
      "11 episodes - episode_reward: 18.273 [8.000, 33.000] - loss: 0.017 - mae: 2.030 - mean_q: 2.441 - mean_eps: 0.105 - ale.lives: 2.018\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0198\n",
      "13 episodes - episode_reward: 16.154 [7.000, 28.000] - loss: 0.017 - mae: 2.035 - mean_q: 2.447 - mean_eps: 0.100 - ale.lives: 2.170\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 19.091 [13.000, 33.000] - loss: 0.016 - mae: 2.053 - mean_q: 2.469 - mean_eps: 0.100 - ale.lives: 2.103\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 19.750 [4.000, 35.000] - loss: 0.015 - mae: 2.053 - mean_q: 2.469 - mean_eps: 0.100 - ale.lives: 2.250\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0197\n",
      "13 episodes - episode_reward: 14.385 [7.000, 25.000] - loss: 0.017 - mae: 2.053 - mean_q: 2.468 - mean_eps: 0.100 - ale.lives: 1.937\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 20.545 [11.000, 31.000] - loss: 0.016 - mae: 2.066 - mean_q: 2.484 - mean_eps: 0.100 - ale.lives: 1.947\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 17.692 [4.000, 29.000] - loss: 0.016 - mae: 2.089 - mean_q: 2.511 - mean_eps: 0.100 - ale.lives: 2.094\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0240\n",
      "11 episodes - episode_reward: 22.091 [10.000, 33.000] - loss: 0.016 - mae: 2.095 - mean_q: 2.519 - mean_eps: 0.100 - ale.lives: 2.077\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.385 [8.000, 22.000] - loss: 0.015 - mae: 2.098 - mean_q: 2.523 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 18.083 [5.000, 27.000] - loss: 0.016 - mae: 2.112 - mean_q: 2.538 - mean_eps: 0.100 - ale.lives: 2.177\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 19.273 [7.000, 27.000] - loss: 0.016 - mae: 2.136 - mean_q: 2.568 - mean_eps: 0.100 - ale.lives: 2.104\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.833 [7.000, 31.000] - loss: 0.016 - mae: 2.132 - mean_q: 2.562 - mean_eps: 0.100 - ale.lives: 2.069\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0213\n",
      "13 episodes - episode_reward: 17.000 [10.000, 26.000] - loss: 0.016 - mae: 2.138 - mean_q: 2.570 - mean_eps: 0.100 - ale.lives: 2.059\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 18.833 [7.000, 28.000] - loss: 0.016 - mae: 2.132 - mean_q: 2.563 - mean_eps: 0.100 - ale.lives: 2.002\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0239\n",
      "12 episodes - episode_reward: 19.417 [5.000, 32.000] - loss: 0.016 - mae: 2.128 - mean_q: 2.558 - mean_eps: 0.100 - ale.lives: 2.239\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0232\n",
      "12 episodes - episode_reward: 17.583 [9.000, 25.000] - loss: 0.016 - mae: 2.136 - mean_q: 2.567 - mean_eps: 0.100 - ale.lives: 2.182\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0236\n",
      "14 episodes - episode_reward: 18.786 [8.000, 27.000] - loss: 0.016 - mae: 2.165 - mean_q: 2.602 - mean_eps: 0.100 - ale.lives: 2.060\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 17.667 [7.000, 35.000] - loss: 0.016 - mae: 2.170 - mean_q: 2.608 - mean_eps: 0.100 - ale.lives: 2.334\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 19.500 [7.000, 32.000] - loss: 0.016 - mae: 2.191 - mean_q: 2.634 - mean_eps: 0.100 - ale.lives: 2.220\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0217\n",
      "12 episodes - episode_reward: 19.250 [9.000, 31.000] - loss: 0.016 - mae: 2.189 - mean_q: 2.631 - mean_eps: 0.100 - ale.lives: 2.130\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0263\n",
      "11 episodes - episode_reward: 22.545 [10.000, 30.000] - loss: 0.016 - mae: 2.216 - mean_q: 2.663 - mean_eps: 0.100 - ale.lives: 2.111\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0207\n",
      "12 episodes - episode_reward: 18.167 [5.000, 30.000] - loss: 0.016 - mae: 2.224 - mean_q: 2.672 - mean_eps: 0.100 - ale.lives: 2.190\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0236\n",
      "14 episodes - episode_reward: 16.786 [4.000, 32.000] - loss: 0.015 - mae: 2.241 - mean_q: 2.692 - mean_eps: 0.100 - ale.lives: 2.166\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0233\n",
      "13 episodes - episode_reward: 17.000 [5.000, 32.000] - loss: 0.016 - mae: 2.274 - mean_q: 2.733 - mean_eps: 0.100 - ale.lives: 1.948\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0219\n",
      "14 episodes - episode_reward: 16.214 [8.000, 25.000] - loss: 0.016 - mae: 2.282 - mean_q: 2.741 - mean_eps: 0.100 - ale.lives: 2.184\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0230\n",
      "12 episodes - episode_reward: 19.917 [10.000, 30.000] - loss: 0.016 - mae: 2.293 - mean_q: 2.755 - mean_eps: 0.100 - ale.lives: 2.279\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0251\n",
      "10 episodes - episode_reward: 23.200 [12.000, 36.000] - loss: 0.016 - mae: 2.297 - mean_q: 2.760 - mean_eps: 0.100 - ale.lives: 2.341\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0241\n",
      "13 episodes - episode_reward: 19.923 [9.000, 30.000] - loss: 0.016 - mae: 2.301 - mean_q: 2.765 - mean_eps: 0.100 - ale.lives: 2.107\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0233\n",
      "11 episodes - episode_reward: 20.727 [16.000, 26.000] - loss: 0.016 - mae: 2.307 - mean_q: 2.772 - mean_eps: 0.100 - ale.lives: 2.196\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0230\n",
      "12 episodes - episode_reward: 18.167 [9.000, 27.000] - loss: 0.016 - mae: 2.329 - mean_q: 2.798 - mean_eps: 0.100 - ale.lives: 2.022\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0215\n",
      "14 episodes - episode_reward: 16.071 [6.000, 35.000] - loss: 0.016 - mae: 2.332 - mean_q: 2.802 - mean_eps: 0.100 - ale.lives: 1.942\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0218\n",
      "12 episodes - episode_reward: 18.833 [13.000, 32.000] - loss: 0.016 - mae: 2.348 - mean_q: 2.821 - mean_eps: 0.100 - ale.lives: 2.032\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0246\n",
      "13 episodes - episode_reward: 18.615 [10.000, 31.000] - loss: 0.016 - mae: 2.356 - mean_q: 2.830 - mean_eps: 0.100 - ale.lives: 2.065\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0262\n",
      "12 episodes - episode_reward: 21.583 [8.000, 34.000] - loss: 0.017 - mae: 2.387 - mean_q: 2.867 - mean_eps: 0.100 - ale.lives: 2.214\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0216\n",
      "11 episodes - episode_reward: 18.818 [4.000, 28.000] - loss: 0.016 - mae: 2.376 - mean_q: 2.854 - mean_eps: 0.100 - ale.lives: 2.270\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.667 [5.000, 34.000] - loss: 0.017 - mae: 2.379 - mean_q: 2.857 - mean_eps: 0.100 - ale.lives: 1.962\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0243\n",
      "13 episodes - episode_reward: 17.846 [5.000, 32.000] - loss: 0.016 - mae: 2.378 - mean_q: 2.856 - mean_eps: 0.100 - ale.lives: 2.121\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0237\n",
      "11 episodes - episode_reward: 23.636 [15.000, 33.000] - loss: 0.016 - mae: 2.376 - mean_q: 2.853 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 18.583 [3.000, 25.000] - loss: 0.016 - mae: 2.382 - mean_q: 2.860 - mean_eps: 0.100 - ale.lives: 2.122\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0249\n",
      "11 episodes - episode_reward: 21.182 [4.000, 30.000] - loss: 0.016 - mae: 2.381 - mean_q: 2.859 - mean_eps: 0.100 - ale.lives: 2.288\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0233\n",
      "11 episodes - episode_reward: 20.909 [6.000, 36.000] - loss: 0.016 - mae: 2.402 - mean_q: 2.885 - mean_eps: 0.100 - ale.lives: 2.332\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0218\n",
      "12 episodes - episode_reward: 20.083 [16.000, 26.000] - loss: 0.016 - mae: 2.391 - mean_q: 2.871 - mean_eps: 0.100 - ale.lives: 2.253\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0241\n",
      "11 episodes - episode_reward: 20.364 [12.000, 33.000] - loss: 0.016 - mae: 2.408 - mean_q: 2.892 - mean_eps: 0.100 - ale.lives: 2.231\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0234\n",
      "12 episodes - episode_reward: 20.750 [12.000, 25.000] - loss: 0.016 - mae: 2.395 - mean_q: 2.876 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0234\n",
      "10 episodes - episode_reward: 21.400 [13.000, 32.000] - loss: 0.016 - mae: 2.396 - mean_q: 2.877 - mean_eps: 0.100 - ale.lives: 2.115\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0229\n",
      "12 episodes - episode_reward: 20.000 [6.000, 32.000] - loss: 0.016 - mae: 2.381 - mean_q: 2.859 - mean_eps: 0.100 - ale.lives: 2.086\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0217\n",
      "10 episodes - episode_reward: 20.800 [10.000, 35.000] - loss: 0.017 - mae: 2.380 - mean_q: 2.857 - mean_eps: 0.100 - ale.lives: 2.036\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0225\n",
      "13 episodes - episode_reward: 18.231 [3.000, 32.000] - loss: 0.016 - mae: 2.390 - mean_q: 2.870 - mean_eps: 0.100 - ale.lives: 2.272\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0239\n",
      "9 episodes - episode_reward: 25.111 [15.000, 36.000] - loss: 0.016 - mae: 2.405 - mean_q: 2.888 - mean_eps: 0.100 - ale.lives: 2.042\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0211\n",
      "12 episodes - episode_reward: 18.917 [4.000, 30.000] - loss: 0.017 - mae: 2.399 - mean_q: 2.881 - mean_eps: 0.100 - ale.lives: 2.200\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 21.182 [15.000, 27.000] - loss: 0.016 - mae: 2.397 - mean_q: 2.877 - mean_eps: 0.100 - ale.lives: 2.066\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0232\n",
      "12 episodes - episode_reward: 18.000 [5.000, 28.000] - loss: 0.016 - mae: 2.402 - mean_q: 2.884 - mean_eps: 0.100 - ale.lives: 2.146\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0238\n",
      "12 episodes - episode_reward: 20.333 [5.000, 30.000] - loss: 0.016 - mae: 2.406 - mean_q: 2.888 - mean_eps: 0.100 - ale.lives: 2.275\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0217\n",
      "11 episodes - episode_reward: 20.182 [7.000, 34.000] - loss: 0.016 - mae: 2.418 - mean_q: 2.903 - mean_eps: 0.100 - ale.lives: 2.153\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 20.333 [4.000, 35.000] - loss: 0.015 - mae: 2.416 - mean_q: 2.900 - mean_eps: 0.100 - ale.lives: 1.972\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0233\n",
      "14 episodes - episode_reward: 16.429 [7.000, 32.000] - loss: 0.016 - mae: 2.418 - mean_q: 2.903 - mean_eps: 0.100 - ale.lives: 2.065\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0226\n",
      "9 episodes - episode_reward: 22.778 [6.000, 48.000] - loss: 0.016 - mae: 2.427 - mean_q: 2.914 - mean_eps: 0.100 - ale.lives: 2.116\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0248\n",
      "11 episodes - episode_reward: 24.182 [10.000, 35.000] - loss: 0.016 - mae: 2.432 - mean_q: 2.919 - mean_eps: 0.100 - ale.lives: 2.181\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0195\n",
      "12 episodes - episode_reward: 16.000 [7.000, 33.000] - loss: 0.016 - mae: 2.435 - mean_q: 2.922 - mean_eps: 0.100 - ale.lives: 2.378\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0247\n",
      "11 episodes - episode_reward: 21.091 [13.000, 30.000] - loss: 0.016 - mae: 2.442 - mean_q: 2.931 - mean_eps: 0.100 - ale.lives: 2.125\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 20.667 [7.000, 28.000] - loss: 0.016 - mae: 2.431 - mean_q: 2.918 - mean_eps: 0.100 - ale.lives: 2.152\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0231\n",
      "10 episodes - episode_reward: 20.600 [7.000, 33.000] - loss: 0.016 - mae: 2.444 - mean_q: 2.933 - mean_eps: 0.100 - ale.lives: 2.142\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0185\n",
      "13 episodes - episode_reward: 15.769 [5.000, 33.000] - loss: 0.016 - mae: 2.441 - mean_q: 2.930 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0230\n",
      "11 episodes - episode_reward: 20.818 [7.000, 33.000] - loss: 0.016 - mae: 2.460 - mean_q: 2.951 - mean_eps: 0.100 - ale.lives: 2.255\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 18.455 [8.000, 31.000] - loss: 0.016 - mae: 2.474 - mean_q: 2.969 - mean_eps: 0.100 - ale.lives: 1.909\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0207\n",
      "14 episodes - episode_reward: 15.143 [6.000, 30.000] - loss: 0.016 - mae: 2.484 - mean_q: 2.981 - mean_eps: 0.100 - ale.lives: 2.119\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 16.692 [5.000, 31.000] - loss: 0.016 - mae: 2.479 - mean_q: 2.975 - mean_eps: 0.100 - ale.lives: 2.008\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0213\n",
      "11 episodes - episode_reward: 19.182 [10.000, 35.000] - loss: 0.016 - mae: 2.473 - mean_q: 2.967 - mean_eps: 0.100 - ale.lives: 2.173\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 19.091 [8.000, 30.000] - loss: 0.016 - mae: 2.468 - mean_q: 2.960 - mean_eps: 0.100 - ale.lives: 1.997\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 301s 30ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 21.818 [9.000, 32.000] - loss: 0.016 - mae: 2.467 - mean_q: 2.960 - mean_eps: 0.100 - ale.lives: 2.311\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 323s 32ms/step - reward: 0.0232\n",
      "13 episodes - episode_reward: 18.154 [8.000, 30.000] - loss: 0.015 - mae: 2.459 - mean_q: 2.951 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0241\n",
      "11 episodes - episode_reward: 20.455 [8.000, 33.000] - loss: 0.015 - mae: 2.486 - mean_q: 2.984 - mean_eps: 0.100 - ale.lives: 2.105\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0240\n",
      "12 episodes - episode_reward: 20.500 [12.000, 29.000] - loss: 0.016 - mae: 2.484 - mean_q: 2.980 - mean_eps: 0.100 - ale.lives: 2.071\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0246\n",
      "12 episodes - episode_reward: 21.250 [12.000, 29.000] - loss: 0.016 - mae: 2.505 - mean_q: 3.006 - mean_eps: 0.100 - ale.lives: 2.116\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0235\n",
      "12 episodes - episode_reward: 19.833 [8.000, 27.000] - loss: 0.016 - mae: 2.498 - mean_q: 2.998 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0248\n",
      "11 episodes - episode_reward: 20.000 [8.000, 30.000] - loss: 0.016 - mae: 2.516 - mean_q: 3.021 - mean_eps: 0.100 - ale.lives: 2.154\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 21.083 [7.000, 37.000] - loss: 0.016 - mae: 2.516 - mean_q: 3.020 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0219\n",
      "13 episodes - episode_reward: 17.231 [3.000, 28.000] - loss: 0.016 - mae: 2.521 - mean_q: 3.026 - mean_eps: 0.100 - ale.lives: 2.189\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0238\n",
      "11 episodes - episode_reward: 20.455 [9.000, 34.000] - loss: 0.015 - mae: 2.532 - mean_q: 3.038 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0233\n",
      "13 episodes - episode_reward: 19.000 [8.000, 29.000] - loss: 0.015 - mae: 2.539 - mean_q: 3.047 - mean_eps: 0.100 - ale.lives: 2.015\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0232\n",
      "10 episodes - episode_reward: 22.800 [15.000, 30.000] - loss: 0.016 - mae: 2.553 - mean_q: 3.065 - mean_eps: 0.100 - ale.lives: 2.173\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0218\n",
      "12 episodes - episode_reward: 17.250 [7.000, 33.000] - loss: 0.015 - mae: 2.542 - mean_q: 3.051 - mean_eps: 0.100 - ale.lives: 1.999\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 20.909 [10.000, 28.000] - loss: 0.016 - mae: 2.533 - mean_q: 3.041 - mean_eps: 0.100 - ale.lives: 2.105\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0233\n",
      "11 episodes - episode_reward: 21.364 [13.000, 34.000] - loss: 0.015 - mae: 2.537 - mean_q: 3.044 - mean_eps: 0.100 - ale.lives: 2.253\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0210\n",
      "11 episodes - episode_reward: 20.091 [12.000, 29.000] - loss: 0.016 - mae: 2.535 - mean_q: 3.042 - mean_eps: 0.100 - ale.lives: 2.208\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 18.250 [6.000, 30.000] - loss: 0.016 - mae: 2.561 - mean_q: 3.075 - mean_eps: 0.100 - ale.lives: 2.151\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0226\n",
      "11 episodes - episode_reward: 20.091 [10.000, 31.000] - loss: 0.016 - mae: 2.562 - mean_q: 3.074 - mean_eps: 0.100 - ale.lives: 2.097\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0229\n",
      "13 episodes - episode_reward: 17.462 [9.000, 25.000] - loss: 0.016 - mae: 2.550 - mean_q: 3.060 - mean_eps: 0.100 - ale.lives: 1.980\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0256\n",
      "11 episodes - episode_reward: 23.909 [15.000, 35.000] - loss: 0.016 - mae: 2.549 - mean_q: 3.060 - mean_eps: 0.100 - ale.lives: 2.064\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 18.000 [5.000, 31.000] - loss: 0.016 - mae: 2.550 - mean_q: 3.060 - mean_eps: 0.100 - ale.lives: 1.933\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0246\n",
      "10 episodes - episode_reward: 22.300 [16.000, 33.000] - loss: 0.016 - mae: 2.537 - mean_q: 3.044 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 22.273 [15.000, 32.000] - loss: 0.016 - mae: 2.544 - mean_q: 3.053 - mean_eps: 0.100 - ale.lives: 2.147\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 18.667 [8.000, 31.000] - loss: 0.016 - mae: 2.547 - mean_q: 3.057 - mean_eps: 0.100 - ale.lives: 2.139\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 19.083 [3.000, 37.000] - loss: 0.016 - mae: 2.566 - mean_q: 3.079 - mean_eps: 0.100 - ale.lives: 2.218\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0234\n",
      "13 episodes - episode_reward: 17.615 [7.000, 31.000] - loss: 0.016 - mae: 2.587 - mean_q: 3.104 - mean_eps: 0.100 - ale.lives: 2.216\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0219\n",
      "10 episodes - episode_reward: 20.400 [9.000, 31.000] - loss: 0.015 - mae: 2.583 - mean_q: 3.098 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0226\n",
      "10 episodes - episode_reward: 21.500 [12.000, 32.000] - loss: 0.015 - mae: 2.579 - mean_q: 3.095 - mean_eps: 0.100 - ale.lives: 2.144\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0240\n",
      "12 episodes - episode_reward: 21.917 [14.000, 32.000] - loss: 0.016 - mae: 2.596 - mean_q: 3.114 - mean_eps: 0.100 - ale.lives: 2.155\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0243\n",
      "10 episodes - episode_reward: 23.900 [15.000, 35.000] - loss: 0.016 - mae: 2.590 - mean_q: 3.107 - mean_eps: 0.100 - ale.lives: 2.280\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0214\n",
      "11 episodes - episode_reward: 20.545 [4.000, 33.000] - loss: 0.016 - mae: 2.601 - mean_q: 3.120 - mean_eps: 0.100 - ale.lives: 2.248\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0246\n",
      "10 episodes - episode_reward: 23.500 [13.000, 32.000] - loss: 0.015 - mae: 2.634 - mean_q: 3.160 - mean_eps: 0.100 - ale.lives: 2.203\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0244\n",
      "10 episodes - episode_reward: 23.400 [14.000, 39.000] - loss: 0.016 - mae: 2.633 - mean_q: 3.159 - mean_eps: 0.100 - ale.lives: 2.027\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 22.545 [6.000, 39.000] - loss: 0.015 - mae: 2.642 - mean_q: 3.170 - mean_eps: 0.100 - ale.lives: 2.109\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0228\n",
      "10 episodes - episode_reward: 21.100 [12.000, 32.000] - loss: 0.016 - mae: 2.630 - mean_q: 3.156 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0245\n",
      "12 episodes - episode_reward: 22.000 [10.000, 35.000] - loss: 0.015 - mae: 2.621 - mean_q: 3.146 - mean_eps: 0.100 - ale.lives: 2.100\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 19.273 [10.000, 30.000] - loss: 0.015 - mae: 2.629 - mean_q: 3.155 - mean_eps: 0.100 - ale.lives: 2.200\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0228\n",
      "12 episodes - episode_reward: 20.333 [10.000, 33.000] - loss: 0.016 - mae: 2.630 - mean_q: 3.155 - mean_eps: 0.100 - ale.lives: 2.091\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0242\n",
      "12 episodes - episode_reward: 20.250 [7.000, 29.000] - loss: 0.016 - mae: 2.624 - mean_q: 3.148 - mean_eps: 0.100 - ale.lives: 1.978\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0236\n",
      "12 episodes - episode_reward: 19.750 [10.000, 28.000] - loss: 0.016 - mae: 2.633 - mean_q: 3.160 - mean_eps: 0.100 - ale.lives: 2.007\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0238\n",
      "9 episodes - episode_reward: 25.222 [14.000, 34.000] - loss: 0.015 - mae: 2.624 - mean_q: 3.149 - mean_eps: 0.100 - ale.lives: 2.192\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 21.750 [15.000, 30.000] - loss: 0.015 - mae: 2.627 - mean_q: 3.153 - mean_eps: 0.100 - ale.lives: 2.206\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 17.077 [4.000, 32.000] - loss: 0.015 - mae: 2.622 - mean_q: 3.147 - mean_eps: 0.100 - ale.lives: 2.220\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 20.182 [9.000, 27.000] - loss: 0.015 - mae: 2.636 - mean_q: 3.164 - mean_eps: 0.100 - ale.lives: 2.355\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0240\n",
      "12 episodes - episode_reward: 19.000 [7.000, 34.000] - loss: 0.015 - mae: 2.651 - mean_q: 3.181 - mean_eps: 0.100 - ale.lives: 2.275\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0249\n",
      "12 episodes - episode_reward: 22.917 [7.000, 33.000] - loss: 0.015 - mae: 2.649 - mean_q: 3.180 - mean_eps: 0.100 - ale.lives: 2.203\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0238\n",
      "11 episodes - episode_reward: 21.091 [4.000, 33.000] - loss: 0.016 - mae: 2.667 - mean_q: 3.201 - mean_eps: 0.100 - ale.lives: 2.108\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0244\n",
      "12 episodes - episode_reward: 19.167 [12.000, 30.000] - loss: 0.016 - mae: 2.670 - mean_q: 3.204 - mean_eps: 0.100 - ale.lives: 2.263\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0247\n",
      "13 episodes - episode_reward: 19.308 [9.000, 35.000] - loss: 0.016 - mae: 2.674 - mean_q: 3.208 - mean_eps: 0.100 - ale.lives: 2.058\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0246\n",
      "13 episodes - episode_reward: 19.308 [6.000, 31.000] - loss: 0.015 - mae: 2.671 - mean_q: 3.204 - mean_eps: 0.100 - ale.lives: 1.973\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 20.333 [11.000, 30.000] - loss: 0.016 - mae: 2.684 - mean_q: 3.221 - mean_eps: 0.100 - ale.lives: 2.028\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0247\n",
      "12 episodes - episode_reward: 20.167 [7.000, 44.000] - loss: 0.015 - mae: 2.695 - mean_q: 3.235 - mean_eps: 0.100 - ale.lives: 2.116\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0246\n",
      "11 episodes - episode_reward: 21.909 [15.000, 31.000] - loss: 0.016 - mae: 2.690 - mean_q: 3.227 - mean_eps: 0.100 - ale.lives: 2.159\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 22.545 [11.000, 34.000] - loss: 0.015 - mae: 2.706 - mean_q: 3.247 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: 0.0241\n",
      "13 episodes - episode_reward: 17.846 [7.000, 28.000] - loss: 0.015 - mae: 2.720 - mean_q: 3.263 - mean_eps: 0.100 - ale.lives: 2.161\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0243\n",
      "12 episodes - episode_reward: 21.083 [8.000, 32.000] - loss: 0.016 - mae: 2.724 - mean_q: 3.270 - mean_eps: 0.100 - ale.lives: 2.053\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0241\n",
      "13 episodes - episode_reward: 19.154 [9.000, 33.000] - loss: 0.016 - mae: 2.709 - mean_q: 3.250 - mean_eps: 0.100 - ale.lives: 2.221\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0241\n",
      "11 episodes - episode_reward: 20.727 [13.000, 30.000] - loss: 0.016 - mae: 2.711 - mean_q: 3.254 - mean_eps: 0.100 - ale.lives: 2.175\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0264\n",
      "11 episodes - episode_reward: 23.364 [15.000, 30.000] - loss: 0.016 - mae: 2.702 - mean_q: 3.241 - mean_eps: 0.100 - ale.lives: 1.976\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0240\n",
      "12 episodes - episode_reward: 20.167 [10.000, 31.000] - loss: 0.016 - mae: 2.703 - mean_q: 3.244 - mean_eps: 0.100 - ale.lives: 1.973\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 21.545 [12.000, 31.000] - loss: 0.016 - mae: 2.713 - mean_q: 3.255 - mean_eps: 0.100 - ale.lives: 2.091\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0229\n",
      "14 episodes - episode_reward: 16.071 [7.000, 29.000] - loss: 0.015 - mae: 2.697 - mean_q: 3.235 - mean_eps: 0.100 - ale.lives: 2.153\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 22.364 [6.000, 35.000] - loss: 0.015 - mae: 2.691 - mean_q: 3.228 - mean_eps: 0.100 - ale.lives: 2.160\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0237\n",
      "11 episodes - episode_reward: 22.364 [5.000, 34.000] - loss: 0.015 - mae: 2.691 - mean_q: 3.228 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0249\n",
      "12 episodes - episode_reward: 18.583 [12.000, 31.000] - loss: 0.014 - mae: 2.678 - mean_q: 3.213 - mean_eps: 0.100 - ale.lives: 2.172\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0219\n",
      "13 episodes - episode_reward: 18.462 [10.000, 31.000] - loss: 0.015 - mae: 2.674 - mean_q: 3.208 - mean_eps: 0.100 - ale.lives: 1.965\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 18.000 [9.000, 28.000] - loss: 0.015 - mae: 2.683 - mean_q: 3.218 - mean_eps: 0.100 - ale.lives: 2.122\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 20.417 [8.000, 33.000] - loss: 0.015 - mae: 2.687 - mean_q: 3.224 - mean_eps: 0.100 - ale.lives: 2.031\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0244\n",
      "10 episodes - episode_reward: 22.600 [7.000, 35.000] - loss: 0.015 - mae: 2.695 - mean_q: 3.234 - mean_eps: 0.100 - ale.lives: 2.142\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0240\n",
      "11 episodes - episode_reward: 21.727 [10.000, 30.000] - loss: 0.015 - mae: 2.704 - mean_q: 3.245 - mean_eps: 0.100 - ale.lives: 2.242\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0242\n",
      "12 episodes - episode_reward: 20.667 [10.000, 28.000] - loss: 0.016 - mae: 2.706 - mean_q: 3.246 - mean_eps: 0.100 - ale.lives: 2.081\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0236\n",
      "14 episodes - episode_reward: 17.000 [6.000, 29.000] - loss: 0.016 - mae: 2.714 - mean_q: 3.256 - mean_eps: 0.100 - ale.lives: 2.128\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0203\n",
      "12 episodes - episode_reward: 16.417 [3.000, 32.000] - loss: 0.016 - mae: 2.731 - mean_q: 3.276 - mean_eps: 0.100 - ale.lives: 2.086\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0243\n",
      "12 episodes - episode_reward: 19.333 [12.000, 31.000] - loss: 0.016 - mae: 2.740 - mean_q: 3.286 - mean_eps: 0.100 - ale.lives: 2.049\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0260\n",
      "13 episodes - episode_reward: 21.154 [9.000, 30.000] - loss: 0.016 - mae: 2.741 - mean_q: 3.287 - mean_eps: 0.100 - ale.lives: 2.178\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0207\n",
      "11 episodes - episode_reward: 18.545 [5.000, 30.000] - loss: 0.016 - mae: 2.738 - mean_q: 3.283 - mean_eps: 0.100 - ale.lives: 2.257\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0257\n",
      "11 episodes - episode_reward: 22.636 [14.000, 32.000] - loss: 0.015 - mae: 2.742 - mean_q: 3.289 - mean_eps: 0.100 - ale.lives: 2.132\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 20.083 [8.000, 34.000] - loss: 0.016 - mae: 2.742 - mean_q: 3.289 - mean_eps: 0.100 - ale.lives: 2.155\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 18.818 [8.000, 30.000] - loss: 0.017 - mae: 2.738 - mean_q: 3.283 - mean_eps: 0.100 - ale.lives: 2.144\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0218\n",
      "13 episodes - episode_reward: 17.462 [9.000, 28.000] - loss: 0.016 - mae: 2.735 - mean_q: 3.280 - mean_eps: 0.100 - ale.lives: 2.117\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0216\n",
      "9 episodes - episode_reward: 22.556 [8.000, 36.000] - loss: 0.016 - mae: 2.740 - mean_q: 3.286 - mean_eps: 0.100 - ale.lives: 2.021\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0226\n",
      "11 episodes - episode_reward: 22.364 [12.000, 32.000] - loss: 0.017 - mae: 2.751 - mean_q: 3.298 - mean_eps: 0.100 - ale.lives: 2.027\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0238\n",
      "10 episodes - episode_reward: 23.900 [14.000, 33.000] - loss: 0.016 - mae: 2.730 - mean_q: 3.273 - mean_eps: 0.100 - ale.lives: 2.282\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0238\n",
      "13 episodes - episode_reward: 18.462 [6.000, 29.000] - loss: 0.016 - mae: 2.738 - mean_q: 3.283 - mean_eps: 0.100 - ale.lives: 2.095\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0228\n",
      "13 episodes - episode_reward: 17.385 [7.000, 28.000] - loss: 0.016 - mae: 2.724 - mean_q: 3.267 - mean_eps: 0.100 - ale.lives: 2.193\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0229\n",
      "12 episodes - episode_reward: 19.167 [9.000, 36.000] - loss: 0.016 - mae: 2.746 - mean_q: 3.293 - mean_eps: 0.100 - ale.lives: 2.127\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0234\n",
      "13 episodes - episode_reward: 17.538 [7.000, 27.000] - loss: 0.016 - mae: 2.759 - mean_q: 3.309 - mean_eps: 0.100 - ale.lives: 2.058\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0244\n",
      "11 episodes - episode_reward: 22.545 [9.000, 32.000] - loss: 0.017 - mae: 2.773 - mean_q: 3.325 - mean_eps: 0.100 - ale.lives: 2.229\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.167 [9.000, 27.000] - loss: 0.016 - mae: 2.768 - mean_q: 3.319 - mean_eps: 0.100 - ale.lives: 2.162\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0213\n",
      "10 episodes - episode_reward: 21.500 [12.000, 33.000] - loss: 0.016 - mae: 2.776 - mean_q: 3.329 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0205\n",
      "14 episodes - episode_reward: 13.929 [7.000, 26.000] - loss: 0.015 - mae: 2.765 - mean_q: 3.316 - mean_eps: 0.100 - ale.lives: 2.127\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0238\n",
      "13 episodes - episode_reward: 20.000 [6.000, 35.000] - loss: 0.015 - mae: 2.770 - mean_q: 3.321 - mean_eps: 0.100 - ale.lives: 2.184\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0232\n",
      "10 episodes - episode_reward: 21.400 [8.000, 34.000] - loss: 0.015 - mae: 2.758 - mean_q: 3.307 - mean_eps: 0.100 - ale.lives: 2.082\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0244\n",
      "12 episodes - episode_reward: 20.667 [9.000, 31.000] - loss: 0.015 - mae: 2.764 - mean_q: 3.316 - mean_eps: 0.100 - ale.lives: 2.136\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0248\n",
      "11 episodes - episode_reward: 22.727 [15.000, 31.000] - loss: 0.015 - mae: 2.785 - mean_q: 3.340 - mean_eps: 0.100 - ale.lives: 1.945\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 19.917 [10.000, 33.000] - loss: 0.015 - mae: 2.772 - mean_q: 3.324 - mean_eps: 0.100 - ale.lives: 2.054\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0249\n",
      "11 episodes - episode_reward: 21.000 [8.000, 27.000] - loss: 0.016 - mae: 2.765 - mean_q: 3.315 - mean_eps: 0.100 - ale.lives: 2.165\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0226\n",
      "12 episodes - episode_reward: 19.500 [12.000, 30.000] - loss: 0.015 - mae: 2.761 - mean_q: 3.310 - mean_eps: 0.100 - ale.lives: 2.221\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0236\n",
      "14 episodes - episode_reward: 17.571 [7.000, 28.000] - loss: 0.016 - mae: 2.775 - mean_q: 3.329 - mean_eps: 0.100 - ale.lives: 2.063\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0250\n",
      "10 episodes - episode_reward: 21.900 [12.000, 29.000] - loss: 0.016 - mae: 2.762 - mean_q: 3.314 - mean_eps: 0.100 - ale.lives: 2.165\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0226\n",
      "11 episodes - episode_reward: 20.818 [11.000, 31.000] - loss: 0.015 - mae: 2.759 - mean_q: 3.310 - mean_eps: 0.100 - ale.lives: 2.050\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0237\n",
      "11 episodes - episode_reward: 22.636 [14.000, 32.000] - loss: 0.015 - mae: 2.765 - mean_q: 3.317 - mean_eps: 0.100 - ale.lives: 2.195\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 19.250 [10.000, 25.000] - loss: 0.016 - mae: 2.780 - mean_q: 3.335 - mean_eps: 0.100 - ale.lives: 2.025\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0236\n",
      "10 episodes - episode_reward: 22.100 [2.000, 31.000] - loss: 0.016 - mae: 2.776 - mean_q: 3.331 - mean_eps: 0.100 - ale.lives: 2.135\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 20.333 [7.000, 28.000] - loss: 0.015 - mae: 2.766 - mean_q: 3.318 - mean_eps: 0.100 - ale.lives: 2.103\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0241\n",
      "11 episodes - episode_reward: 21.545 [10.000, 30.000] - loss: 0.016 - mae: 2.766 - mean_q: 3.318 - mean_eps: 0.100 - ale.lives: 2.221\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0248\n",
      "10 episodes - episode_reward: 22.200 [11.000, 30.000] - loss: 0.015 - mae: 2.751 - mean_q: 3.301 - mean_eps: 0.100 - ale.lives: 2.241\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0244\n",
      "11 episodes - episode_reward: 23.727 [11.000, 39.000] - loss: 0.015 - mae: 2.740 - mean_q: 3.288 - mean_eps: 0.100 - ale.lives: 2.156\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0250\n",
      "10 episodes - episode_reward: 24.200 [11.000, 35.000] - loss: 0.016 - mae: 2.744 - mean_q: 3.291 - mean_eps: 0.100 - ale.lives: 2.262\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0229\n",
      "10 episodes - episode_reward: 21.800 [14.000, 35.000] - loss: 0.015 - mae: 2.755 - mean_q: 3.304 - mean_eps: 0.100 - ale.lives: 2.210\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0236\n",
      "14 episodes - episode_reward: 18.786 [5.000, 33.000] - loss: 0.015 - mae: 2.740 - mean_q: 3.287 - mean_eps: 0.100 - ale.lives: 2.044\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0242\n",
      "11 episodes - episode_reward: 20.182 [7.000, 35.000] - loss: 0.015 - mae: 2.742 - mean_q: 3.288 - mean_eps: 0.100 - ale.lives: 2.232\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0234\n",
      "9 episodes - episode_reward: 28.111 [18.000, 35.000] - loss: 0.016 - mae: 2.746 - mean_q: 3.293 - mean_eps: 0.100 - ale.lives: 1.994\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0230\n",
      "11 episodes - episode_reward: 20.545 [6.000, 36.000] - loss: 0.015 - mae: 2.749 - mean_q: 3.298 - mean_eps: 0.100 - ale.lives: 2.142\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0245\n",
      "12 episodes - episode_reward: 19.667 [7.000, 33.000] - loss: 0.015 - mae: 2.730 - mean_q: 3.274 - mean_eps: 0.100 - ale.lives: 2.231\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0237\n",
      "12 episodes - episode_reward: 19.167 [5.000, 33.000] - loss: 0.015 - mae: 2.728 - mean_q: 3.274 - mean_eps: 0.100 - ale.lives: 2.054\n",
      "\n",
      "Interval 285 (2840000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0229\n",
      "11 episodes - episode_reward: 23.182 [12.000, 36.000] - loss: 0.015 - mae: 2.723 - mean_q: 3.265 - mean_eps: 0.100 - ale.lives: 1.909\n",
      "\n",
      "Interval 286 (2850000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0225\n",
      "12 episodes - episode_reward: 18.750 [5.000, 34.000] - loss: 0.015 - mae: 2.721 - mean_q: 3.263 - mean_eps: 0.100 - ale.lives: 2.142\n",
      "\n",
      "Interval 287 (2860000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 20.909 [10.000, 32.000] - loss: 0.015 - mae: 2.733 - mean_q: 3.279 - mean_eps: 0.100 - ale.lives: 2.227\n",
      "\n",
      "Interval 288 (2870000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 20.455 [4.000, 32.000] - loss: 0.016 - mae: 2.728 - mean_q: 3.272 - mean_eps: 0.100 - ale.lives: 2.070\n",
      "\n",
      "Interval 289 (2880000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0233\n",
      "11 episodes - episode_reward: 22.364 [12.000, 33.000] - loss: 0.016 - mae: 2.723 - mean_q: 3.267 - mean_eps: 0.100 - ale.lives: 2.179\n",
      "\n",
      "Interval 290 (2890000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0241\n",
      "10 episodes - episode_reward: 22.600 [10.000, 40.000] - loss: 0.017 - mae: 2.736 - mean_q: 3.281 - mean_eps: 0.100 - ale.lives: 2.266\n",
      "\n",
      "Interval 291 (2900000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0230\n",
      "12 episodes - episode_reward: 20.750 [12.000, 27.000] - loss: 0.017 - mae: 2.726 - mean_q: 3.270 - mean_eps: 0.100 - ale.lives: 2.067\n",
      "\n",
      "Interval 292 (2910000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0216\n",
      "10 episodes - episode_reward: 20.700 [8.000, 29.000] - loss: 0.016 - mae: 2.746 - mean_q: 3.294 - mean_eps: 0.100 - ale.lives: 2.174\n",
      "\n",
      "Interval 293 (2920000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0249\n",
      "11 episodes - episode_reward: 22.455 [13.000, 31.000] - loss: 0.016 - mae: 2.746 - mean_q: 3.293 - mean_eps: 0.100 - ale.lives: 2.217\n",
      "\n",
      "Interval 294 (2930000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0257\n",
      "11 episodes - episode_reward: 22.364 [12.000, 35.000] - loss: 0.017 - mae: 2.766 - mean_q: 3.317 - mean_eps: 0.100 - ale.lives: 2.220\n",
      "\n",
      "Interval 295 (2940000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0256\n",
      "12 episodes - episode_reward: 22.833 [15.000, 29.000] - loss: 0.016 - mae: 2.772 - mean_q: 3.324 - mean_eps: 0.100 - ale.lives: 2.107\n",
      "\n",
      "Interval 296 (2950000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 14.769 [5.000, 27.000] - loss: 0.016 - mae: 2.768 - mean_q: 3.320 - mean_eps: 0.100 - ale.lives: 2.053\n",
      "\n",
      "Interval 297 (2960000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0269\n",
      "12 episodes - episode_reward: 22.000 [13.000, 33.000] - loss: 0.017 - mae: 2.777 - mean_q: 3.330 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 298 (2970000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0220\n",
      "11 episodes - episode_reward: 20.909 [12.000, 38.000] - loss: 0.016 - mae: 2.768 - mean_q: 3.321 - mean_eps: 0.100 - ale.lives: 2.019\n",
      "\n",
      "Interval 299 (2980000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0234\n",
      "13 episodes - episode_reward: 18.923 [11.000, 29.000] - loss: 0.016 - mae: 2.786 - mean_q: 3.342 - mean_eps: 0.100 - ale.lives: 2.046\n",
      "\n",
      "Interval 300 (2990000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 19.818 [6.000, 34.000] - loss: 0.016 - mae: 2.782 - mean_q: 3.338 - mean_eps: 0.100 - ale.lives: 2.056\n",
      "\n",
      "Interval 301 (3000000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0239\n",
      "12 episodes - episode_reward: 20.667 [7.000, 35.000] - loss: 0.016 - mae: 2.788 - mean_q: 3.344 - mean_eps: 0.100 - ale.lives: 2.058\n",
      "\n",
      "Interval 302 (3010000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0254\n",
      "10 episodes - episode_reward: 24.400 [16.000, 31.000] - loss: 0.016 - mae: 2.784 - mean_q: 3.340 - mean_eps: 0.100 - ale.lives: 2.175\n",
      "\n",
      "Interval 303 (3020000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 20.727 [7.000, 29.000] - loss: 0.016 - mae: 2.792 - mean_q: 3.350 - mean_eps: 0.100 - ale.lives: 2.300\n",
      "\n",
      "Interval 304 (3030000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0270\n",
      "12 episodes - episode_reward: 22.500 [18.000, 33.000] - loss: 0.016 - mae: 2.784 - mean_q: 3.340 - mean_eps: 0.100 - ale.lives: 1.986\n",
      "\n",
      "Interval 305 (3040000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0254\n",
      "11 episodes - episode_reward: 24.455 [16.000, 33.000] - loss: 0.015 - mae: 2.768 - mean_q: 3.322 - mean_eps: 0.100 - ale.lives: 2.106\n",
      "\n",
      "Interval 306 (3050000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0246\n",
      "11 episodes - episode_reward: 22.364 [16.000, 28.000] - loss: 0.016 - mae: 2.783 - mean_q: 3.339 - mean_eps: 0.100 - ale.lives: 2.253\n",
      "\n",
      "Interval 307 (3060000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0258\n",
      "11 episodes - episode_reward: 21.455 [7.000, 32.000] - loss: 0.016 - mae: 2.778 - mean_q: 3.334 - mean_eps: 0.100 - ale.lives: 2.021\n",
      "\n",
      "Interval 308 (3070000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0255\n",
      "12 episodes - episode_reward: 21.833 [13.000, 34.000] - loss: 0.016 - mae: 2.779 - mean_q: 3.335 - mean_eps: 0.100 - ale.lives: 2.177\n",
      "\n",
      "Interval 309 (3080000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 18.308 [8.000, 29.000] - loss: 0.016 - mae: 2.786 - mean_q: 3.343 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 310 (3090000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0237\n",
      "12 episodes - episode_reward: 19.833 [11.000, 36.000] - loss: 0.016 - mae: 2.785 - mean_q: 3.341 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 311 (3100000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0233\n",
      "11 episodes - episode_reward: 19.273 [7.000, 28.000] - loss: 0.016 - mae: 2.796 - mean_q: 3.356 - mean_eps: 0.100 - ale.lives: 2.022\n",
      "\n",
      "Interval 312 (3110000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0251\n",
      "11 episodes - episode_reward: 23.636 [11.000, 32.000] - loss: 0.016 - mae: 2.807 - mean_q: 3.369 - mean_eps: 0.100 - ale.lives: 2.286\n",
      "\n",
      "Interval 313 (3120000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0255\n",
      "11 episodes - episode_reward: 23.182 [12.000, 31.000] - loss: 0.015 - mae: 2.800 - mean_q: 3.360 - mean_eps: 0.100 - ale.lives: 2.075\n",
      "\n",
      "Interval 314 (3130000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0256\n",
      "10 episodes - episode_reward: 24.300 [17.000, 33.000] - loss: 0.015 - mae: 2.795 - mean_q: 3.354 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 315 (3140000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0230\n",
      "10 episodes - episode_reward: 22.400 [9.000, 31.000] - loss: 0.016 - mae: 2.785 - mean_q: 3.341 - mean_eps: 0.100 - ale.lives: 2.098\n",
      "\n",
      "Interval 316 (3150000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0264\n",
      "12 episodes - episode_reward: 23.083 [12.000, 33.000] - loss: 0.015 - mae: 2.763 - mean_q: 3.316 - mean_eps: 0.100 - ale.lives: 2.109\n",
      "\n",
      "Interval 317 (3160000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0256\n",
      "11 episodes - episode_reward: 24.455 [12.000, 35.000] - loss: 0.015 - mae: 2.757 - mean_q: 3.309 - mean_eps: 0.100 - ale.lives: 2.040\n",
      "\n",
      "Interval 318 (3170000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0257\n",
      "10 episodes - episode_reward: 25.000 [11.000, 32.000] - loss: 0.015 - mae: 2.776 - mean_q: 3.331 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 319 (3180000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0255\n",
      "11 episodes - episode_reward: 23.545 [11.000, 33.000] - loss: 0.015 - mae: 2.769 - mean_q: 3.324 - mean_eps: 0.100 - ale.lives: 2.109\n",
      "\n",
      "Interval 320 (3190000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0246\n",
      "11 episodes - episode_reward: 22.364 [9.000, 30.000] - loss: 0.015 - mae: 2.779 - mean_q: 3.336 - mean_eps: 0.100 - ale.lives: 2.199\n",
      "\n",
      "Interval 321 (3200000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 21.455 [8.000, 31.000] - loss: 0.015 - mae: 2.800 - mean_q: 3.361 - mean_eps: 0.100 - ale.lives: 2.125\n",
      "\n",
      "Interval 322 (3210000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 20.000 [8.000, 35.000] - loss: 0.016 - mae: 2.786 - mean_q: 3.342 - mean_eps: 0.100 - ale.lives: 2.094\n",
      "\n",
      "Interval 323 (3220000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0214\n",
      "11 episodes - episode_reward: 21.455 [13.000, 32.000] - loss: 0.015 - mae: 2.788 - mean_q: 3.344 - mean_eps: 0.100 - ale.lives: 1.975\n",
      "\n",
      "Interval 324 (3230000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0252\n",
      "10 episodes - episode_reward: 22.600 [14.000, 34.000] - loss: 0.015 - mae: 2.784 - mean_q: 3.341 - mean_eps: 0.100 - ale.lives: 2.224\n",
      "\n",
      "Interval 325 (3240000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0243\n",
      "10 episodes - episode_reward: 25.600 [19.000, 31.000] - loss: 0.015 - mae: 2.776 - mean_q: 3.332 - mean_eps: 0.100 - ale.lives: 2.118\n",
      "\n",
      "Interval 326 (3250000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0254\n",
      "11 episodes - episode_reward: 23.364 [11.000, 32.000] - loss: 0.016 - mae: 2.776 - mean_q: 3.330 - mean_eps: 0.100 - ale.lives: 2.064\n",
      "\n",
      "Interval 327 (3260000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0258\n",
      "12 episodes - episode_reward: 21.500 [11.000, 34.000] - loss: 0.016 - mae: 2.782 - mean_q: 3.339 - mean_eps: 0.100 - ale.lives: 2.017\n",
      "\n",
      "Interval 328 (3270000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0242\n",
      "11 episodes - episode_reward: 22.636 [14.000, 28.000] - loss: 0.016 - mae: 2.768 - mean_q: 3.320 - mean_eps: 0.100 - ale.lives: 2.082\n",
      "\n",
      "Interval 329 (3280000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0255\n",
      "12 episodes - episode_reward: 21.333 [8.000, 27.000] - loss: 0.015 - mae: 2.759 - mean_q: 3.310 - mean_eps: 0.100 - ale.lives: 2.061\n",
      "\n",
      "Interval 330 (3290000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0259\n",
      "14 episodes - episode_reward: 18.214 [6.000, 30.000] - loss: 0.016 - mae: 2.761 - mean_q: 3.313 - mean_eps: 0.100 - ale.lives: 2.102\n",
      "\n",
      "Interval 331 (3300000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0253\n",
      "12 episodes - episode_reward: 19.250 [7.000, 28.000] - loss: 0.015 - mae: 2.752 - mean_q: 3.300 - mean_eps: 0.100 - ale.lives: 1.981\n",
      "\n",
      "Interval 332 (3310000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0260\n",
      "11 episodes - episode_reward: 25.000 [19.000, 33.000] - loss: 0.015 - mae: 2.747 - mean_q: 3.297 - mean_eps: 0.100 - ale.lives: 2.218\n",
      "\n",
      "Interval 333 (3320000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0228\n",
      "10 episodes - episode_reward: 23.500 [10.000, 33.000] - loss: 0.015 - mae: 2.754 - mean_q: 3.305 - mean_eps: 0.100 - ale.lives: 2.129\n",
      "\n",
      "Interval 334 (3330000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0242\n",
      "9 episodes - episode_reward: 24.556 [14.000, 34.000] - loss: 0.015 - mae: 2.754 - mean_q: 3.304 - mean_eps: 0.100 - ale.lives: 2.124\n",
      "\n",
      "Interval 335 (3340000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0248\n",
      "13 episodes - episode_reward: 20.154 [8.000, 31.000] - loss: 0.015 - mae: 2.751 - mean_q: 3.300 - mean_eps: 0.100 - ale.lives: 2.062\n",
      "\n",
      "Interval 336 (3350000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0239\n",
      "10 episodes - episode_reward: 23.700 [15.000, 31.000] - loss: 0.015 - mae: 2.736 - mean_q: 3.284 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 337 (3360000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0258\n",
      "13 episodes - episode_reward: 19.846 [11.000, 27.000] - loss: 0.016 - mae: 2.737 - mean_q: 3.285 - mean_eps: 0.100 - ale.lives: 2.184\n",
      "\n",
      "Interval 338 (3370000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0252\n",
      "11 episodes - episode_reward: 23.364 [14.000, 32.000] - loss: 0.015 - mae: 2.744 - mean_q: 3.294 - mean_eps: 0.100 - ale.lives: 2.191\n",
      "\n",
      "Interval 339 (3380000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0254\n",
      "12 episodes - episode_reward: 21.167 [11.000, 32.000] - loss: 0.016 - mae: 2.750 - mean_q: 3.300 - mean_eps: 0.100 - ale.lives: 2.271\n",
      "\n",
      "Interval 340 (3390000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 20.833 [8.000, 34.000] - loss: 0.015 - mae: 2.760 - mean_q: 3.312 - mean_eps: 0.100 - ale.lives: 2.004\n",
      "\n",
      "Interval 341 (3400000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0251\n",
      "10 episodes - episode_reward: 24.100 [15.000, 36.000] - loss: 0.015 - mae: 2.753 - mean_q: 3.304 - mean_eps: 0.100 - ale.lives: 2.199\n",
      "\n",
      "Interval 342 (3410000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0260\n",
      "12 episodes - episode_reward: 22.583 [12.000, 30.000] - loss: 0.015 - mae: 2.756 - mean_q: 3.307 - mean_eps: 0.100 - ale.lives: 2.153\n",
      "\n",
      "Interval 343 (3420000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0240\n",
      "11 episodes - episode_reward: 20.364 [16.000, 24.000] - loss: 0.015 - mae: 2.762 - mean_q: 3.316 - mean_eps: 0.100 - ale.lives: 2.216\n",
      "\n",
      "Interval 344 (3430000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0242\n",
      "10 episodes - episode_reward: 23.400 [13.000, 32.000] - loss: 0.015 - mae: 2.768 - mean_q: 3.322 - mean_eps: 0.100 - ale.lives: 2.229\n",
      "\n",
      "Interval 345 (3440000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0251\n",
      "10 episodes - episode_reward: 26.000 [16.000, 33.000] - loss: 0.016 - mae: 2.772 - mean_q: 3.326 - mean_eps: 0.100 - ale.lives: 2.217\n",
      "\n",
      "Interval 346 (3450000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0241\n",
      "10 episodes - episode_reward: 25.300 [19.000, 34.000] - loss: 0.017 - mae: 2.763 - mean_q: 3.315 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 347 (3460000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0243\n",
      "10 episodes - episode_reward: 23.100 [10.000, 32.000] - loss: 0.016 - mae: 2.774 - mean_q: 3.329 - mean_eps: 0.100 - ale.lives: 2.147\n",
      "\n",
      "Interval 348 (3470000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0260\n",
      "10 episodes - episode_reward: 26.600 [17.000, 33.000] - loss: 0.016 - mae: 2.764 - mean_q: 3.317 - mean_eps: 0.100 - ale.lives: 2.391\n",
      "\n",
      "Interval 349 (3480000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 20.636 [12.000, 30.000] - loss: 0.016 - mae: 2.753 - mean_q: 3.303 - mean_eps: 0.100 - ale.lives: 2.111\n",
      "\n",
      "Interval 350 (3490000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0245\n",
      "12 episodes - episode_reward: 20.500 [6.000, 32.000] - loss: 0.016 - mae: 2.750 - mean_q: 3.300 - mean_eps: 0.100 - ale.lives: 1.997\n",
      "\n",
      "Interval 351 (3500000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 21.167 [11.000, 30.000] - loss: 0.016 - mae: 2.748 - mean_q: 3.300 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 352 (3510000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0235\n",
      "12 episodes - episode_reward: 18.083 [8.000, 33.000] - loss: 0.015 - mae: 2.753 - mean_q: 3.303 - mean_eps: 0.100 - ale.lives: 2.155\n",
      "\n",
      "Interval 353 (3520000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0264\n",
      "12 episodes - episode_reward: 23.667 [15.000, 30.000] - loss: 0.015 - mae: 2.753 - mean_q: 3.303 - mean_eps: 0.100 - ale.lives: 2.142\n",
      "\n",
      "Interval 354 (3530000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 21.000 [7.000, 31.000] - loss: 0.016 - mae: 2.759 - mean_q: 3.311 - mean_eps: 0.100 - ale.lives: 2.116\n",
      "\n",
      "Interval 355 (3540000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0224\n",
      "10 episodes - episode_reward: 21.100 [13.000, 34.000] - loss: 0.016 - mae: 2.761 - mean_q: 3.313 - mean_eps: 0.100 - ale.lives: 2.194\n",
      "\n",
      "Interval 356 (3550000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0231\n",
      "10 episodes - episode_reward: 23.700 [12.000, 33.000] - loss: 0.015 - mae: 2.775 - mean_q: 3.329 - mean_eps: 0.100 - ale.lives: 2.208\n",
      "\n",
      "Interval 357 (3560000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0240\n",
      "13 episodes - episode_reward: 18.923 [11.000, 25.000] - loss: 0.015 - mae: 2.760 - mean_q: 3.313 - mean_eps: 0.100 - ale.lives: 2.120\n",
      "\n",
      "Interval 358 (3570000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0265\n",
      "11 episodes - episode_reward: 22.545 [14.000, 31.000] - loss: 0.015 - mae: 2.759 - mean_q: 3.311 - mean_eps: 0.100 - ale.lives: 2.204\n",
      "\n",
      "Interval 359 (3580000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0238\n",
      "12 episodes - episode_reward: 20.083 [6.000, 29.000] - loss: 0.015 - mae: 2.776 - mean_q: 3.331 - mean_eps: 0.100 - ale.lives: 2.156\n",
      "\n",
      "Interval 360 (3590000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0228\n",
      "12 episodes - episode_reward: 19.167 [10.000, 34.000] - loss: 0.015 - mae: 2.782 - mean_q: 3.339 - mean_eps: 0.100 - ale.lives: 2.091\n",
      "\n",
      "Interval 361 (3600000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0234\n",
      "12 episodes - episode_reward: 20.583 [8.000, 34.000] - loss: 0.015 - mae: 2.785 - mean_q: 3.342 - mean_eps: 0.100 - ale.lives: 2.104\n",
      "\n",
      "Interval 362 (3610000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0252\n",
      "12 episodes - episode_reward: 19.500 [7.000, 31.000] - loss: 0.015 - mae: 2.785 - mean_q: 3.342 - mean_eps: 0.100 - ale.lives: 2.129\n",
      "\n",
      "Interval 363 (3620000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0266\n",
      "11 episodes - episode_reward: 24.182 [14.000, 32.000] - loss: 0.015 - mae: 2.790 - mean_q: 3.349 - mean_eps: 0.100 - ale.lives: 2.174\n",
      "\n",
      "Interval 364 (3630000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0251\n",
      "12 episodes - episode_reward: 21.500 [12.000, 32.000] - loss: 0.015 - mae: 2.799 - mean_q: 3.361 - mean_eps: 0.100 - ale.lives: 2.095\n",
      "\n",
      "Interval 365 (3640000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0258\n",
      "11 episodes - episode_reward: 22.545 [14.000, 32.000] - loss: 0.015 - mae: 2.787 - mean_q: 3.344 - mean_eps: 0.100 - ale.lives: 2.219\n",
      "\n",
      "Interval 366 (3650000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0269\n",
      "11 episodes - episode_reward: 25.727 [17.000, 44.000] - loss: 0.016 - mae: 2.806 - mean_q: 3.368 - mean_eps: 0.100 - ale.lives: 2.205\n",
      "\n",
      "Interval 367 (3660000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0255\n",
      "11 episodes - episode_reward: 22.727 [13.000, 31.000] - loss: 0.016 - mae: 2.800 - mean_q: 3.360 - mean_eps: 0.100 - ale.lives: 2.240\n",
      "\n",
      "Interval 368 (3670000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0252\n",
      "11 episodes - episode_reward: 22.545 [14.000, 30.000] - loss: 0.015 - mae: 2.820 - mean_q: 3.385 - mean_eps: 0.100 - ale.lives: 2.294\n",
      "\n",
      "Interval 369 (3680000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0261\n",
      "13 episodes - episode_reward: 19.923 [11.000, 31.000] - loss: 0.016 - mae: 2.838 - mean_q: 3.407 - mean_eps: 0.100 - ale.lives: 2.115\n",
      "\n",
      "Interval 370 (3690000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0245\n",
      "10 episodes - episode_reward: 24.700 [10.000, 33.000] - loss: 0.016 - mae: 2.853 - mean_q: 3.423 - mean_eps: 0.100 - ale.lives: 2.281\n",
      "\n",
      "Interval 371 (3700000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 21.545 [15.000, 28.000] - loss: 0.016 - mae: 2.866 - mean_q: 3.439 - mean_eps: 0.100 - ale.lives: 2.129\n",
      "\n",
      "Interval 372 (3710000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0238\n",
      "12 episodes - episode_reward: 19.250 [10.000, 28.000] - loss: 0.015 - mae: 2.865 - mean_q: 3.438 - mean_eps: 0.100 - ale.lives: 2.136\n",
      "\n",
      "Interval 373 (3720000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0236\n",
      "10 episodes - episode_reward: 25.500 [16.000, 35.000] - loss: 0.015 - mae: 2.829 - mean_q: 3.394 - mean_eps: 0.100 - ale.lives: 2.146\n",
      "\n",
      "Interval 374 (3730000 steps performed)\n",
      "10000/10000 [==============================] - 281s 28ms/step - reward: 0.0257\n",
      "10 episodes - episode_reward: 24.800 [15.000, 34.000] - loss: 0.015 - mae: 2.830 - mean_q: 3.397 - mean_eps: 0.100 - ale.lives: 2.269\n",
      "\n",
      "Interval 375 (3740000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0266\n",
      "11 episodes - episode_reward: 25.182 [18.000, 31.000] - loss: 0.015 - mae: 2.840 - mean_q: 3.409 - mean_eps: 0.100 - ale.lives: 2.160\n",
      "\n",
      "Interval 376 (3750000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0242\n",
      "13 episodes - episode_reward: 18.462 [1.000, 32.000] - loss: 0.015 - mae: 2.843 - mean_q: 3.412 - mean_eps: 0.100 - ale.lives: 2.207\n",
      "\n",
      "Interval 377 (3760000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0249\n",
      "11 episodes - episode_reward: 23.545 [13.000, 32.000] - loss: 0.015 - mae: 2.853 - mean_q: 3.424 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 378 (3770000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0239\n",
      "10 episodes - episode_reward: 23.400 [13.000, 35.000] - loss: 0.015 - mae: 2.856 - mean_q: 3.428 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 379 (3780000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0253\n",
      "10 episodes - episode_reward: 25.100 [17.000, 33.000] - loss: 0.015 - mae: 2.856 - mean_q: 3.428 - mean_eps: 0.100 - ale.lives: 2.238\n",
      "\n",
      "Interval 380 (3790000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0264\n",
      "12 episodes - episode_reward: 22.250 [6.000, 31.000] - loss: 0.014 - mae: 2.855 - mean_q: 3.427 - mean_eps: 0.100 - ale.lives: 2.220\n",
      "\n",
      "Interval 381 (3800000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0227\n",
      "10 episodes - episode_reward: 21.800 [14.000, 28.000] - loss: 0.015 - mae: 2.870 - mean_q: 3.443 - mean_eps: 0.100 - ale.lives: 2.174\n",
      "\n",
      "Interval 382 (3810000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0248\n",
      "12 episodes - episode_reward: 21.667 [15.000, 31.000] - loss: 0.015 - mae: 2.864 - mean_q: 3.437 - mean_eps: 0.100 - ale.lives: 2.226\n",
      "\n",
      "Interval 383 (3820000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0250\n",
      "12 episodes - episode_reward: 19.333 [11.000, 28.000] - loss: 0.015 - mae: 2.862 - mean_q: 3.434 - mean_eps: 0.100 - ale.lives: 2.160\n",
      "\n",
      "Interval 384 (3830000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 22.091 [11.000, 32.000] - loss: 0.015 - mae: 2.871 - mean_q: 3.445 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 385 (3840000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0234\n",
      "12 episodes - episode_reward: 18.333 [5.000, 28.000] - loss: 0.015 - mae: 2.863 - mean_q: 3.434 - mean_eps: 0.100 - ale.lives: 2.172\n",
      "\n",
      "Interval 386 (3850000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: 0.0252\n",
      "11 episodes - episode_reward: 23.455 [11.000, 35.000] - loss: 0.015 - mae: 2.856 - mean_q: 3.427 - mean_eps: 0.100 - ale.lives: 2.210\n",
      "\n",
      "Interval 387 (3860000 steps performed)\n",
      "10000/10000 [==============================] - 282s 28ms/step - reward: 0.0249\n",
      "13 episodes - episode_reward: 19.385 [11.000, 30.000] - loss: 0.015 - mae: 2.853 - mean_q: 3.423 - mean_eps: 0.100 - ale.lives: 2.287\n",
      "\n",
      "Interval 388 (3870000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0225\n",
      "10 episodes - episode_reward: 22.300 [10.000, 36.000] - loss: 0.015 - mae: 2.844 - mean_q: 3.413 - mean_eps: 0.100 - ale.lives: 2.078\n",
      "\n",
      "Interval 389 (3880000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: 0.0237\n",
      "13 episodes - episode_reward: 18.769 [6.000, 27.000] - loss: 0.016 - mae: 2.857 - mean_q: 3.429 - mean_eps: 0.100 - ale.lives: 1.966\n",
      "\n",
      "Interval 390 (3890000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 20.636 [15.000, 27.000] - loss: 0.015 - mae: 2.870 - mean_q: 3.444 - mean_eps: 0.100 - ale.lives: 2.128\n",
      "\n",
      "Interval 391 (3900000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: 0.0241\n",
      "10 episodes - episode_reward: 25.200 [16.000, 33.000] - loss: 0.015 - mae: 2.879 - mean_q: 3.454 - mean_eps: 0.100 - ale.lives: 2.213\n",
      "\n",
      "Interval 392 (3910000 steps performed)\n",
      "10000/10000 [==============================] - 295s 30ms/step - reward: 0.0254\n",
      "12 episodes - episode_reward: 20.583 [4.000, 29.000] - loss: 0.015 - mae: 2.877 - mean_q: 3.451 - mean_eps: 0.100 - ale.lives: 2.155\n",
      "\n",
      "Interval 393 (3920000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.0249\n",
      "11 episodes - episode_reward: 21.273 [11.000, 33.000] - loss: 0.015 - mae: 2.881 - mean_q: 3.457 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 394 (3930000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: 0.0236\n",
      "13 episodes - episode_reward: 20.231 [9.000, 30.000] - loss: 0.015 - mae: 2.882 - mean_q: 3.459 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 395 (3940000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 20.182 [11.000, 27.000] - loss: 0.015 - mae: 2.876 - mean_q: 3.452 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 396 (3950000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: 0.0260\n",
      "12 episodes - episode_reward: 21.250 [6.000, 34.000] - loss: 0.015 - mae: 2.880 - mean_q: 3.456 - mean_eps: 0.100 - ale.lives: 2.009\n",
      "\n",
      "Interval 397 (3960000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0247\n",
      "12 episodes - episode_reward: 21.917 [15.000, 30.000] - loss: 0.015 - mae: 2.864 - mean_q: 3.437 - mean_eps: 0.100 - ale.lives: 2.271\n",
      "\n",
      "Interval 398 (3970000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: 0.0257\n",
      "10 episodes - episode_reward: 23.900 [18.000, 30.000] - loss: 0.015 - mae: 2.864 - mean_q: 3.437 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 399 (3980000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0252\n",
      "12 episodes - episode_reward: 21.167 [10.000, 30.000] - loss: 0.015 - mae: 2.864 - mean_q: 3.436 - mean_eps: 0.100 - ale.lives: 2.126\n",
      "\n",
      "Interval 400 (3990000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: 0.0247\n",
      "13 episodes - episode_reward: 19.846 [5.000, 34.000] - loss: 0.015 - mae: 2.866 - mean_q: 3.441 - mean_eps: 0.100 - ale.lives: 2.220\n",
      "\n",
      "Interval 401 (4000000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 20.091 [13.000, 41.000] - loss: 0.015 - mae: 2.844 - mean_q: 3.414 - mean_eps: 0.100 - ale.lives: 2.261\n",
      "\n",
      "Interval 402 (4010000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: 0.0249\n",
      "10 episodes - episode_reward: 25.800 [13.000, 34.000] - loss: 0.015 - mae: 2.829 - mean_q: 3.395 - mean_eps: 0.100 - ale.lives: 1.968\n",
      "\n",
      "Interval 403 (4020000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0252\n",
      "12 episodes - episode_reward: 19.917 [9.000, 34.000] - loss: 0.015 - mae: 2.851 - mean_q: 3.422 - mean_eps: 0.100 - ale.lives: 2.240\n",
      "\n",
      "Interval 404 (4030000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0241\n",
      "10 episodes - episode_reward: 23.400 [14.000, 35.000] - loss: 0.014 - mae: 2.858 - mean_q: 3.429 - mean_eps: 0.100 - ale.lives: 2.255\n",
      "\n",
      "Interval 405 (4040000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0249\n",
      "13 episodes - episode_reward: 20.231 [9.000, 30.000] - loss: 0.015 - mae: 2.863 - mean_q: 3.435 - mean_eps: 0.100 - ale.lives: 2.009\n",
      "\n",
      "Interval 406 (4050000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: 0.0232\n",
      "13 episodes - episode_reward: 17.846 [6.000, 33.000] - loss: 0.015 - mae: 2.865 - mean_q: 3.438 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 407 (4060000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0245\n",
      "12 episodes - episode_reward: 18.583 [7.000, 35.000] - loss: 0.015 - mae: 2.876 - mean_q: 3.450 - mean_eps: 0.100 - ale.lives: 2.001\n",
      "\n",
      "Interval 408 (4070000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0249\n",
      "12 episodes - episode_reward: 21.667 [11.000, 31.000] - loss: 0.015 - mae: 2.872 - mean_q: 3.446 - mean_eps: 0.100 - ale.lives: 2.184\n",
      "\n",
      "Interval 409 (4080000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0253\n",
      "13 episodes - episode_reward: 20.231 [3.000, 30.000] - loss: 0.015 - mae: 2.859 - mean_q: 3.430 - mean_eps: 0.100 - ale.lives: 2.057\n",
      "\n",
      "Interval 410 (4090000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0249\n",
      "12 episodes - episode_reward: 21.333 [14.000, 30.000] - loss: 0.015 - mae: 2.854 - mean_q: 3.425 - mean_eps: 0.100 - ale.lives: 2.266\n",
      "\n",
      "Interval 411 (4100000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 16.769 [5.000, 34.000] - loss: 0.015 - mae: 2.860 - mean_q: 3.432 - mean_eps: 0.100 - ale.lives: 2.135\n",
      "\n",
      "Interval 412 (4110000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0246\n",
      "11 episodes - episode_reward: 21.818 [14.000, 33.000] - loss: 0.015 - mae: 2.868 - mean_q: 3.441 - mean_eps: 0.100 - ale.lives: 2.076\n",
      "\n",
      "Interval 413 (4120000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0259\n",
      "11 episodes - episode_reward: 24.545 [11.000, 35.000] - loss: 0.015 - mae: 2.850 - mean_q: 3.419 - mean_eps: 0.100 - ale.lives: 2.152\n",
      "\n",
      "Interval 414 (4130000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0239\n",
      "14 episodes - episode_reward: 17.643 [5.000, 30.000] - loss: 0.015 - mae: 2.846 - mean_q: 3.413 - mean_eps: 0.100 - ale.lives: 2.113\n",
      "\n",
      "Interval 415 (4140000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0254\n",
      "12 episodes - episode_reward: 19.083 [4.000, 33.000] - loss: 0.015 - mae: 2.845 - mean_q: 3.413 - mean_eps: 0.100 - ale.lives: 2.069\n",
      "\n",
      "Interval 416 (4150000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0237\n",
      "12 episodes - episode_reward: 21.417 [12.000, 32.000] - loss: 0.015 - mae: 2.842 - mean_q: 3.409 - mean_eps: 0.100 - ale.lives: 1.910\n",
      "\n",
      "Interval 417 (4160000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0252\n",
      "12 episodes - episode_reward: 20.917 [12.000, 30.000] - loss: 0.015 - mae: 2.853 - mean_q: 3.423 - mean_eps: 0.100 - ale.lives: 2.007\n",
      "\n",
      "Interval 418 (4170000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0240\n",
      "13 episodes - episode_reward: 17.231 [8.000, 27.000] - loss: 0.015 - mae: 2.866 - mean_q: 3.438 - mean_eps: 0.100 - ale.lives: 2.031\n",
      "\n",
      "Interval 419 (4180000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0223\n",
      "11 episodes - episode_reward: 21.091 [9.000, 27.000] - loss: 0.015 - mae: 2.863 - mean_q: 3.434 - mean_eps: 0.100 - ale.lives: 2.116\n",
      "\n",
      "Interval 420 (4190000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0224\n",
      "13 episodes - episode_reward: 18.077 [10.000, 29.000] - loss: 0.015 - mae: 2.863 - mean_q: 3.434 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 421 (4200000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0245\n",
      "12 episodes - episode_reward: 18.917 [8.000, 26.000] - loss: 0.015 - mae: 2.855 - mean_q: 3.425 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 422 (4210000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0229\n",
      "12 episodes - episode_reward: 20.583 [8.000, 29.000] - loss: 0.015 - mae: 2.861 - mean_q: 3.432 - mean_eps: 0.100 - ale.lives: 1.841\n",
      "\n",
      "Interval 423 (4220000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 21.909 [9.000, 34.000] - loss: 0.015 - mae: 2.858 - mean_q: 3.429 - mean_eps: 0.100 - ale.lives: 2.074\n",
      "\n",
      "Interval 424 (4230000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 17.917 [6.000, 25.000] - loss: 0.015 - mae: 2.880 - mean_q: 3.456 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 425 (4240000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0253\n",
      "12 episodes - episode_reward: 20.667 [9.000, 30.000] - loss: 0.015 - mae: 2.874 - mean_q: 3.448 - mean_eps: 0.100 - ale.lives: 2.003\n",
      "\n",
      "Interval 426 (4250000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0245\n",
      "10 episodes - episode_reward: 23.200 [11.000, 34.000] - loss: 0.015 - mae: 2.867 - mean_q: 3.439 - mean_eps: 0.100 - ale.lives: 2.160\n",
      "\n",
      "Interval 427 (4260000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 19.462 [10.000, 30.000] - loss: 0.015 - mae: 2.859 - mean_q: 3.429 - mean_eps: 0.100 - ale.lives: 2.086\n",
      "\n",
      "Interval 428 (4270000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0232\n",
      "12 episodes - episode_reward: 17.250 [5.000, 31.000] - loss: 0.015 - mae: 2.875 - mean_q: 3.449 - mean_eps: 0.100 - ale.lives: 2.289\n",
      "\n",
      "Interval 429 (4280000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 21.417 [9.000, 33.000] - loss: 0.015 - mae: 2.865 - mean_q: 3.437 - mean_eps: 0.100 - ale.lives: 2.149\n",
      "\n",
      "Interval 430 (4290000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 20.909 [5.000, 30.000] - loss: 0.015 - mae: 2.860 - mean_q: 3.430 - mean_eps: 0.100 - ale.lives: 2.081\n",
      "\n",
      "Interval 431 (4300000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0235\n",
      "11 episodes - episode_reward: 21.182 [12.000, 31.000] - loss: 0.015 - mae: 2.847 - mean_q: 3.414 - mean_eps: 0.100 - ale.lives: 2.099\n",
      "\n",
      "Interval 432 (4310000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0259\n",
      "11 episodes - episode_reward: 22.455 [8.000, 31.000] - loss: 0.015 - mae: 2.863 - mean_q: 3.435 - mean_eps: 0.100 - ale.lives: 2.392\n",
      "\n",
      "Interval 433 (4320000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0236\n",
      "9 episodes - episode_reward: 25.667 [15.000, 35.000] - loss: 0.015 - mae: 2.870 - mean_q: 3.445 - mean_eps: 0.100 - ale.lives: 2.046\n",
      "\n",
      "Interval 434 (4330000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0237\n",
      "12 episodes - episode_reward: 20.500 [5.000, 30.000] - loss: 0.015 - mae: 2.872 - mean_q: 3.447 - mean_eps: 0.100 - ale.lives: 2.345\n",
      "\n",
      "Interval 435 (4340000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 21.000 [12.000, 30.000] - loss: 0.016 - mae: 2.874 - mean_q: 3.449 - mean_eps: 0.100 - ale.lives: 2.076\n",
      "\n",
      "Interval 436 (4350000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0257\n",
      "11 episodes - episode_reward: 21.364 [7.000, 29.000] - loss: 0.015 - mae: 2.874 - mean_q: 3.449 - mean_eps: 0.100 - ale.lives: 2.027\n",
      "\n",
      "Interval 437 (4360000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0252\n",
      "11 episodes - episode_reward: 23.909 [10.000, 33.000] - loss: 0.015 - mae: 2.864 - mean_q: 3.436 - mean_eps: 0.100 - ale.lives: 2.072\n",
      "\n",
      "Interval 438 (4370000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 21.273 [15.000, 25.000] - loss: 0.015 - mae: 2.892 - mean_q: 3.470 - mean_eps: 0.100 - ale.lives: 2.266\n",
      "\n",
      "Interval 439 (4380000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0258\n",
      "11 episodes - episode_reward: 24.455 [15.000, 30.000] - loss: 0.015 - mae: 2.902 - mean_q: 3.482 - mean_eps: 0.100 - ale.lives: 2.145\n",
      "\n",
      "Interval 440 (4390000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 21.455 [10.000, 34.000] - loss: 0.015 - mae: 2.891 - mean_q: 3.467 - mean_eps: 0.100 - ale.lives: 2.142\n",
      "\n",
      "Interval 441 (4400000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0236\n",
      "10 episodes - episode_reward: 24.300 [8.000, 35.000] - loss: 0.015 - mae: 2.907 - mean_q: 3.489 - mean_eps: 0.100 - ale.lives: 2.066\n",
      "\n",
      "Interval 442 (4410000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0278\n",
      "10 episodes - episode_reward: 26.600 [19.000, 50.000] - loss: 0.015 - mae: 2.912 - mean_q: 3.495 - mean_eps: 0.100 - ale.lives: 2.267\n",
      "\n",
      "Interval 443 (4420000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 20.250 [10.000, 32.000] - loss: 0.015 - mae: 2.918 - mean_q: 3.503 - mean_eps: 0.100 - ale.lives: 2.256\n",
      "\n",
      "Interval 444 (4430000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0279\n",
      "11 episodes - episode_reward: 24.182 [10.000, 33.000] - loss: 0.015 - mae: 2.914 - mean_q: 3.496 - mean_eps: 0.100 - ale.lives: 2.198\n",
      "\n",
      "Interval 445 (4440000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0237\n",
      "11 episodes - episode_reward: 20.909 [14.000, 27.000] - loss: 0.016 - mae: 2.922 - mean_q: 3.507 - mean_eps: 0.100 - ale.lives: 2.177\n",
      "\n",
      "Interval 446 (4450000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0226\n",
      "9 episodes - episode_reward: 25.222 [16.000, 39.000] - loss: 0.015 - mae: 2.918 - mean_q: 3.501 - mean_eps: 0.100 - ale.lives: 2.108\n",
      "\n",
      "Interval 447 (4460000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 18.385 [5.000, 29.000] - loss: 0.016 - mae: 2.929 - mean_q: 3.514 - mean_eps: 0.100 - ale.lives: 1.972\n",
      "\n",
      "Interval 448 (4470000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0271\n",
      "13 episodes - episode_reward: 21.615 [13.000, 30.000] - loss: 0.015 - mae: 2.925 - mean_q: 3.510 - mean_eps: 0.100 - ale.lives: 2.127\n",
      "\n",
      "Interval 449 (4480000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0267\n",
      "12 episodes - episode_reward: 20.833 [10.000, 30.000] - loss: 0.016 - mae: 2.934 - mean_q: 3.521 - mean_eps: 0.100 - ale.lives: 2.046\n",
      "\n",
      "Interval 450 (4490000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 19.083 [7.000, 33.000] - loss: 0.016 - mae: 2.922 - mean_q: 3.506 - mean_eps: 0.100 - ale.lives: 2.119\n",
      "\n",
      "Interval 451 (4500000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0274\n",
      "11 episodes - episode_reward: 24.545 [15.000, 33.000] - loss: 0.016 - mae: 2.928 - mean_q: 3.513 - mean_eps: 0.100 - ale.lives: 2.262\n",
      "\n",
      "Interval 452 (4510000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0246\n",
      "10 episodes - episode_reward: 25.500 [12.000, 36.000] - loss: 0.016 - mae: 2.924 - mean_q: 3.509 - mean_eps: 0.100 - ale.lives: 2.303\n",
      "\n",
      "Interval 453 (4520000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0271\n",
      "11 episodes - episode_reward: 25.545 [17.000, 32.000] - loss: 0.015 - mae: 2.921 - mean_q: 3.506 - mean_eps: 0.100 - ale.lives: 2.280\n",
      "\n",
      "Interval 454 (4530000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0262\n",
      "10 episodes - episode_reward: 23.300 [7.000, 33.000] - loss: 0.016 - mae: 2.909 - mean_q: 3.491 - mean_eps: 0.100 - ale.lives: 2.193\n",
      "\n",
      "Interval 455 (4540000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0255\n",
      "10 episodes - episode_reward: 25.200 [15.000, 33.000] - loss: 0.016 - mae: 2.920 - mean_q: 3.503 - mean_eps: 0.100 - ale.lives: 2.114\n",
      "\n",
      "Interval 456 (4550000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0251\n",
      "12 episodes - episode_reward: 22.000 [12.000, 33.000] - loss: 0.016 - mae: 2.907 - mean_q: 3.489 - mean_eps: 0.100 - ale.lives: 2.199\n",
      "\n",
      "Interval 457 (4560000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0258\n",
      "12 episodes - episode_reward: 22.917 [10.000, 34.000] - loss: 0.016 - mae: 2.921 - mean_q: 3.507 - mean_eps: 0.100 - ale.lives: 2.002\n",
      "\n",
      "Interval 458 (4570000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0258\n",
      "11 episodes - episode_reward: 22.000 [16.000, 30.000] - loss: 0.015 - mae: 2.906 - mean_q: 3.488 - mean_eps: 0.100 - ale.lives: 2.172\n",
      "\n",
      "Interval 459 (4580000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0255\n",
      "12 episodes - episode_reward: 21.667 [15.000, 31.000] - loss: 0.016 - mae: 2.886 - mean_q: 3.463 - mean_eps: 0.100 - ale.lives: 2.109\n",
      "\n",
      "Interval 460 (4590000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0235\n",
      "12 episodes - episode_reward: 19.333 [8.000, 36.000] - loss: 0.016 - mae: 2.881 - mean_q: 3.457 - mean_eps: 0.100 - ale.lives: 2.121\n",
      "\n",
      "Interval 461 (4600000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0246\n",
      "10 episodes - episode_reward: 23.000 [16.000, 32.000] - loss: 0.015 - mae: 2.872 - mean_q: 3.445 - mean_eps: 0.100 - ale.lives: 2.201\n",
      "\n",
      "Interval 462 (4610000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 22.455 [13.000, 32.000] - loss: 0.016 - mae: 2.880 - mean_q: 3.455 - mean_eps: 0.100 - ale.lives: 2.133\n",
      "\n",
      "Interval 463 (4620000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0222\n",
      "11 episodes - episode_reward: 21.455 [12.000, 31.000] - loss: 0.016 - mae: 2.872 - mean_q: 3.447 - mean_eps: 0.100 - ale.lives: 2.035\n",
      "\n",
      "Interval 464 (4630000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0239\n",
      "12 episodes - episode_reward: 19.667 [12.000, 31.000] - loss: 0.016 - mae: 2.884 - mean_q: 3.461 - mean_eps: 0.100 - ale.lives: 2.036\n",
      "\n",
      "Interval 465 (4640000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0217\n",
      "10 episodes - episode_reward: 22.100 [8.000, 34.000] - loss: 0.015 - mae: 2.882 - mean_q: 3.458 - mean_eps: 0.100 - ale.lives: 2.307\n",
      "\n",
      "Interval 466 (4650000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0237\n",
      "13 episodes - episode_reward: 19.077 [5.000, 30.000] - loss: 0.016 - mae: 2.896 - mean_q: 3.475 - mean_eps: 0.100 - ale.lives: 1.894\n",
      "\n",
      "Interval 467 (4660000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0259\n",
      "11 episodes - episode_reward: 23.182 [14.000, 34.000] - loss: 0.015 - mae: 2.879 - mean_q: 3.453 - mean_eps: 0.100 - ale.lives: 2.087\n",
      "\n",
      "Interval 468 (4670000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0214\n",
      "13 episodes - episode_reward: 16.077 [5.000, 23.000] - loss: 0.016 - mae: 2.877 - mean_q: 3.452 - mean_eps: 0.100 - ale.lives: 2.184\n",
      "\n",
      "Interval 469 (4680000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0245\n",
      "12 episodes - episode_reward: 19.750 [13.000, 29.000] - loss: 0.016 - mae: 2.878 - mean_q: 3.453 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 470 (4690000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0246\n",
      "13 episodes - episode_reward: 20.000 [9.000, 30.000] - loss: 0.016 - mae: 2.888 - mean_q: 3.465 - mean_eps: 0.100 - ale.lives: 2.027\n",
      "\n",
      "Interval 471 (4700000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0234\n",
      "13 episodes - episode_reward: 17.769 [7.000, 32.000] - loss: 0.016 - mae: 2.885 - mean_q: 3.463 - mean_eps: 0.100 - ale.lives: 2.186\n",
      "\n",
      "Interval 472 (4710000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0245\n",
      "11 episodes - episode_reward: 22.182 [11.000, 35.000] - loss: 0.016 - mae: 2.894 - mean_q: 3.474 - mean_eps: 0.100 - ale.lives: 2.100\n",
      "\n",
      "Interval 473 (4720000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0223\n",
      "11 episodes - episode_reward: 19.455 [11.000, 28.000] - loss: 0.016 - mae: 2.900 - mean_q: 3.480 - mean_eps: 0.100 - ale.lives: 2.307\n",
      "\n",
      "Interval 474 (4730000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0240\n",
      "11 episodes - episode_reward: 22.182 [12.000, 32.000] - loss: 0.016 - mae: 2.915 - mean_q: 3.497 - mean_eps: 0.100 - ale.lives: 2.175\n",
      "\n",
      "Interval 475 (4740000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0265\n",
      "11 episodes - episode_reward: 24.091 [10.000, 30.000] - loss: 0.016 - mae: 2.921 - mean_q: 3.507 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 476 (4750000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 18.417 [6.000, 28.000] - loss: 0.016 - mae: 2.927 - mean_q: 3.512 - mean_eps: 0.100 - ale.lives: 2.228\n",
      "\n",
      "Interval 477 (4760000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0235\n",
      "11 episodes - episode_reward: 21.636 [10.000, 34.000] - loss: 0.016 - mae: 2.932 - mean_q: 3.518 - mean_eps: 0.100 - ale.lives: 2.291\n",
      "\n",
      "Interval 478 (4770000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0240\n",
      "13 episodes - episode_reward: 18.000 [2.000, 31.000] - loss: 0.016 - mae: 2.927 - mean_q: 3.512 - mean_eps: 0.100 - ale.lives: 2.144\n",
      "\n",
      "Interval 479 (4780000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0239\n",
      "12 episodes - episode_reward: 20.750 [5.000, 34.000] - loss: 0.016 - mae: 2.907 - mean_q: 3.488 - mean_eps: 0.100 - ale.lives: 2.034\n",
      "\n",
      "Interval 480 (4790000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0230\n",
      "10 episodes - episode_reward: 21.900 [9.000, 31.000] - loss: 0.015 - mae: 2.908 - mean_q: 3.490 - mean_eps: 0.100 - ale.lives: 2.061\n",
      "\n",
      "Interval 481 (4800000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0257\n",
      "11 episodes - episode_reward: 23.273 [19.000, 28.000] - loss: 0.015 - mae: 2.907 - mean_q: 3.489 - mean_eps: 0.100 - ale.lives: 2.205\n",
      "\n",
      "Interval 482 (4810000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 22.364 [16.000, 32.000] - loss: 0.015 - mae: 2.903 - mean_q: 3.484 - mean_eps: 0.100 - ale.lives: 2.055\n",
      "\n",
      "Interval 483 (4820000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0255\n",
      "13 episodes - episode_reward: 20.231 [11.000, 31.000] - loss: 0.015 - mae: 2.900 - mean_q: 3.480 - mean_eps: 0.100 - ale.lives: 2.139\n",
      "\n",
      "Interval 484 (4830000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0238\n",
      "10 episodes - episode_reward: 23.000 [13.000, 33.000] - loss: 0.015 - mae: 2.900 - mean_q: 3.480 - mean_eps: 0.100 - ale.lives: 2.095\n",
      "\n",
      "Interval 485 (4840000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0238\n",
      "11 episodes - episode_reward: 20.091 [13.000, 28.000] - loss: 0.015 - mae: 2.915 - mean_q: 3.497 - mean_eps: 0.100 - ale.lives: 2.009\n",
      "\n",
      "Interval 486 (4850000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 21.500 [5.000, 34.000] - loss: 0.015 - mae: 2.916 - mean_q: 3.499 - mean_eps: 0.100 - ale.lives: 2.236\n",
      "\n",
      "Interval 487 (4860000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0249\n",
      "12 episodes - episode_reward: 21.167 [10.000, 28.000] - loss: 0.016 - mae: 2.930 - mean_q: 3.515 - mean_eps: 0.100 - ale.lives: 2.107\n",
      "\n",
      "Interval 488 (4870000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0253\n",
      "11 episodes - episode_reward: 21.182 [10.000, 28.000] - loss: 0.015 - mae: 2.931 - mean_q: 3.518 - mean_eps: 0.100 - ale.lives: 2.179\n",
      "\n",
      "Interval 489 (4880000 steps performed)\n",
      "10000/10000 [==============================] - 263s 26ms/step - reward: 0.0260\n",
      "13 episodes - episode_reward: 20.538 [10.000, 32.000] - loss: 0.015 - mae: 2.941 - mean_q: 3.530 - mean_eps: 0.100 - ale.lives: 2.084\n",
      "\n",
      "Interval 490 (4890000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 21.182 [12.000, 28.000] - loss: 0.015 - mae: 2.931 - mean_q: 3.518 - mean_eps: 0.100 - ale.lives: 2.132\n",
      "\n",
      "Interval 491 (4900000 steps performed)\n",
      "10000/10000 [==============================] - 263s 26ms/step - reward: 0.0250\n",
      "11 episodes - episode_reward: 22.727 [13.000, 33.000] - loss: 0.015 - mae: 2.911 - mean_q: 3.493 - mean_eps: 0.100 - ale.lives: 2.012\n",
      "\n",
      "Interval 492 (4910000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0252\n",
      "12 episodes - episode_reward: 20.167 [14.000, 28.000] - loss: 0.016 - mae: 2.899 - mean_q: 3.479 - mean_eps: 0.100 - ale.lives: 2.071\n",
      "\n",
      "Interval 493 (4920000 steps performed)\n",
      "10000/10000 [==============================] - 263s 26ms/step - reward: 0.0244\n",
      "11 episodes - episode_reward: 23.818 [15.000, 29.000] - loss: 0.015 - mae: 2.897 - mean_q: 3.477 - mean_eps: 0.100 - ale.lives: 2.212\n",
      "\n",
      "Interval 494 (4930000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0254\n",
      "11 episodes - episode_reward: 23.455 [11.000, 29.000] - loss: 0.015 - mae: 2.896 - mean_q: 3.476 - mean_eps: 0.100 - ale.lives: 1.872\n",
      "\n",
      "Interval 495 (4940000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0252\n",
      "11 episodes - episode_reward: 22.273 [13.000, 29.000] - loss: 0.015 - mae: 2.878 - mean_q: 3.455 - mean_eps: 0.100 - ale.lives: 2.245\n",
      "\n",
      "Interval 496 (4950000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0254\n",
      "11 episodes - episode_reward: 23.273 [16.000, 35.000] - loss: 0.015 - mae: 2.876 - mean_q: 3.452 - mean_eps: 0.100 - ale.lives: 2.117\n",
      "\n",
      "Interval 497 (4960000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 20.818 [9.000, 33.000] - loss: 0.015 - mae: 2.880 - mean_q: 3.457 - mean_eps: 0.100 - ale.lives: 2.179\n",
      "\n",
      "Interval 498 (4970000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 17.385 [10.000, 26.000] - loss: 0.016 - mae: 2.881 - mean_q: 3.456 - mean_eps: 0.100 - ale.lives: 2.045\n",
      "\n",
      "Interval 499 (4980000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0221\n",
      "11 episodes - episode_reward: 20.636 [8.000, 32.000] - loss: 0.015 - mae: 2.856 - mean_q: 3.427 - mean_eps: 0.100 - ale.lives: 1.922\n",
      "\n",
      "Interval 500 (4990000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0254\n",
      "done, took 129903.857 seconds\n",
      "\n",
      "=== ENTRENAMIENTO COMPLETADO ===\n",
      "Pesos guardados en: dqn_SpaceInvaders-v0_weights.h5f\n"
     ]
    }
   ],
   "source": [
    "# Configuración de callbacks para guardar progreso\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "\n",
    "callbacks = [\n",
    "    ModelIntervalCheckpoint(checkpoint_weights_filename, interval=500000),\n",
    "    FileLogger(log_filename, interval=100)\n",
    "]\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"\\n=== INICIANDO ENTRENAMIENTO ===\")\n",
    "print(f\"Pasos totales: 5,000,000\")\n",
    "print(f\"Checkpoints se guardarán cada 500,000 pasos\")\n",
    "print(\"\\nPara continuar desde un checkpoint:\")\n",
    "print(\"  dqn.load_weights('dqn_SpaceInvaders-v0_weights_XXXXX.h5f')\")\n",
    "print(\"\\nIniciando...\\n\")\n",
    "\n",
    "history = dqn.fit(\n",
    "    env,\n",
    "    callbacks=callbacks,\n",
    "    nb_steps=5000000,\n",
    "    log_interval=10000,\n",
    "    visualize=False\n",
    ")\n",
    "\n",
    "# Guardar pesos finales\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "print(f\"\\n=== ENTRENAMIENTO COMPLETADO ===\")\n",
    "print(f\"Pesos guardados en: {weights_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoadWeightsSection"
   },
   "source": [
    "---\n",
    "### **Cargar pesos pre-entrenados**\n",
    "\n",
    "Al tener el modelo entrenado, permite cargar los pesos aquí en lugar de entrenar desde cero.\n",
    "Esto es útil para:\n",
    "- Continuar el entrenamiento desde un checkpoint\n",
    "- Evaluar un modelo ya entrenado\n",
    "- Comparar diferentes versiones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoadWeightsCell"
   },
   "outputs": [],
   "source": [
    "# Descomentar para cargar pesos pre-entrenados\n",
    "weights_filename = 'dqn_SpaceInvaders-v0_weights.h5f'\n",
    "dqn.load_weights(weights_filename)\n",
    "print(f\"Pesos cargados desde: {weights_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TestingSection"
   },
   "source": [
    "---\n",
    "### **Evaluación del modelo (Testing)**\n",
    "\n",
    "Esta celda evalúa el rendimiento del agente entrenado en 100 episodios de test.\n",
    "\n",
    "**Objetivo:** Alcanzar **más de 20 puntos durante más de 100 episodios consecutivos**\n",
    "\n",
    "**Criterio de éxito:**\n",
    "- Ejecutar 100 episodios de evaluación\n",
    "- Contar la racha máxima de episodios consecutivos con recompensa >20\n",
    "- Si racha_máxima >= 100 → Objetivo alcanzado ✅\n",
    "\n",
    "**Interpretación de resultados:**\n",
    "- < 50 consecutivos: El agente necesita más entrenamiento\n",
    "- 50-99 consecutivos: El agente está cerca, continuar entrenamiento\n",
    "- >= 100 consecutivos: ¡Objetivo conseguido!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OHYryKd1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUACIÓN DEL MODELO ===\n",
      "Ejecutando 100 episodios de test...\n",
      "\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 11.000, steps: 615\n",
      "Episode 2: reward: 16.000, steps: 1338\n",
      "Episode 3: reward: 17.000, steps: 1045\n",
      "Episode 4: reward: 12.000, steps: 1304\n",
      "Episode 5: reward: 8.000, steps: 1216\n",
      "Episode 6: reward: 14.000, steps: 1109\n",
      "Episode 7: reward: 9.000, steps: 626\n",
      "Episode 8: reward: 9.000, steps: 917\n",
      "Episode 9: reward: 12.000, steps: 617\n",
      "Episode 10: reward: 10.000, steps: 1241\n",
      "Episode 11: reward: 10.000, steps: 925\n",
      "Episode 12: reward: 11.000, steps: 514\n",
      "Episode 13: reward: 7.000, steps: 623\n",
      "Episode 14: reward: 8.000, steps: 482\n",
      "Episode 15: reward: 13.000, steps: 807\n",
      "Episode 16: reward: 11.000, steps: 1555\n",
      "Episode 17: reward: 13.000, steps: 1296\n",
      "Episode 18: reward: 10.000, steps: 1091\n",
      "Episode 19: reward: 12.000, steps: 503\n",
      "Episode 20: reward: 6.000, steps: 599\n",
      "Episode 21: reward: 7.000, steps: 623\n",
      "Episode 22: reward: 12.000, steps: 794\n",
      "Episode 23: reward: 8.000, steps: 1567\n",
      "Episode 24: reward: 8.000, steps: 486\n",
      "Episode 25: reward: 9.000, steps: 963\n",
      "Episode 26: reward: 12.000, steps: 944\n",
      "Episode 27: reward: 9.000, steps: 924\n",
      "Episode 28: reward: 14.000, steps: 958\n",
      "Episode 29: reward: 11.000, steps: 1614\n",
      "Episode 30: reward: 14.000, steps: 1466\n",
      "Episode 31: reward: 8.000, steps: 638\n",
      "Episode 32: reward: 10.000, steps: 614\n",
      "Episode 33: reward: 10.000, steps: 1251\n",
      "Episode 34: reward: 9.000, steps: 655\n",
      "Episode 35: reward: 12.000, steps: 1347\n",
      "Episode 36: reward: 12.000, steps: 498\n",
      "Episode 37: reward: 8.000, steps: 659\n",
      "Episode 38: reward: 15.000, steps: 762\n",
      "Episode 39: reward: 13.000, steps: 750\n",
      "Episode 40: reward: 7.000, steps: 988\n",
      "Episode 41: reward: 14.000, steps: 792\n",
      "Episode 42: reward: 17.000, steps: 949\n",
      "Episode 43: reward: 10.000, steps: 638\n",
      "Episode 44: reward: 10.000, steps: 700\n",
      "Episode 45: reward: 7.000, steps: 795\n",
      "Episode 46: reward: 9.000, steps: 1494\n",
      "Episode 47: reward: 7.000, steps: 915\n",
      "Episode 48: reward: 9.000, steps: 613\n",
      "Episode 49: reward: 9.000, steps: 1108\n",
      "Episode 50: reward: 10.000, steps: 1195\n",
      "Episode 51: reward: 6.000, steps: 666\n",
      "Episode 52: reward: 9.000, steps: 1198\n",
      "Episode 53: reward: 13.000, steps: 610\n",
      "Episode 54: reward: 10.000, steps: 683\n",
      "Episode 55: reward: 16.000, steps: 1278\n",
      "Episode 56: reward: 9.000, steps: 655\n",
      "Episode 57: reward: 8.000, steps: 746\n",
      "Episode 58: reward: 17.000, steps: 940\n",
      "Episode 59: reward: 9.000, steps: 1268\n",
      "Episode 60: reward: 8.000, steps: 617\n",
      "Episode 61: reward: 10.000, steps: 613\n",
      "Episode 62: reward: 7.000, steps: 983\n",
      "Episode 63: reward: 12.000, steps: 610\n",
      "Episode 64: reward: 6.000, steps: 940\n",
      "Episode 65: reward: 8.000, steps: 937\n",
      "Episode 66: reward: 10.000, steps: 678\n",
      "Episode 67: reward: 10.000, steps: 598\n",
      "Episode 68: reward: 9.000, steps: 1360\n",
      "Episode 69: reward: 13.000, steps: 932\n",
      "Episode 70: reward: 11.000, steps: 646\n",
      "Episode 71: reward: 3.000, steps: 638\n",
      "Episode 72: reward: 10.000, steps: 801\n",
      "Episode 73: reward: 9.000, steps: 799\n",
      "Episode 74: reward: 9.000, steps: 819\n",
      "Episode 75: reward: 12.000, steps: 924\n",
      "Episode 76: reward: 8.000, steps: 635\n",
      "Episode 77: reward: 14.000, steps: 951\n",
      "Episode 78: reward: 4.000, steps: 628\n",
      "Episode 79: reward: 15.000, steps: 973\n",
      "Episode 80: reward: 7.000, steps: 606\n",
      "Episode 81: reward: 18.000, steps: 1463\n",
      "Episode 82: reward: 7.000, steps: 1599\n",
      "Episode 83: reward: 9.000, steps: 654\n",
      "Episode 84: reward: 7.000, steps: 624\n",
      "Episode 85: reward: 16.000, steps: 1182\n",
      "Episode 86: reward: 14.000, steps: 727\n",
      "Episode 87: reward: 14.000, steps: 982\n",
      "Episode 88: reward: 10.000, steps: 628\n",
      "Episode 89: reward: 7.000, steps: 679\n",
      "Episode 90: reward: 10.000, steps: 634\n",
      "Episode 91: reward: 14.000, steps: 936\n",
      "Episode 92: reward: 15.000, steps: 988\n",
      "Episode 93: reward: 6.000, steps: 617\n",
      "Episode 94: reward: 7.000, steps: 716\n",
      "Episode 95: reward: 12.000, steps: 1236\n",
      "Episode 96: reward: 7.000, steps: 792\n",
      "Episode 97: reward: 8.000, steps: 599\n",
      "Episode 98: reward: 10.000, steps: 1496\n",
      "Episode 99: reward: 8.000, steps: 745\n",
      "Episode 100: reward: 12.000, steps: 613\n",
      "\n",
      "=== RESULTADOS ===\n",
      "Recompensa media: 10.32\n",
      "Recompensa mínima: 3.00\n",
      "Recompensa máxima: 18.00\n",
      "Desviación estándar: 3.04\n",
      "\n",
      "Episodios con >20.0 puntos: 0/100\n",
      "Máximo de episodios consecutivos >20.0: 0\n",
      "\n",
      "==================================================\n",
      "❌ OBJETIVO NO ALCANZADO\n",
      "Máximo consecutivo: 0/100 episodios\n",
      "\n",
      "Sugerencias:\n",
      "  - Entrenar por más pasos (recomendado: 4-5M)\n",
      "  - Continuar entrenamiento desde checkpoint actual\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EVALUACIÓN DEL MODELO ===\")\n",
    "print(\"Ejecutando 100 episodios de test...\\n\")\n",
    "\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "test_results = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "\n",
    "rewards = test_results.history['episode_reward']\n",
    "mean_reward = np.mean(rewards)\n",
    "min_reward = np.min(rewards)\n",
    "max_reward = np.max(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "threshold = 20.0\n",
    "consecutive = 0\n",
    "max_consecutive = 0\n",
    "\n",
    "for reward in rewards:\n",
    "    if reward > threshold:\n",
    "        consecutive += 1\n",
    "        max_consecutive = max(max_consecutive, consecutive)\n",
    "    else:\n",
    "        consecutive = 0\n",
    "\n",
    "episodios_exitosos = sum(r > threshold for r in rewards)\n",
    "\n",
    "print(\"\\n=== RESULTADOS ===\")\n",
    "print(f\"Recompensa media: {mean_reward:.2f}\")\n",
    "print(f\"Recompensa mínima: {min_reward:.2f}\")\n",
    "print(f\"Recompensa máxima: {max_reward:.2f}\")\n",
    "print(f\"Desviación estándar: {std_reward:.2f}\")\n",
    "print(f\"\\nEpisodios con >{threshold} puntos: {episodios_exitosos}/100\")\n",
    "print(f\"Máximo de episodios consecutivos >{threshold}: {max_consecutive}\")\n",
    "\n",
    "objetivo_alcanzado = max_consecutive >= 100\n",
    "print(f\"\\n{'='*50}\")\n",
    "if objetivo_alcanzado:\n",
    "    print(f\"✅ OBJETIVO ALCANZADO\")\n",
    "    print(f\"El agente logró >20 puntos durante {max_consecutive} episodios consecutivos\")\n",
    "else:\n",
    "    print(f\"❌ OBJETIVO NO ALCANZADO\")\n",
    "    print(f\"Máximo consecutivo: {max_consecutive}/100 episodios\")\n",
    "    print(f\"\\nSugerencias:\")\n",
    "    print(f\"  - Entrenar por más pasos (recomendado: 4-5M)\")\n",
    "    print(f\"  - Continuar entrenamiento desde checkpoint actual\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VisualizationSection"
   },
   "source": [
    "---\n",
    "### **Visualización del agente (solo local)**\n",
    "\n",
    "Esta celda permite visualizar el agente jugando.\n",
    "**Nota:** Solo funciona en entorno local (puesto en False dado que al ejecutar el Kernel se reinicializa), no en Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VisualizationCell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizando 3 episodios...\n",
      "Testing for 3 episodes ...\n",
      "Episode 1: reward: 6.000, steps: 487\n",
      "Episode 2: reward: 9.000, steps: 754\n",
      "Episode 3: reward: 16.000, steps: 1551\n"
     ]
    }
   ],
   "source": [
    "# Visualización (solo funciona en local)\n",
    "if not IN_COLAB:\n",
    "    print(\"Visualizando 3 episodios...\")\n",
    "    dqn.test(env, nb_episodes=3, visualize=False)\n",
    "else:\n",
    "    print(\"La visualización no está disponible en Google Colab.\")\n",
    "    print(\"Para ver al agente jugar, ejecuta este notebook en local.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MetricsSection"
   },
   "source": [
    "---\n",
    "## **ANEXO: Análisis de Métricas del Entrenamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar este código para visualizar el progreso del test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Extraer datos disponibles\n",
    "episodes = list(range(1, len(test_results.history['episode_reward']) + 1))\n",
    "rewards = test_results.history['episode_reward']\n",
    "steps = test_results.history['nb_steps']\n",
    "\n",
    "# Calcular estadísticas\n",
    "episodios_exitosos = sum(r > 20 for r in rewards)\n",
    "tasa_exito = (episodios_exitosos / len(rewards)) * 100\n",
    "\n",
    "# Crear subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Recompensas por Episodio (Test)', \n",
    "                    'Steps por Episodio',\n",
    "                    'Distribución de Recompensas', \n",
    "                    'Estadísticas del Test'),\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"histogram\"}, {\"type\": \"table\"}]]  # ← CAMBIO: tabla en lugar de scatter\n",
    ")\n",
    "\n",
    "# 1. Recompensas por episodio\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=episodes, y=rewards, mode='lines+markers', \n",
    "               name='Recompensa', line=dict(color='blue', width=2),\n",
    "               marker=dict(size=10)),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=20, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=\"Objetivo (20)\", row=1, col=1)\n",
    "fig.add_hline(y=np.mean(rewards), line_dash=\"dot\", line_color=\"green\",\n",
    "              annotation_text=f\"Media ({np.mean(rewards):.2f})\", row=1, col=1)\n",
    "\n",
    "# 2. Steps por episodio\n",
    "fig.add_trace(\n",
    "    go.Bar(x=episodes, y=steps, name='Steps', marker_color='orange',\n",
    "           showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Distribución (histograma)\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rewards, nbinsx=8, name='Distribución',\n",
    "                 marker_color='purple', showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Tabla de estadísticas (SOLUCIÓN)\n",
    "fig.add_trace(\n",
    "    go.Table(\n",
    "        header=dict(\n",
    "            values=['<b>Métrica</b>', '<b>Valor</b>'],\n",
    "            fill_color='lightblue',\n",
    "            align='left',\n",
    "            font=dict(size=14, color='black')\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                ['Episodios evaluados', 'Recompensa media', 'Recompensa máxima', \n",
    "                 'Recompensa mínima', 'Desviación estándar', 'Mediana',\n",
    "                 'Steps promedio', '', 'Episodios >20 pts', 'Tasa de éxito', \n",
    "                 '', '<b>Objetivo (μ>20)</b>'],\n",
    "                [f'{len(rewards)}', f'{np.mean(rewards):.2f}', f'{np.max(rewards):.2f}',\n",
    "                 f'{np.min(rewards):.2f}', f'{np.std(rewards):.2f}', f'{np.median(rewards):.2f}',\n",
    "                 f'{np.mean(steps):.0f}', '', f'{episodios_exitosos}/{len(rewards)}', \n",
    "                 f'{tasa_exito:.0f}%', '',\n",
    "                 f'<b>{\"✅ ALCANZADO\" if np.mean(rewards) > 20 else \"❌ NO ALCANZADO\"}</b>']\n",
    "            ],\n",
    "            fill_color=[['white', 'lightgray']*6],\n",
    "            align='left',\n",
    "            font=dict(size=12),\n",
    "            height=30\n",
    "        )\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Actualizar layouts\n",
    "fig.update_xaxes(title_text=\"Episodio\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Episodio\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Recompensa\", row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Recompensa\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Steps\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Frecuencia\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, width=1200,\n",
    "    showlegend=False,\n",
    "    title_text=\"<b>Resultados del Test - DQN SpaceInvaders</b>\",\n",
    "    title_font_size=20\n",
    ")\n",
    "\n",
    "# Guardar\n",
    "fig.write_html('test_results.html')\n",
    "fig.write_image('test_results.png', width=1200, height=800)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Visualización guardada:\")\n",
    "print(\"  - test_results.html (interactivo)\")\n",
    "print(\"  - test_results.png (imagen)\")\n",
    "\n",
    "# Resumen en consola\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANÁLISIS DETALLADO DEL TEST\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n Recompensas por episodio:\")\n",
    "for i, r in enumerate(rewards, 1):\n",
    "    emoji = \"🟢\" if r > 20 else \"🟡\" if r > 10 else \"🔴\"\n",
    "    print(f\"  {emoji} Episodio {i:2d}: {r:5.2f} puntos\")\n",
    "\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\"Estadísticas agregadas:\")\n",
    "print(f\"{'─'*60}\")\n",
    "print(f\"  Recompensa media:      {np.mean(rewards):6.2f}\")\n",
    "print(f\"  Recompensa mediana:    {np.median(rewards):6.2f}\")\n",
    "print(f\"  Recompensa máxima:     {np.max(rewards):6.2f}\")\n",
    "print(f\"  Recompensa mínima:     {np.min(rewards):6.2f}\")\n",
    "print(f\"  Desviación estándar:   {np.std(rewards):6.2f}\")\n",
    "print(f\"  Steps promedio:        {np.mean(steps):6.0f}\")\n",
    "\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\" Evaluación de objetivo:\")\n",
    "print(f\"{'─'*60}\")\n",
    "print(f\"  Episodios >20 puntos:  {episodios_exitosos}/{len(rewards)} ({tasa_exito:.0f}%)\")\n",
    "print(f\"  Media vs objetivo:     {np.mean(rewards):.2f} vs 20.00\")\n",
    "print(f\"  Estado:                {'✅ ALCANZADO' if np.mean(rewards) > 20 else '❌ NO ALCANZADO'}\")\n",
    "\n",
    "if np.mean(rewards) < 20:\n",
    "    deficit = 20 - np.mean(rewards)\n",
    "    print(f\"\\n El modelo necesita mejorar {deficit:.2f} puntos para alcanzar el objetivo\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_results.png\" alt=\"Estadisticas del modelo\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar este código para visualizar el progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuar Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el modelo con los últimos pesos\n",
    "dqn.load_weights('dqn_SpaceInvaders-v0_weights.h5f')\n",
    "print(\" Pesos cargados\")\n",
    "\n",
    "# 2. Continuar entrenando más steps\n",
    "dqn.fit(\n",
    "    env, \n",
    "    nb_steps=500000,  # 500k steps adicionales\n",
    "    visualize=False, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 3. Los nuevos episodios se añadirán automáticamente al log\n",
    "print(\" Entrenamiento continuado completado!\")\n",
    "\n",
    "# 4. Evaluar\n",
    "test_results = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(f\"Media: {np.mean(test_results.history['episode_reward']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gym_gpu",
   "language": "python",
   "name": "gym_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
