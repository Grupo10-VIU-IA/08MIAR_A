{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: [Jose Aurelio Bollas Taboada]\n",
    "*   Alumno 2: [Antonio Jose Bonafede Salas]\n",
    "*   Alumno 3: [Elvis David Pachacama Cabezas]\n",
    "*   Alumno 4: [Jose Fernando Sarmiento Sarmiento]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto SpaceInvaders\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.ipynb_checkpoints', 'checkpoint', 'continuar entrenamiento.zip', 'Corrida 0', 'Corrida 1', 'Corrida 2', 'Corrida 3', 'dqn_SpaceInvaders-v0_log.json', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_500000.h5f.index', 'ppo_spaceinvaders v0.1.pt', 'ppo_spaceinvaders V0.2.pt', 'ppo_spaceinvaders.pt', 'Proyecto_DQN_SpaceInvaders.ipynb', 'Proyecto_PPO_SpaceInvaders - V0.2.ipynb', 'Proyecto_PPO_SpaceInvaders - V0.3.ipynb', 'Proyecto_PPO_SpaceInvaders - V1.0 - 5M.ipynb', 'Proyecto_PPO_SpaceInvaders - V1.0.ipynb', 'Proyecto_practico_SpaceInvaders - copia.ipynb', 'Proyecto_practico_SpaceInvaders - Exp1.ipynb', 'Proyecto_practico_SpaceInvaders - Exp2.ipynb', 'Proyecto_practico_SpaceInvaders - Exp2DUE.ipynb', 'Proyecto_practico_SpaceInvaders v0.ipynb', 'Proyecto_practico_SpaceInvaders v1.ipynb', 'Proyecto_practico_SpaceInvaders.ipynb', 'Proyecto_práctico.ipynb', 'Recomendaciones Chatgpt.txt', 'Recomendaciones Claude.txt', 'spaceinvaders_ppo_episode.gif', 'test_results.html', 'test_results.png', 'training_metrics.html', 'training_metrics.png', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbVRjvHCJ8UF"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de acciones disponibles: 6\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "print(f\"Número de acciones disponibles: {nb_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtariProcessorCell"
   },
   "source": [
    "#### Procesador Atari\n",
    "\n",
    "El procesador Atari se encarga de:\n",
    "1. Preprocesar las observaciones (redimensionar a 84x84 y convertir a escala de grises)\n",
    "2. Normalizar el estado batch (dividir por 255)\n",
    "3. Clipear las recompensas entre -1 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"Procesa cada observación: redimensiona a 84x84 y convierte a escala de grises\"\"\"\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # L = luminance (escala de grises)\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"Normaliza el batch de estados dividiendo por 255\"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"Clipea las recompensas entre -1 y 1 para estabilizar el entrenamiento\"\"\"\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "---\n",
    "### **1. Implementación de la red neuronal**\n",
    "\n",
    "Se implementa una red neuronal convolucional (CNN) basada en la arquitectura descrita en el paper de Mnih et al. (2015) \"Human-level control through deep reinforcement learning\".\n",
    "\n",
    "**Arquitectura:**\n",
    "- Capa Permute: Reorganiza las dimensiones de entrada según el formato de Keras\n",
    "- Conv2D #1: 32 filtros, kernel 8x8, stride 4 → Extrae características de bajo nivel\n",
    "- Conv2D #2: 64 filtros, kernel 4x4, stride 2 → Extrae características de nivel medio\n",
    "- Conv2D #3: 64 filtros, kernel 3x3, stride 1 → Refina características\n",
    "- Dense #1: 512 neuronas → Capa completamente conectada\n",
    "- Dense #2: nb_actions neuronas, activación lineal → Salida Q-values para cada acción\n",
    "\n",
    "**Justificación:**\n",
    "- Las capas convolucionales permiten extraer características espaciales relevantes del juego\n",
    "- Los strides progresivamente más pequeños permiten capturar detalles a diferentes escalas\n",
    "- La capa densa de 512 neuronas permite combinar las características extraídas\n",
    "- La activación lineal final es apropiada para estimar Q-values (pueden ser negativos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de datos de imagen de Keras: channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construcción del modelo CNN siguiendo la arquitectura de Mnih et al. (2015)\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "\n",
    "print(f\"Formato de datos de imagen de Keras: {K.image_data_format()}\")\n",
    "\n",
    "# Reorganizar dimensiones según el formato de Keras\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # Formato: (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # Formato: (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_data_format.')\n",
    "\n",
    "# Primera capa convolucional: detecta características básicas (bordes, colores)\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Segunda capa convolucional: detecta patrones más complejos\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Tercera capa convolucional: refina las características detectadas\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Aplanar las características para las capas densas\n",
    "model.add(Flatten())\n",
    "\n",
    "# Capa completamente conectada: combina características\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Capa de salida: un Q-value por cada acción posible\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))  # Lineal porque los Q-values pueden ser negativos\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "---\n",
    "### **2. Implementación de la solución DQN**\n",
    "\n",
    "Se implementan los componentes principales del algoritmo DQN:\n",
    "\n",
    "**Memoria de Experiencia (Experience Replay):**\n",
    "- Tamaño: 1,000,000 transiciones\n",
    "- Window length: 4 frames (captura movimiento)\n",
    "- Permite romper correlaciones temporales y reutilizar experiencias\n",
    "\n",
    "**Política de Exploración:**\n",
    "- Epsilon-greedy con decaimiento lineal\n",
    "- ε inicial: 1.0 (exploración total)\n",
    "- ε final entrenamiento: 0.1 (10% exploración)\n",
    "- ε test: 0.05 (5% exploración)\n",
    "- Pasos de decaimiento: 1,000,000 (decae gradualmente)\n",
    "\n",
    "**Configuración del Agente DQN:**\n",
    "- Warmup: 50,000 pasos (acumula experiencias antes de entrenar)\n",
    "- Gamma (γ): 0.99 (factor de descuento, valora recompensas futuras)\n",
    "- Target model update: 10,000 pasos (actualiza la red objetivo)\n",
    "- Train interval: 4 pasos (entrena cada 4 acciones)\n",
    "- Optimizer: Adam con learning rate 0.00025\n",
    "- Batch size: 32 (por defecto en keras-rl)\n",
    "\n",
    "**Justificación de hiperparámetros:**\n",
    "- Learning rate bajo (0.00025): previene oscilaciones en el aprendizaje\n",
    "- Warmup alto (50,000): asegura suficiente diversidad en la memoria inicial\n",
    "- Target update (10,000): balance entre estabilidad y adaptación\n",
    "- Train interval (4): reduce correlación sin perder mucha información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONFIGURACIÓN DEL AGENTE DQN ===\n",
      "Memoria: 1,000,000 transiciones\n",
      "Window length: 4 frames\n",
      "Política: Epsilon-greedy con decaimiento lineal\n",
      "  - ε inicial: 1.0\n",
      "  - ε final: 0.1\n",
      "  - ε test: 0.05\n",
      "Warmup: 50,000 pasos\n",
      "Gamma (γ): 0.99\n",
      "Target update: cada 10,000 pasos\n",
      "Train interval: cada 4 acciones\n",
      "Learning rate: 0.00025\n"
     ]
    }
   ],
   "source": [
    "# 1. MEMORIA DE EXPERIENCIA (Experience Replay)\n",
    "# Almacena transiciones (s, a, r, s') para romper correlaciones temporales\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "# 2. PROCESADOR\n",
    "# Preprocesa observaciones, estados y recompensas\n",
    "processor = AtariProcessor()\n",
    "\n",
    "# 3. POLÍTICA DE EXPLORACIÓN\n",
    "# Epsilon-greedy con decaimiento lineal: empieza explorando (ε=1.0) \n",
    "# y gradualmente explota (ε=0.1)\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=1.,      # ε inicial: exploración total\n",
    "    value_min=.1,      # ε final: 10% exploración, 90% explotación\n",
    "    value_test=.05,    # ε test: 5% exploración durante evaluación\n",
    "    nb_steps=1000000   # Pasos para decaer de value_max a value_min\n",
    ")\n",
    "\n",
    "# 4. AGENTE DQN\n",
    "# Configuración del algoritmo Deep Q-Network\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    processor=processor,\n",
    "    nb_steps_warmup=50000,      # Pasos de warmup antes de entrenar\n",
    "    gamma=.99,                  # Factor de descuento (importancia futuro)\n",
    "    target_model_update=10000,  # Frecuencia de actualización de red objetivo\n",
    "    train_interval=4,           # Entrena cada 4 acciones\n",
    "    delta_clip=1,               # Clip del error TD para estabilidad,\n",
    "    enable_double_dqn=True\n",
    ")\n",
    "\n",
    "# 5. COMPILACIÓN\n",
    "# Optimizer Adam con learning rate bajo para estabilidad\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n",
    "\n",
    "print(\"\\n=== CONFIGURACIÓN DEL AGENTE DQN ===\")\n",
    "print(f\"Memoria: {memory.limit:,} transiciones\")\n",
    "print(f\"Window length: {WINDOW_LENGTH} frames\")\n",
    "print(f\"Política: Epsilon-greedy con decaimiento lineal\")\n",
    "print(f\"  - ε inicial: {policy.value_max}\")\n",
    "print(f\"  - ε final: {policy.value_min}\")\n",
    "print(f\"  - ε test: {policy.value_test}\")\n",
    "print(f\"Warmup: {dqn.nb_steps_warmup:,} pasos\")\n",
    "print(f\"Gamma (γ): {dqn.gamma}\")\n",
    "print(f\"Target update: cada {dqn.target_model_update:,} pasos\")\n",
    "print(f\"Train interval: cada {dqn.train_interval} acciones\")\n",
    "print(f\"Learning rate: 0.00025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrainingSection"
   },
   "source": [
    "---\n",
    "### **Entrenamiento del agente**\n",
    "\n",
    "**Configuración del entrenamiento:**\n",
    "- Pasos totales: 2,000,000 (suficiente para convergencia en Atari)\n",
    "- Log interval: cada 10,000 pasos (monitorizar progreso)\n",
    "- Checkpoints: cada 500,000 pasos (guardar progreso)\n",
    "- Visualización: desactivada (más rápido)\n",
    "\n",
    "**Métricas a observar durante el entrenamiento:**\n",
    "- episode_reward: recompensa total por episodio (objetivo: >20)\n",
    "- loss: error de predicción de Q-values\n",
    "- mae: error absoluto medio\n",
    "- mean_q: Q-value promedio (debe aumentar con el tiempo)\n",
    "- mean_eps: epsilon actual (debe decrecer)\n",
    "\n",
    "**Nota:** El entrenamiento puede dura varias horas (12-24h en GPU, 36 en CPU).\n",
    "Se recomienda usar GPU y guardar checkpoints para continuar si se interrumpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TrainingCell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INICIANDO ENTRENAMIENTO ===\n",
      "Pasos totales: 3,000,000\n",
      "Tiempo estimado: 12-24 horas en GPU\n",
      "Checkpoints se guardarán cada 500,000 pasos\n",
      "\n",
      "Para continuar desde un checkpoint:\n",
      "  dqn.load_weights('dqn_SpaceInvaders-v0_weights_XXXXX.h5f')\n",
      "\n",
      "Iniciando...\n",
      "\n",
      "Training for 3000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   83/10000 [..............................] - ETA: 18s - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bonaf\\.conda\\envs\\gym_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0141\n",
      "13 episodes - episode_reward: 10.231 [4.000, 25.000] - ale.lives: 2.161\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0142\n",
      "14 episodes - episode_reward: 10.714 [3.000, 20.000] - ale.lives: 2.202\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0125\n",
      "13 episodes - episode_reward: 9.000 [4.000, 16.000] - ale.lives: 2.115\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0148\n",
      "14 episodes - episode_reward: 10.143 [6.000, 17.000] - ale.lives: 2.056\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 18s 2ms/step - reward: 0.0151\n",
      "12 episodes - episode_reward: 13.500 [4.000, 25.000] - ale.lives: 2.134\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: 0.0145\n",
      "15 episodes - episode_reward: 9.333 [3.000, 17.000] - loss: 0.007 - mae: 0.020 - mean_q: 0.022 - mean_eps: 0.951 - ale.lives: 2.089\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 184s 18ms/step - reward: 0.0134\n",
      "13 episodes - episode_reward: 10.615 [1.000, 22.000] - loss: 0.007 - mae: 0.018 - mean_q: 0.022 - mean_eps: 0.942 - ale.lives: 2.064\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0145\n",
      "15 episodes - episode_reward: 9.667 [4.000, 16.000] - loss: 0.007 - mae: 0.029 - mean_q: 0.038 - mean_eps: 0.933 - ale.lives: 2.146\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0131\n",
      "18 episodes - episode_reward: 7.500 [3.000, 22.000] - loss: 0.007 - mae: 0.043 - mean_q: 0.056 - mean_eps: 0.924 - ale.lives: 2.086\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: 0.0146\n",
      "13 episodes - episode_reward: 10.769 [5.000, 18.000] - loss: 0.007 - mae: 0.061 - mean_q: 0.079 - mean_eps: 0.915 - ale.lives: 2.156\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0141\n",
      "14 episodes - episode_reward: 10.000 [4.000, 19.000] - loss: 0.006 - mae: 0.077 - mean_q: 0.100 - mean_eps: 0.906 - ale.lives: 2.029\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 188s 19ms/step - reward: 0.0148\n",
      "15 episodes - episode_reward: 9.800 [4.000, 22.000] - loss: 0.007 - mae: 0.096 - mean_q: 0.124 - mean_eps: 0.897 - ale.lives: 2.063\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 189s 19ms/step - reward: 0.0136\n",
      "14 episodes - episode_reward: 10.214 [4.000, 19.000] - loss: 0.007 - mae: 0.116 - mean_q: 0.148 - mean_eps: 0.888 - ale.lives: 2.046\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 189s 19ms/step - reward: 0.0141\n",
      "13 episodes - episode_reward: 10.462 [5.000, 17.000] - loss: 0.007 - mae: 0.136 - mean_q: 0.173 - mean_eps: 0.879 - ale.lives: 2.056\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 189s 19ms/step - reward: 0.0157\n",
      "16 episodes - episode_reward: 10.188 [4.000, 29.000] - loss: 0.007 - mae: 0.156 - mean_q: 0.198 - mean_eps: 0.870 - ale.lives: 2.178\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 192s 19ms/step - reward: 0.0148\n",
      "15 episodes - episode_reward: 9.667 [3.000, 19.000] - loss: 0.007 - mae: 0.174 - mean_q: 0.220 - mean_eps: 0.861 - ale.lives: 2.151\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 192s 19ms/step - reward: 0.0152\n",
      "14 episodes - episode_reward: 10.786 [5.000, 20.000] - loss: 0.007 - mae: 0.182 - mean_q: 0.229 - mean_eps: 0.852 - ale.lives: 2.076\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: 0.0135\n",
      "16 episodes - episode_reward: 8.375 [4.000, 18.000] - loss: 0.007 - mae: 0.203 - mean_q: 0.256 - mean_eps: 0.843 - ale.lives: 2.099\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 192s 19ms/step - reward: 0.0147\n",
      "14 episodes - episode_reward: 10.571 [4.000, 23.000] - loss: 0.007 - mae: 0.222 - mean_q: 0.280 - mean_eps: 0.834 - ale.lives: 2.014\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: 0.0142\n",
      "16 episodes - episode_reward: 8.938 [3.000, 18.000] - loss: 0.007 - mae: 0.246 - mean_q: 0.310 - mean_eps: 0.825 - ale.lives: 2.013\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 195s 20ms/step - reward: 0.0158\n",
      "15 episodes - episode_reward: 9.933 [4.000, 24.000] - loss: 0.007 - mae: 0.263 - mean_q: 0.330 - mean_eps: 0.816 - ale.lives: 2.136\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 197s 20ms/step - reward: 0.0171\n",
      "14 episodes - episode_reward: 12.357 [5.000, 28.000] - loss: 0.007 - mae: 0.279 - mean_q: 0.350 - mean_eps: 0.807 - ale.lives: 2.186\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 198s 20ms/step - reward: 0.0162\n",
      "16 episodes - episode_reward: 10.750 [4.000, 20.000] - loss: 0.008 - mae: 0.309 - mean_q: 0.388 - mean_eps: 0.798 - ale.lives: 2.191\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 199s 20ms/step - reward: 0.0162\n",
      "13 episodes - episode_reward: 11.692 [4.000, 20.000] - loss: 0.008 - mae: 0.349 - mean_q: 0.435 - mean_eps: 0.789 - ale.lives: 2.156\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: 0.0154\n",
      "15 episodes - episode_reward: 10.800 [5.000, 21.000] - loss: 0.008 - mae: 0.369 - mean_q: 0.460 - mean_eps: 0.780 - ale.lives: 2.033\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: 0.0177\n",
      "13 episodes - episode_reward: 12.846 [5.000, 24.000] - loss: 0.008 - mae: 0.372 - mean_q: 0.463 - mean_eps: 0.771 - ale.lives: 2.088\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 203s 20ms/step - reward: 0.0160\n",
      "16 episodes - episode_reward: 10.500 [5.000, 19.000] - loss: 0.008 - mae: 0.387 - mean_q: 0.482 - mean_eps: 0.762 - ale.lives: 2.108\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 204s 20ms/step - reward: 0.0175\n",
      "16 episodes - episode_reward: 10.812 [4.000, 20.000] - loss: 0.008 - mae: 0.396 - mean_q: 0.493 - mean_eps: 0.753 - ale.lives: 2.006\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 205s 20ms/step - reward: 0.0159\n",
      "15 episodes - episode_reward: 10.400 [6.000, 14.000] - loss: 0.008 - mae: 0.418 - mean_q: 0.522 - mean_eps: 0.744 - ale.lives: 2.141\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 206s 21ms/step - reward: 0.0164\n",
      "13 episodes - episode_reward: 12.615 [6.000, 24.000] - loss: 0.009 - mae: 0.450 - mean_q: 0.560 - mean_eps: 0.735 - ale.lives: 2.127\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 206s 21ms/step - reward: 0.0171\n",
      "16 episodes - episode_reward: 11.062 [4.000, 20.000] - loss: 0.009 - mae: 0.469 - mean_q: 0.584 - mean_eps: 0.726 - ale.lives: 2.064\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: 0.0174\n",
      "14 episodes - episode_reward: 11.643 [4.000, 25.000] - loss: 0.009 - mae: 0.506 - mean_q: 0.627 - mean_eps: 0.717 - ale.lives: 2.218\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: 0.0184\n",
      "14 episodes - episode_reward: 13.429 [6.000, 20.000] - loss: 0.010 - mae: 0.531 - mean_q: 0.658 - mean_eps: 0.708 - ale.lives: 2.255\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0153\n",
      "16 episodes - episode_reward: 10.062 [5.000, 23.000] - loss: 0.010 - mae: 0.566 - mean_q: 0.701 - mean_eps: 0.699 - ale.lives: 2.044\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0168\n",
      "16 episodes - episode_reward: 9.938 [4.000, 22.000] - loss: 0.010 - mae: 0.587 - mean_q: 0.727 - mean_eps: 0.690 - ale.lives: 2.193\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0175\n",
      "15 episodes - episode_reward: 11.933 [6.000, 22.000] - loss: 0.010 - mae: 0.612 - mean_q: 0.758 - mean_eps: 0.681 - ale.lives: 2.177\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: 0.0186\n",
      "15 episodes - episode_reward: 12.867 [7.000, 19.000] - loss: 0.011 - mae: 0.659 - mean_q: 0.815 - mean_eps: 0.672 - ale.lives: 2.083\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0176\n",
      "14 episodes - episode_reward: 12.500 [8.000, 18.000] - loss: 0.011 - mae: 0.692 - mean_q: 0.855 - mean_eps: 0.663 - ale.lives: 2.072\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0172\n",
      "14 episodes - episode_reward: 11.929 [4.000, 20.000] - loss: 0.011 - mae: 0.714 - mean_q: 0.882 - mean_eps: 0.654 - ale.lives: 2.201\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0183\n",
      "14 episodes - episode_reward: 13.500 [6.000, 24.000] - loss: 0.011 - mae: 0.730 - mean_q: 0.901 - mean_eps: 0.645 - ale.lives: 1.933\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0169\n",
      "16 episodes - episode_reward: 10.062 [4.000, 23.000] - loss: 0.012 - mae: 0.757 - mean_q: 0.934 - mean_eps: 0.636 - ale.lives: 2.174\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0170\n",
      "13 episodes - episode_reward: 13.154 [7.000, 19.000] - loss: 0.012 - mae: 0.776 - mean_q: 0.957 - mean_eps: 0.627 - ale.lives: 2.122\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0179\n",
      "15 episodes - episode_reward: 12.333 [4.000, 25.000] - loss: 0.012 - mae: 0.818 - mean_q: 1.007 - mean_eps: 0.618 - ale.lives: 2.114\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0186\n",
      "14 episodes - episode_reward: 12.786 [6.000, 26.000] - loss: 0.012 - mae: 0.850 - mean_q: 1.045 - mean_eps: 0.609 - ale.lives: 2.174\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 0.0181\n",
      "14 episodes - episode_reward: 13.500 [6.000, 23.000] - loss: 0.012 - mae: 0.888 - mean_q: 1.093 - mean_eps: 0.600 - ale.lives: 2.306\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0190\n",
      "13 episodes - episode_reward: 14.000 [9.000, 30.000] - loss: 0.012 - mae: 0.924 - mean_q: 1.135 - mean_eps: 0.591 - ale.lives: 2.157\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 230s 23ms/step - reward: 0.0184\n",
      "15 episodes - episode_reward: 12.333 [6.000, 21.000] - loss: 0.012 - mae: 0.954 - mean_q: 1.170 - mean_eps: 0.582 - ale.lives: 2.097\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 230s 23ms/step - reward: 0.0185\n",
      "15 episodes - episode_reward: 12.600 [6.000, 19.000] - loss: 0.012 - mae: 0.984 - mean_q: 1.207 - mean_eps: 0.573 - ale.lives: 2.116\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 231s 23ms/step - reward: 0.0192\n",
      "16 episodes - episode_reward: 11.750 [4.000, 18.000] - loss: 0.012 - mae: 1.016 - mean_q: 1.244 - mean_eps: 0.564 - ale.lives: 2.087\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 234s 23ms/step - reward: 0.0177\n",
      "14 episodes - episode_reward: 13.143 [6.000, 25.000] - loss: 0.013 - mae: 1.049 - mean_q: 1.283 - mean_eps: 0.555 - ale.lives: 2.135\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 232s 23ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 14.231 [9.000, 22.000] - loss: 0.012 - mae: 1.059 - mean_q: 1.294 - mean_eps: 0.546 - ale.lives: 2.211\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 234s 23ms/step - reward: 0.0199\n",
      "15 episodes - episode_reward: 13.067 [7.000, 24.000] - loss: 0.013 - mae: 1.098 - mean_q: 1.341 - mean_eps: 0.537 - ale.lives: 2.097\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0195\n",
      "12 episodes - episode_reward: 17.000 [6.000, 31.000] - loss: 0.013 - mae: 1.113 - mean_q: 1.359 - mean_eps: 0.528 - ale.lives: 2.034\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 237s 24ms/step - reward: 0.0199\n",
      "13 episodes - episode_reward: 14.846 [6.000, 26.000] - loss: 0.013 - mae: 1.149 - mean_q: 1.402 - mean_eps: 0.519 - ale.lives: 2.195\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 237s 24ms/step - reward: 0.0187\n",
      "15 episodes - episode_reward: 12.333 [6.000, 21.000] - loss: 0.013 - mae: 1.161 - mean_q: 1.416 - mean_eps: 0.510 - ale.lives: 2.236\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 240s 24ms/step - reward: 0.0186\n",
      "13 episodes - episode_reward: 14.154 [7.000, 21.000] - loss: 0.013 - mae: 1.209 - mean_q: 1.475 - mean_eps: 0.501 - ale.lives: 2.101\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 241s 24ms/step - reward: 0.0194\n",
      "13 episodes - episode_reward: 15.692 [8.000, 28.000] - loss: 0.013 - mae: 1.184 - mean_q: 1.443 - mean_eps: 0.492 - ale.lives: 2.121\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 242s 24ms/step - reward: 0.0199\n",
      "13 episodes - episode_reward: 14.615 [9.000, 24.000] - loss: 0.013 - mae: 1.212 - mean_q: 1.478 - mean_eps: 0.483 - ale.lives: 2.100\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 243s 24ms/step - reward: 0.0204\n",
      "15 episodes - episode_reward: 13.733 [7.000, 23.000] - loss: 0.014 - mae: 1.251 - mean_q: 1.525 - mean_eps: 0.474 - ale.lives: 2.193\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 246s 25ms/step - reward: 0.0192\n",
      "14 episodes - episode_reward: 13.429 [7.000, 21.000] - loss: 0.014 - mae: 1.272 - mean_q: 1.549 - mean_eps: 0.465 - ale.lives: 2.117\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 247s 25ms/step - reward: 0.0197\n",
      "16 episodes - episode_reward: 12.938 [4.000, 26.000] - loss: 0.013 - mae: 1.282 - mean_q: 1.561 - mean_eps: 0.456 - ale.lives: 2.064\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: 0.0182\n",
      "16 episodes - episode_reward: 11.312 [7.000, 16.000] - loss: 0.014 - mae: 1.334 - mean_q: 1.624 - mean_eps: 0.447 - ale.lives: 2.148\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: 0.0198\n",
      "14 episodes - episode_reward: 14.071 [11.000, 17.000] - loss: 0.014 - mae: 1.338 - mean_q: 1.629 - mean_eps: 0.438 - ale.lives: 2.155\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 252s 25ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.154 [8.000, 22.000] - loss: 0.014 - mae: 1.364 - mean_q: 1.661 - mean_eps: 0.429 - ale.lives: 2.092\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 252s 25ms/step - reward: 0.0197\n",
      "12 episodes - episode_reward: 17.167 [8.000, 30.000] - loss: 0.015 - mae: 1.378 - mean_q: 1.678 - mean_eps: 0.420 - ale.lives: 2.059\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 254s 25ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 20.000 [13.000, 28.000] - loss: 0.014 - mae: 1.365 - mean_q: 1.662 - mean_eps: 0.411 - ale.lives: 2.100\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 255s 25ms/step - reward: 0.0216\n",
      "15 episodes - episode_reward: 14.333 [9.000, 22.000] - loss: 0.014 - mae: 1.420 - mean_q: 1.727 - mean_eps: 0.402 - ale.lives: 2.053\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: 0.0214\n",
      "14 episodes - episode_reward: 15.429 [9.000, 21.000] - loss: 0.015 - mae: 1.477 - mean_q: 1.796 - mean_eps: 0.393 - ale.lives: 2.133\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 258s 26ms/step - reward: 0.0210\n",
      "13 episodes - episode_reward: 16.385 [9.000, 31.000] - loss: 0.016 - mae: 1.492 - mean_q: 1.813 - mean_eps: 0.384 - ale.lives: 2.122\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0202\n",
      "12 episodes - episode_reward: 17.167 [9.000, 32.000] - loss: 0.015 - mae: 1.497 - mean_q: 1.820 - mean_eps: 0.375 - ale.lives: 2.059\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: 0.0187\n",
      "12 episodes - episode_reward: 14.667 [7.000, 25.000] - loss: 0.016 - mae: 1.524 - mean_q: 1.853 - mean_eps: 0.366 - ale.lives: 2.189\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: 0.0207\n",
      "14 episodes - episode_reward: 14.929 [8.000, 23.000] - loss: 0.016 - mae: 1.546 - mean_q: 1.878 - mean_eps: 0.357 - ale.lives: 2.300\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.538 [4.000, 24.000] - loss: 0.015 - mae: 1.550 - mean_q: 1.883 - mean_eps: 0.348 - ale.lives: 2.219\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0222\n",
      "13 episodes - episode_reward: 17.154 [10.000, 23.000] - loss: 0.015 - mae: 1.581 - mean_q: 1.920 - mean_eps: 0.339 - ale.lives: 2.126\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0199\n",
      "14 episodes - episode_reward: 14.429 [4.000, 24.000] - loss: 0.016 - mae: 1.606 - mean_q: 1.948 - mean_eps: 0.330 - ale.lives: 2.141\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0200\n",
      "13 episodes - episode_reward: 14.308 [8.000, 23.000] - loss: 0.016 - mae: 1.596 - mean_q: 1.936 - mean_eps: 0.321 - ale.lives: 2.117\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 270s 27ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 18.333 [9.000, 33.000] - loss: 0.015 - mae: 1.585 - mean_q: 1.921 - mean_eps: 0.312 - ale.lives: 2.053\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 18.923 [8.000, 27.000] - loss: 0.016 - mae: 1.590 - mean_q: 1.929 - mean_eps: 0.303 - ale.lives: 2.146\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 273s 27ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 19.000 [7.000, 35.000] - loss: 0.016 - mae: 1.590 - mean_q: 1.928 - mean_eps: 0.294 - ale.lives: 2.223\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0189\n",
      "14 episodes - episode_reward: 14.286 [8.000, 23.000] - loss: 0.016 - mae: 1.632 - mean_q: 1.978 - mean_eps: 0.285 - ale.lives: 2.097\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 275s 28ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 17.833 [10.000, 28.000] - loss: 0.016 - mae: 1.624 - mean_q: 1.968 - mean_eps: 0.276 - ale.lives: 2.103\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0219\n",
      "14 episodes - episode_reward: 15.929 [6.000, 27.000] - loss: 0.016 - mae: 1.657 - mean_q: 2.008 - mean_eps: 0.267 - ale.lives: 2.164\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0202\n",
      "11 episodes - episode_reward: 16.636 [5.000, 26.000] - loss: 0.016 - mae: 1.675 - mean_q: 2.030 - mean_eps: 0.258 - ale.lives: 2.151\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0207\n",
      "13 episodes - episode_reward: 17.615 [8.000, 31.000] - loss: 0.016 - mae: 1.666 - mean_q: 2.020 - mean_eps: 0.249 - ale.lives: 2.147\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 282s 28ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 18.083 [5.000, 28.000] - loss: 0.016 - mae: 1.701 - mean_q: 2.062 - mean_eps: 0.240 - ale.lives: 2.090\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 282s 28ms/step - reward: 0.0210\n",
      "13 episodes - episode_reward: 16.154 [5.000, 29.000] - loss: 0.017 - mae: 1.721 - mean_q: 2.086 - mean_eps: 0.231 - ale.lives: 1.973\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 284s 28ms/step - reward: 0.0207\n",
      "13 episodes - episode_reward: 15.615 [8.000, 26.000] - loss: 0.016 - mae: 1.708 - mean_q: 2.070 - mean_eps: 0.222 - ale.lives: 2.118\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 18.000 [10.000, 33.000] - loss: 0.016 - mae: 1.725 - mean_q: 2.088 - mean_eps: 0.213 - ale.lives: 2.254\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 287s 29ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 17.846 [8.000, 29.000] - loss: 0.017 - mae: 1.716 - mean_q: 2.078 - mean_eps: 0.204 - ale.lives: 2.091\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 288s 29ms/step - reward: 0.0190\n",
      "13 episodes - episode_reward: 14.308 [4.000, 27.000] - loss: 0.016 - mae: 1.708 - mean_q: 2.069 - mean_eps: 0.195 - ale.lives: 2.113\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 288s 29ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 20.545 [11.000, 31.000] - loss: 0.016 - mae: 1.703 - mean_q: 2.062 - mean_eps: 0.186 - ale.lives: 2.144\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 291s 29ms/step - reward: 0.0230\n",
      "13 episodes - episode_reward: 17.077 [8.000, 26.000] - loss: 0.016 - mae: 1.726 - mean_q: 2.090 - mean_eps: 0.177 - ale.lives: 2.040\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 293s 29ms/step - reward: 0.0228\n",
      "13 episodes - episode_reward: 16.308 [7.000, 25.000] - loss: 0.017 - mae: 1.746 - mean_q: 2.113 - mean_eps: 0.168 - ale.lives: 1.990\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 296s 30ms/step - reward: 0.0201\n",
      "13 episodes - episode_reward: 17.846 [9.000, 32.000] - loss: 0.016 - mae: 1.747 - mean_q: 2.116 - mean_eps: 0.159 - ale.lives: 2.108\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 296s 30ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 14.769 [10.000, 28.000] - loss: 0.017 - mae: 1.762 - mean_q: 2.135 - mean_eps: 0.150 - ale.lives: 2.210\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 296s 30ms/step - reward: 0.0235\n",
      "12 episodes - episode_reward: 19.667 [12.000, 25.000] - loss: 0.017 - mae: 1.802 - mean_q: 2.182 - mean_eps: 0.141 - ale.lives: 2.169\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 299s 30ms/step - reward: 0.0231\n",
      "12 episodes - episode_reward: 20.417 [7.000, 34.000] - loss: 0.017 - mae: 1.820 - mean_q: 2.205 - mean_eps: 0.132 - ale.lives: 2.023\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 301s 30ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 17.231 [10.000, 25.000] - loss: 0.017 - mae: 1.826 - mean_q: 2.212 - mean_eps: 0.123 - ale.lives: 2.179\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 304s 30ms/step - reward: 0.0213\n",
      "14 episodes - episode_reward: 15.643 [6.000, 25.000] - loss: 0.017 - mae: 1.873 - mean_q: 2.268 - mean_eps: 0.114 - ale.lives: 2.051\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 305s 30ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 17.333 [7.000, 32.000] - loss: 0.018 - mae: 1.901 - mean_q: 2.302 - mean_eps: 0.105 - ale.lives: 2.007\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 18.833 [8.000, 29.000] - loss: 0.018 - mae: 1.927 - mean_q: 2.334 - mean_eps: 0.100 - ale.lives: 2.010\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0210\n",
      "11 episodes - episode_reward: 18.273 [10.000, 24.000] - loss: 0.018 - mae: 1.929 - mean_q: 2.335 - mean_eps: 0.100 - ale.lives: 2.036\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 16.538 [8.000, 27.000] - loss: 0.018 - mae: 1.954 - mean_q: 2.365 - mean_eps: 0.100 - ale.lives: 2.087\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0203\n",
      "11 episodes - episode_reward: 18.636 [7.000, 34.000] - loss: 0.018 - mae: 1.975 - mean_q: 2.389 - mean_eps: 0.100 - ale.lives: 2.028\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0199\n",
      "11 episodes - episode_reward: 16.818 [9.000, 28.000] - loss: 0.018 - mae: 1.985 - mean_q: 2.400 - mean_eps: 0.100 - ale.lives: 1.940\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0211\n",
      "12 episodes - episode_reward: 19.500 [9.000, 26.000] - loss: 0.017 - mae: 1.979 - mean_q: 2.394 - mean_eps: 0.100 - ale.lives: 2.121\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0197\n",
      "13 episodes - episode_reward: 14.462 [4.000, 27.000] - loss: 0.018 - mae: 2.000 - mean_q: 2.418 - mean_eps: 0.100 - ale.lives: 2.128\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0214\n",
      "14 episodes - episode_reward: 15.857 [4.000, 24.000] - loss: 0.017 - mae: 1.994 - mean_q: 2.409 - mean_eps: 0.100 - ale.lives: 2.218\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0217\n",
      "12 episodes - episode_reward: 16.583 [9.000, 30.000] - loss: 0.018 - mae: 2.001 - mean_q: 2.419 - mean_eps: 0.100 - ale.lives: 2.078\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0196\n",
      "14 episodes - episode_reward: 15.286 [4.000, 24.000] - loss: 0.018 - mae: 1.992 - mean_q: 2.405 - mean_eps: 0.100 - ale.lives: 1.993\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0197\n",
      "13 episodes - episode_reward: 14.769 [5.000, 24.000] - loss: 0.017 - mae: 2.018 - mean_q: 2.438 - mean_eps: 0.100 - ale.lives: 2.038\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 19.727 [7.000, 30.000] - loss: 0.018 - mae: 2.023 - mean_q: 2.446 - mean_eps: 0.100 - ale.lives: 2.045\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0196\n",
      "12 episodes - episode_reward: 16.917 [8.000, 29.000] - loss: 0.018 - mae: 2.014 - mean_q: 2.435 - mean_eps: 0.100 - ale.lives: 2.045\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0210\n",
      "12 episodes - episode_reward: 16.583 [7.000, 25.000] - loss: 0.018 - mae: 2.033 - mean_q: 2.457 - mean_eps: 0.100 - ale.lives: 2.076\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0208\n",
      "12 episodes - episode_reward: 18.083 [7.000, 31.000] - loss: 0.018 - mae: 2.041 - mean_q: 2.467 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0199\n",
      "11 episodes - episode_reward: 17.545 [9.000, 25.000] - loss: 0.019 - mae: 2.069 - mean_q: 2.500 - mean_eps: 0.100 - ale.lives: 2.028\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0221\n",
      "13 episodes - episode_reward: 17.000 [7.000, 25.000] - loss: 0.018 - mae: 2.084 - mean_q: 2.518 - mean_eps: 0.100 - ale.lives: 2.258\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0206\n",
      "13 episodes - episode_reward: 16.538 [6.000, 30.000] - loss: 0.019 - mae: 2.121 - mean_q: 2.562 - mean_eps: 0.100 - ale.lives: 2.103\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0176\n",
      "13 episodes - episode_reward: 11.846 [7.000, 21.000] - loss: 0.020 - mae: 2.124 - mean_q: 2.567 - mean_eps: 0.100 - ale.lives: 2.074\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0197\n",
      "14 episodes - episode_reward: 14.643 [3.000, 28.000] - loss: 0.019 - mae: 2.130 - mean_q: 2.573 - mean_eps: 0.100 - ale.lives: 2.019\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0198\n",
      "13 episodes - episode_reward: 14.077 [5.000, 21.000] - loss: 0.019 - mae: 2.138 - mean_q: 2.583 - mean_eps: 0.100 - ale.lives: 1.927\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 19.583 [11.000, 32.000] - loss: 0.019 - mae: 2.158 - mean_q: 2.606 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 18.417 [6.000, 35.000] - loss: 0.019 - mae: 2.170 - mean_q: 2.620 - mean_eps: 0.100 - ale.lives: 2.049\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0241\n",
      "10 episodes - episode_reward: 24.600 [13.000, 33.000] - loss: 0.019 - mae: 2.170 - mean_q: 2.622 - mean_eps: 0.100 - ale.lives: 1.922\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 16.000 [7.000, 28.000] - loss: 0.018 - mae: 2.171 - mean_q: 2.622 - mean_eps: 0.100 - ale.lives: 2.057\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0173\n",
      "10 episodes - episode_reward: 17.100 [5.000, 30.000] - loss: 0.018 - mae: 2.176 - mean_q: 2.628 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0206\n",
      "11 episodes - episode_reward: 18.000 [10.000, 26.000] - loss: 0.019 - mae: 2.185 - mean_q: 2.640 - mean_eps: 0.100 - ale.lives: 1.996\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 18.917 [11.000, 34.000] - loss: 0.018 - mae: 2.211 - mean_q: 2.671 - mean_eps: 0.100 - ale.lives: 2.133\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0224\n",
      "12 episodes - episode_reward: 17.417 [9.000, 30.000] - loss: 0.019 - mae: 2.241 - mean_q: 2.707 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 20.250 [13.000, 30.000] - loss: 0.019 - mae: 2.248 - mean_q: 2.717 - mean_eps: 0.100 - ale.lives: 1.960\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0220\n",
      "14 episodes - episode_reward: 16.500 [11.000, 26.000] - loss: 0.018 - mae: 2.272 - mean_q: 2.744 - mean_eps: 0.100 - ale.lives: 2.034\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0237\n",
      "11 episodes - episode_reward: 20.727 [11.000, 28.000] - loss: 0.018 - mae: 2.250 - mean_q: 2.718 - mean_eps: 0.100 - ale.lives: 2.171\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0218\n",
      "13 episodes - episode_reward: 16.923 [10.000, 30.000] - loss: 0.018 - mae: 2.271 - mean_q: 2.742 - mean_eps: 0.100 - ale.lives: 2.163\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0233\n",
      "13 episodes - episode_reward: 18.000 [5.000, 33.000] - loss: 0.018 - mae: 2.283 - mean_q: 2.757 - mean_eps: 0.100 - ale.lives: 2.261\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0245\n",
      "12 episodes - episode_reward: 20.833 [12.000, 40.000] - loss: 0.019 - mae: 2.303 - mean_q: 2.781 - mean_eps: 0.100 - ale.lives: 1.996\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0250\n",
      "13 episodes - episode_reward: 17.769 [8.000, 28.000] - loss: 0.019 - mae: 2.310 - mean_q: 2.790 - mean_eps: 0.100 - ale.lives: 2.050\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0247\n",
      "12 episodes - episode_reward: 22.083 [9.000, 35.000] - loss: 0.018 - mae: 2.286 - mean_q: 2.760 - mean_eps: 0.100 - ale.lives: 2.140\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0228\n",
      "12 episodes - episode_reward: 18.417 [8.000, 35.000] - loss: 0.018 - mae: 2.291 - mean_q: 2.764 - mean_eps: 0.100 - ale.lives: 2.043\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0199\n",
      "13 episodes - episode_reward: 15.308 [5.000, 30.000] - loss: 0.019 - mae: 2.316 - mean_q: 2.796 - mean_eps: 0.100 - ale.lives: 2.135\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0202\n",
      "13 episodes - episode_reward: 15.769 [7.000, 26.000] - loss: 0.019 - mae: 2.312 - mean_q: 2.789 - mean_eps: 0.100 - ale.lives: 2.135\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0235\n",
      "11 episodes - episode_reward: 18.909 [9.000, 33.000] - loss: 0.019 - mae: 2.336 - mean_q: 2.818 - mean_eps: 0.100 - ale.lives: 2.090\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0225\n",
      "14 episodes - episode_reward: 17.571 [3.000, 34.000] - loss: 0.018 - mae: 2.341 - mean_q: 2.825 - mean_eps: 0.100 - ale.lives: 2.082\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0218\n",
      "11 episodes - episode_reward: 19.182 [12.000, 31.000] - loss: 0.018 - mae: 2.341 - mean_q: 2.824 - mean_eps: 0.100 - ale.lives: 2.289\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0201\n",
      "14 episodes - episode_reward: 14.571 [6.000, 25.000] - loss: 0.019 - mae: 2.338 - mean_q: 2.819 - mean_eps: 0.100 - ale.lives: 2.086\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0211\n",
      "14 episodes - episode_reward: 15.214 [8.000, 24.000] - loss: 0.018 - mae: 2.336 - mean_q: 2.818 - mean_eps: 0.100 - ale.lives: 1.992\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0217\n",
      "11 episodes - episode_reward: 19.545 [10.000, 35.000] - loss: 0.018 - mae: 2.349 - mean_q: 2.833 - mean_eps: 0.100 - ale.lives: 2.010\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0231\n",
      "11 episodes - episode_reward: 20.364 [6.000, 32.000] - loss: 0.019 - mae: 2.353 - mean_q: 2.839 - mean_eps: 0.100 - ale.lives: 2.147\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 19.154 [7.000, 30.000] - loss: 0.019 - mae: 2.317 - mean_q: 2.794 - mean_eps: 0.100 - ale.lives: 2.225\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0233\n",
      "10 episodes - episode_reward: 21.300 [11.000, 30.000] - loss: 0.018 - mae: 2.321 - mean_q: 2.801 - mean_eps: 0.100 - ale.lives: 2.149\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0226\n",
      "12 episodes - episode_reward: 19.833 [7.000, 28.000] - loss: 0.019 - mae: 2.337 - mean_q: 2.821 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0267\n",
      "11 episodes - episode_reward: 23.818 [8.000, 31.000] - loss: 0.018 - mae: 2.329 - mean_q: 2.809 - mean_eps: 0.100 - ale.lives: 2.226\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0233\n",
      "13 episodes - episode_reward: 17.769 [3.000, 25.000] - loss: 0.018 - mae: 2.339 - mean_q: 2.822 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0208\n",
      "11 episodes - episode_reward: 19.727 [12.000, 29.000] - loss: 0.018 - mae: 2.326 - mean_q: 2.806 - mean_eps: 0.100 - ale.lives: 2.098\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0221\n",
      "13 episodes - episode_reward: 17.462 [6.000, 33.000] - loss: 0.019 - mae: 2.338 - mean_q: 2.821 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0198\n",
      "13 episodes - episode_reward: 14.077 [5.000, 25.000] - loss: 0.018 - mae: 2.340 - mean_q: 2.822 - mean_eps: 0.100 - ale.lives: 2.107\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0221\n",
      "11 episodes - episode_reward: 19.545 [11.000, 36.000] - loss: 0.018 - mae: 2.345 - mean_q: 2.829 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0237\n",
      "13 episodes - episode_reward: 19.538 [8.000, 33.000] - loss: 0.018 - mae: 2.329 - mean_q: 2.809 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0224\n",
      "13 episodes - episode_reward: 17.462 [12.000, 22.000] - loss: 0.018 - mae: 2.340 - mean_q: 2.823 - mean_eps: 0.100 - ale.lives: 2.125\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0242\n",
      "10 episodes - episode_reward: 24.300 [16.000, 34.000] - loss: 0.017 - mae: 2.323 - mean_q: 2.804 - mean_eps: 0.100 - ale.lives: 2.117\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0230\n",
      "10 episodes - episode_reward: 20.300 [7.000, 35.000] - loss: 0.018 - mae: 2.337 - mean_q: 2.820 - mean_eps: 0.100 - ale.lives: 2.158\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0232\n",
      "13 episodes - episode_reward: 19.923 [11.000, 32.000] - loss: 0.018 - mae: 2.364 - mean_q: 2.852 - mean_eps: 0.100 - ale.lives: 2.016\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0218\n",
      "12 episodes - episode_reward: 16.917 [3.000, 28.000] - loss: 0.017 - mae: 2.372 - mean_q: 2.863 - mean_eps: 0.100 - ale.lives: 2.163\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0217\n",
      "13 episodes - episode_reward: 17.462 [6.000, 31.000] - loss: 0.018 - mae: 2.382 - mean_q: 2.874 - mean_eps: 0.100 - ale.lives: 2.140\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 20.364 [5.000, 34.000] - loss: 0.018 - mae: 2.351 - mean_q: 2.835 - mean_eps: 0.100 - ale.lives: 2.177\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0213\n",
      "13 episodes - episode_reward: 16.231 [6.000, 24.000] - loss: 0.018 - mae: 2.374 - mean_q: 2.865 - mean_eps: 0.100 - ale.lives: 2.093\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0216\n",
      "15 episodes - episode_reward: 14.667 [3.000, 28.000] - loss: 0.018 - mae: 2.389 - mean_q: 2.881 - mean_eps: 0.100 - ale.lives: 2.077\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0204\n",
      "14 episodes - episode_reward: 15.000 [5.000, 28.000] - loss: 0.018 - mae: 2.382 - mean_q: 2.871 - mean_eps: 0.100 - ale.lives: 2.147\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0184\n",
      "12 episodes - episode_reward: 13.833 [4.000, 31.000] - loss: 0.018 - mae: 2.380 - mean_q: 2.871 - mean_eps: 0.100 - ale.lives: 2.032\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0242\n",
      "12 episodes - episode_reward: 20.333 [15.000, 26.000] - loss: 0.017 - mae: 2.397 - mean_q: 2.893 - mean_eps: 0.100 - ale.lives: 2.122\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0235\n",
      "13 episodes - episode_reward: 18.385 [5.000, 34.000] - loss: 0.018 - mae: 2.403 - mean_q: 2.898 - mean_eps: 0.100 - ale.lives: 2.183\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0248\n",
      "11 episodes - episode_reward: 22.273 [10.000, 33.000] - loss: 0.018 - mae: 2.423 - mean_q: 2.922 - mean_eps: 0.100 - ale.lives: 2.016\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 23.000 [11.000, 29.000] - loss: 0.019 - mae: 2.460 - mean_q: 2.965 - mean_eps: 0.100 - ale.lives: 2.140\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 19.364 [9.000, 30.000] - loss: 0.018 - mae: 2.450 - mean_q: 2.954 - mean_eps: 0.100 - ale.lives: 2.160\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0225\n",
      "13 episodes - episode_reward: 19.385 [6.000, 31.000] - loss: 0.017 - mae: 2.446 - mean_q: 2.950 - mean_eps: 0.100 - ale.lives: 2.244\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0234\n",
      "10 episodes - episode_reward: 20.800 [12.000, 31.000] - loss: 0.018 - mae: 2.451 - mean_q: 2.954 - mean_eps: 0.100 - ale.lives: 1.859\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0231\n",
      "12 episodes - episode_reward: 19.750 [7.000, 34.000] - loss: 0.018 - mae: 2.454 - mean_q: 2.960 - mean_eps: 0.100 - ale.lives: 2.027\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0228\n",
      "9 episodes - episode_reward: 25.000 [20.000, 32.000] - loss: 0.018 - mae: 2.469 - mean_q: 2.977 - mean_eps: 0.100 - ale.lives: 2.052\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0239\n",
      "12 episodes - episode_reward: 20.250 [6.000, 35.000] - loss: 0.018 - mae: 2.485 - mean_q: 2.997 - mean_eps: 0.100 - ale.lives: 2.182\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0238\n",
      "12 episodes - episode_reward: 21.000 [8.000, 30.000] - loss: 0.018 - mae: 2.507 - mean_q: 3.024 - mean_eps: 0.100 - ale.lives: 2.040\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0259\n",
      "11 episodes - episode_reward: 23.091 [12.000, 43.000] - loss: 0.018 - mae: 2.533 - mean_q: 3.056 - mean_eps: 0.100 - ale.lives: 2.280\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0222\n",
      "11 episodes - episode_reward: 21.091 [10.000, 32.000] - loss: 0.018 - mae: 2.533 - mean_q: 3.055 - mean_eps: 0.100 - ale.lives: 2.137\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0238\n",
      "11 episodes - episode_reward: 20.091 [11.000, 44.000] - loss: 0.018 - mae: 2.520 - mean_q: 3.038 - mean_eps: 0.100 - ale.lives: 1.890\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0237\n",
      "12 episodes - episode_reward: 18.917 [9.000, 31.000] - loss: 0.018 - mae: 2.543 - mean_q: 3.065 - mean_eps: 0.100 - ale.lives: 2.314\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0222\n",
      "12 episodes - episode_reward: 19.250 [5.000, 42.000] - loss: 0.018 - mae: 2.547 - mean_q: 3.071 - mean_eps: 0.100 - ale.lives: 2.223\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0247\n",
      "12 episodes - episode_reward: 21.333 [6.000, 33.000] - loss: 0.018 - mae: 2.529 - mean_q: 3.048 - mean_eps: 0.100 - ale.lives: 1.997\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0221\n",
      "11 episodes - episode_reward: 20.909 [17.000, 30.000] - loss: 0.018 - mae: 2.547 - mean_q: 3.069 - mean_eps: 0.100 - ale.lives: 2.162\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 21.182 [4.000, 31.000] - loss: 0.018 - mae: 2.543 - mean_q: 3.065 - mean_eps: 0.100 - ale.lives: 2.161\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0222\n",
      "11 episodes - episode_reward: 20.273 [9.000, 30.000] - loss: 0.019 - mae: 2.547 - mean_q: 3.069 - mean_eps: 0.100 - ale.lives: 2.045\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 17.417 [4.000, 31.000] - loss: 0.019 - mae: 2.575 - mean_q: 3.102 - mean_eps: 0.100 - ale.lives: 2.099\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 16.231 [7.000, 36.000] - loss: 0.018 - mae: 2.576 - mean_q: 3.104 - mean_eps: 0.100 - ale.lives: 1.968\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0228\n",
      "13 episodes - episode_reward: 17.231 [6.000, 30.000] - loss: 0.018 - mae: 2.583 - mean_q: 3.112 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0226\n",
      "11 episodes - episode_reward: 20.455 [15.000, 31.000] - loss: 0.019 - mae: 2.566 - mean_q: 3.093 - mean_eps: 0.100 - ale.lives: 2.042\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0221\n",
      "11 episodes - episode_reward: 19.364 [8.000, 34.000] - loss: 0.019 - mae: 2.602 - mean_q: 3.135 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0228\n",
      "11 episodes - episode_reward: 21.545 [15.000, 31.000] - loss: 0.020 - mae: 2.623 - mean_q: 3.161 - mean_eps: 0.100 - ale.lives: 2.000\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0230\n",
      "11 episodes - episode_reward: 21.273 [5.000, 30.000] - loss: 0.018 - mae: 2.624 - mean_q: 3.164 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0233\n",
      "13 episodes - episode_reward: 18.154 [4.000, 29.000] - loss: 0.018 - mae: 2.634 - mean_q: 3.176 - mean_eps: 0.100 - ale.lives: 2.168\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 18.750 [5.000, 34.000] - loss: 0.019 - mae: 2.647 - mean_q: 3.190 - mean_eps: 0.100 - ale.lives: 1.979\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0229\n",
      "11 episodes - episode_reward: 19.727 [5.000, 28.000] - loss: 0.019 - mae: 2.658 - mean_q: 3.203 - mean_eps: 0.100 - ale.lives: 1.996\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0237\n",
      "14 episodes - episode_reward: 17.286 [4.000, 27.000] - loss: 0.019 - mae: 2.652 - mean_q: 3.197 - mean_eps: 0.100 - ale.lives: 2.084\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0220\n",
      "9 episodes - episode_reward: 24.333 [13.000, 33.000] - loss: 0.020 - mae: 2.654 - mean_q: 3.199 - mean_eps: 0.100 - ale.lives: 2.043\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0226\n",
      "10 episodes - episode_reward: 22.600 [14.000, 34.000] - loss: 0.019 - mae: 2.660 - mean_q: 3.207 - mean_eps: 0.100 - ale.lives: 2.138\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0219\n",
      "10 episodes - episode_reward: 21.100 [12.000, 33.000] - loss: 0.019 - mae: 2.651 - mean_q: 3.196 - mean_eps: 0.100 - ale.lives: 1.905\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0244\n",
      "12 episodes - episode_reward: 19.250 [6.000, 35.000] - loss: 0.019 - mae: 2.651 - mean_q: 3.196 - mean_eps: 0.100 - ale.lives: 1.983\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0231\n",
      "11 episodes - episode_reward: 23.364 [15.000, 31.000] - loss: 0.018 - mae: 2.611 - mean_q: 3.150 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0231\n",
      "11 episodes - episode_reward: 20.636 [16.000, 29.000] - loss: 0.018 - mae: 2.635 - mean_q: 3.176 - mean_eps: 0.100 - ale.lives: 2.134\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0206\n",
      "13 episodes - episode_reward: 15.308 [7.000, 31.000] - loss: 0.018 - mae: 2.644 - mean_q: 3.187 - mean_eps: 0.100 - ale.lives: 2.287\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0221\n",
      "14 episodes - episode_reward: 16.571 [5.000, 30.000] - loss: 0.017 - mae: 2.631 - mean_q: 3.172 - mean_eps: 0.100 - ale.lives: 2.168\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0242\n",
      "11 episodes - episode_reward: 20.091 [14.000, 32.000] - loss: 0.018 - mae: 2.608 - mean_q: 3.143 - mean_eps: 0.100 - ale.lives: 2.183\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0213\n",
      "13 episodes - episode_reward: 17.846 [10.000, 31.000] - loss: 0.018 - mae: 2.624 - mean_q: 3.163 - mean_eps: 0.100 - ale.lives: 2.131\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 17.667 [12.000, 30.000] - loss: 0.019 - mae: 2.636 - mean_q: 3.177 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0250\n",
      "11 episodes - episode_reward: 22.182 [6.000, 32.000] - loss: 0.019 - mae: 2.654 - mean_q: 3.199 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 17.500 [5.000, 35.000] - loss: 0.018 - mae: 2.675 - mean_q: 3.223 - mean_eps: 0.100 - ale.lives: 2.075\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0231\n",
      "11 episodes - episode_reward: 21.182 [6.000, 32.000] - loss: 0.018 - mae: 2.674 - mean_q: 3.222 - mean_eps: 0.100 - ale.lives: 2.093\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 19.583 [9.000, 32.000] - loss: 0.018 - mae: 2.680 - mean_q: 3.229 - mean_eps: 0.100 - ale.lives: 2.151\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0218\n",
      "13 episodes - episode_reward: 17.000 [8.000, 33.000] - loss: 0.018 - mae: 2.680 - mean_q: 3.230 - mean_eps: 0.100 - ale.lives: 2.199\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0234\n",
      "12 episodes - episode_reward: 18.833 [5.000, 30.000] - loss: 0.019 - mae: 2.673 - mean_q: 3.221 - mean_eps: 0.100 - ale.lives: 2.021\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0189\n",
      "14 episodes - episode_reward: 13.714 [3.000, 27.000] - loss: 0.019 - mae: 2.671 - mean_q: 3.219 - mean_eps: 0.100 - ale.lives: 2.240\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 16.077 [3.000, 29.000] - loss: 0.019 - mae: 2.679 - mean_q: 3.229 - mean_eps: 0.100 - ale.lives: 1.934\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0241\n",
      "11 episodes - episode_reward: 21.545 [12.000, 30.000] - loss: 0.018 - mae: 2.682 - mean_q: 3.232 - mean_eps: 0.100 - ale.lives: 2.101\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0211\n",
      "9 episodes - episode_reward: 23.889 [13.000, 33.000] - loss: 0.019 - mae: 2.671 - mean_q: 3.219 - mean_eps: 0.100 - ale.lives: 2.053\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 17.167 [8.000, 27.000] - loss: 0.018 - mae: 2.667 - mean_q: 3.213 - mean_eps: 0.100 - ale.lives: 2.151\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0221\n",
      "13 episodes - episode_reward: 18.077 [10.000, 32.000] - loss: 0.018 - mae: 2.672 - mean_q: 3.219 - mean_eps: 0.100 - ale.lives: 2.191\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0221\n",
      "10 episodes - episode_reward: 20.100 [10.000, 30.000] - loss: 0.018 - mae: 2.670 - mean_q: 3.217 - mean_eps: 0.100 - ale.lives: 2.028\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 18.583 [10.000, 29.000] - loss: 0.018 - mae: 2.693 - mean_q: 3.244 - mean_eps: 0.100 - ale.lives: 2.132\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0217\n",
      "12 episodes - episode_reward: 17.833 [7.000, 26.000] - loss: 0.017 - mae: 2.707 - mean_q: 3.262 - mean_eps: 0.100 - ale.lives: 2.324\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 18.333 [3.000, 30.000] - loss: 0.018 - mae: 2.720 - mean_q: 3.275 - mean_eps: 0.100 - ale.lives: 2.170\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0218\n",
      "10 episodes - episode_reward: 20.900 [8.000, 31.000] - loss: 0.017 - mae: 2.722 - mean_q: 3.279 - mean_eps: 0.100 - ale.lives: 2.040\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 18.231 [8.000, 32.000] - loss: 0.017 - mae: 2.711 - mean_q: 3.266 - mean_eps: 0.100 - ale.lives: 2.214\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0226\n",
      "14 episodes - episode_reward: 15.786 [8.000, 34.000] - loss: 0.018 - mae: 2.735 - mean_q: 3.296 - mean_eps: 0.100 - ale.lives: 2.013\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0264\n",
      "12 episodes - episode_reward: 21.583 [13.000, 29.000] - loss: 0.017 - mae: 2.737 - mean_q: 3.298 - mean_eps: 0.100 - ale.lives: 2.218\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 18.667 [8.000, 34.000] - loss: 0.018 - mae: 2.724 - mean_q: 3.283 - mean_eps: 0.100 - ale.lives: 2.037\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 17.231 [8.000, 29.000] - loss: 0.017 - mae: 2.736 - mean_q: 3.297 - mean_eps: 0.100 - ale.lives: 2.017\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0234\n",
      "10 episodes - episode_reward: 22.100 [12.000, 35.000] - loss: 0.017 - mae: 2.750 - mean_q: 3.314 - mean_eps: 0.100 - ale.lives: 2.014\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0225\n",
      "12 episodes - episode_reward: 20.583 [12.000, 35.000] - loss: 0.017 - mae: 2.772 - mean_q: 3.340 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0236\n",
      "11 episodes - episode_reward: 21.455 [15.000, 32.000] - loss: 0.017 - mae: 2.758 - mean_q: 3.323 - mean_eps: 0.100 - ale.lives: 1.931\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 18.667 [8.000, 33.000] - loss: 0.018 - mae: 2.753 - mean_q: 3.317 - mean_eps: 0.100 - ale.lives: 2.256\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0249\n",
      "11 episodes - episode_reward: 20.727 [11.000, 30.000] - loss: 0.017 - mae: 2.744 - mean_q: 3.306 - mean_eps: 0.100 - ale.lives: 1.990\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0235\n",
      "12 episodes - episode_reward: 20.000 [7.000, 29.000] - loss: 0.017 - mae: 2.742 - mean_q: 3.303 - mean_eps: 0.100 - ale.lives: 2.286\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0232\n",
      "13 episodes - episode_reward: 19.077 [1.000, 32.000] - loss: 0.017 - mae: 2.728 - mean_q: 3.288 - mean_eps: 0.100 - ale.lives: 2.049\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0259\n",
      "13 episodes - episode_reward: 18.769 [10.000, 33.000] - loss: 0.016 - mae: 2.696 - mean_q: 3.249 - mean_eps: 0.100 - ale.lives: 2.215\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 19.000 [5.000, 42.000] - loss: 0.017 - mae: 2.700 - mean_q: 3.253 - mean_eps: 0.100 - ale.lives: 2.161\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 18.727 [7.000, 34.000] - loss: 0.016 - mae: 2.729 - mean_q: 3.290 - mean_eps: 0.100 - ale.lives: 1.990\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0250\n",
      "11 episodes - episode_reward: 24.182 [9.000, 35.000] - loss: 0.017 - mae: 2.735 - mean_q: 3.297 - mean_eps: 0.100 - ale.lives: 2.138\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0252\n",
      "13 episodes - episode_reward: 19.000 [7.000, 27.000] - loss: 0.017 - mae: 2.726 - mean_q: 3.286 - mean_eps: 0.100 - ale.lives: 2.229\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0217\n",
      "12 episodes - episode_reward: 17.750 [6.000, 26.000] - loss: 0.017 - mae: 2.723 - mean_q: 3.281 - mean_eps: 0.100 - ale.lives: 1.950\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 19.000 [13.000, 30.000] - loss: 0.017 - mae: 2.727 - mean_q: 3.285 - mean_eps: 0.100 - ale.lives: 2.260\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0247\n",
      "11 episodes - episode_reward: 21.636 [12.000, 33.000] - loss: 0.017 - mae: 2.742 - mean_q: 3.303 - mean_eps: 0.100 - ale.lives: 2.104\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0237\n",
      "12 episodes - episode_reward: 19.167 [7.000, 33.000] - loss: 0.017 - mae: 2.752 - mean_q: 3.316 - mean_eps: 0.100 - ale.lives: 2.048\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0232\n",
      "11 episodes - episode_reward: 20.545 [9.000, 33.000] - loss: 0.017 - mae: 2.768 - mean_q: 3.336 - mean_eps: 0.100 - ale.lives: 2.213\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0245\n",
      "14 episodes - episode_reward: 18.786 [12.000, 30.000] - loss: 0.017 - mae: 2.767 - mean_q: 3.333 - mean_eps: 0.100 - ale.lives: 1.927\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0211\n",
      "12 episodes - episode_reward: 17.583 [3.000, 27.000] - loss: 0.017 - mae: 2.777 - mean_q: 3.345 - mean_eps: 0.100 - ale.lives: 2.153\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 21.182 [10.000, 35.000] - loss: 0.018 - mae: 2.770 - mean_q: 3.337 - mean_eps: 0.100 - ale.lives: 1.899\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0233\n",
      "12 episodes - episode_reward: 19.583 [11.000, 34.000] - loss: 0.018 - mae: 2.790 - mean_q: 3.362 - mean_eps: 0.100 - ale.lives: 2.119\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0224\n",
      "10 episodes - episode_reward: 22.300 [7.000, 32.000] - loss: 0.017 - mae: 2.795 - mean_q: 3.367 - mean_eps: 0.100 - ale.lives: 2.125\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0245\n",
      "10 episodes - episode_reward: 25.500 [12.000, 36.000] - loss: 0.018 - mae: 2.799 - mean_q: 3.373 - mean_eps: 0.100 - ale.lives: 2.325\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0216\n",
      "12 episodes - episode_reward: 17.500 [8.000, 28.000] - loss: 0.018 - mae: 2.814 - mean_q: 3.390 - mean_eps: 0.100 - ale.lives: 1.994\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0227\n",
      "14 episodes - episode_reward: 16.571 [6.000, 32.000] - loss: 0.018 - mae: 2.819 - mean_q: 3.396 - mean_eps: 0.100 - ale.lives: 2.158\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 16.462 [7.000, 26.000] - loss: 0.018 - mae: 2.821 - mean_q: 3.399 - mean_eps: 0.100 - ale.lives: 2.156\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0230\n",
      "13 episodes - episode_reward: 16.923 [6.000, 29.000] - loss: 0.018 - mae: 2.804 - mean_q: 3.379 - mean_eps: 0.100 - ale.lives: 2.119\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0217\n",
      "12 episodes - episode_reward: 17.750 [9.000, 32.000] - loss: 0.018 - mae: 2.778 - mean_q: 3.347 - mean_eps: 0.100 - ale.lives: 2.069\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0233\n",
      "15 episodes - episode_reward: 16.133 [8.000, 28.000] - loss: 0.017 - mae: 2.771 - mean_q: 3.339 - mean_eps: 0.100 - ale.lives: 2.257\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0220\n",
      "12 episodes - episode_reward: 18.667 [8.000, 33.000] - loss: 0.018 - mae: 2.757 - mean_q: 3.322 - mean_eps: 0.100 - ale.lives: 2.200\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0209\n",
      "11 episodes - episode_reward: 18.727 [10.000, 32.000] - loss: 0.019 - mae: 2.777 - mean_q: 3.346 - mean_eps: 0.100 - ale.lives: 2.064\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0213\n",
      "12 episodes - episode_reward: 18.000 [5.000, 36.000] - loss: 0.018 - mae: 2.790 - mean_q: 3.363 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0244\n",
      "13 episodes - episode_reward: 17.692 [8.000, 28.000] - loss: 0.018 - mae: 2.784 - mean_q: 3.355 - mean_eps: 0.100 - ale.lives: 2.022\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0225\n",
      "15 episodes - episode_reward: 15.800 [12.000, 23.000] - loss: 0.018 - mae: 2.789 - mean_q: 3.362 - mean_eps: 0.100 - ale.lives: 1.870\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0245\n",
      "14 episodes - episode_reward: 17.143 [8.000, 35.000] - loss: 0.019 - mae: 2.801 - mean_q: 3.377 - mean_eps: 0.100 - ale.lives: 2.114\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0240\n",
      "14 episodes - episode_reward: 17.714 [3.000, 32.000] - loss: 0.018 - mae: 2.795 - mean_q: 3.370 - mean_eps: 0.100 - ale.lives: 2.139\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0250\n",
      "12 episodes - episode_reward: 19.500 [9.000, 34.000] - loss: 0.018 - mae: 2.781 - mean_q: 3.352 - mean_eps: 0.100 - ale.lives: 2.091\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 18.154 [7.000, 29.000] - loss: 0.018 - mae: 2.767 - mean_q: 3.334 - mean_eps: 0.100 - ale.lives: 2.129\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 20.818 [7.000, 34.000] - loss: 0.019 - mae: 2.778 - mean_q: 3.349 - mean_eps: 0.100 - ale.lives: 2.005\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0227\n",
      "11 episodes - episode_reward: 18.455 [6.000, 32.000] - loss: 0.019 - mae: 2.772 - mean_q: 3.341 - mean_eps: 0.100 - ale.lives: 1.957\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0244\n",
      "12 episodes - episode_reward: 21.667 [13.000, 29.000] - loss: 0.018 - mae: 2.774 - mean_q: 3.343 - mean_eps: 0.100 - ale.lives: 2.166\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0225\n",
      "15 episodes - episode_reward: 15.800 [6.000, 28.000] - loss: 0.019 - mae: 2.770 - mean_q: 3.339 - mean_eps: 0.100 - ale.lives: 2.066\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0250\n",
      "12 episodes - episode_reward: 20.000 [5.000, 29.000] - loss: 0.018 - mae: 2.785 - mean_q: 3.356 - mean_eps: 0.100 - ale.lives: 2.032\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0240\n",
      "10 episodes - episode_reward: 24.300 [8.000, 34.000] - loss: 0.019 - mae: 2.793 - mean_q: 3.365 - mean_eps: 0.100 - ale.lives: 2.145\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0224\n",
      "13 episodes - episode_reward: 17.769 [6.000, 33.000] - loss: 0.018 - mae: 2.778 - mean_q: 3.347 - mean_eps: 0.100 - ale.lives: 2.116\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0236\n",
      "12 episodes - episode_reward: 19.417 [10.000, 31.000] - loss: 0.018 - mae: 2.749 - mean_q: 3.311 - mean_eps: 0.100 - ale.lives: 2.076\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0216\n",
      "14 episodes - episode_reward: 15.357 [2.000, 28.000] - loss: 0.017 - mae: 2.754 - mean_q: 3.317 - mean_eps: 0.100 - ale.lives: 2.031\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0226\n",
      "13 episodes - episode_reward: 17.308 [8.000, 30.000] - loss: 0.018 - mae: 2.772 - mean_q: 3.340 - mean_eps: 0.100 - ale.lives: 1.892\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0234\n",
      "14 episodes - episode_reward: 16.643 [9.000, 29.000] - loss: 0.018 - mae: 2.756 - mean_q: 3.320 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0215\n",
      "12 episodes - episode_reward: 18.167 [8.000, 28.000] - loss: 0.018 - mae: 2.750 - mean_q: 3.314 - mean_eps: 0.100 - ale.lives: 2.004\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 21.455 [7.000, 32.000] - loss: 0.018 - mae: 2.726 - mean_q: 3.285 - mean_eps: 0.100 - ale.lives: 1.905\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0224\n",
      "14 episodes - episode_reward: 15.000 [6.000, 21.000] - loss: 0.018 - mae: 2.738 - mean_q: 3.300 - mean_eps: 0.100 - ale.lives: 2.052\n",
      "\n",
      "Interval 285 (2840000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0235\n",
      "14 episodes - episode_reward: 17.929 [7.000, 28.000] - loss: 0.018 - mae: 2.739 - mean_q: 3.301 - mean_eps: 0.100 - ale.lives: 2.028\n",
      "\n",
      "Interval 286 (2850000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0227\n",
      "12 episodes - episode_reward: 18.000 [12.000, 25.000] - loss: 0.018 - mae: 2.731 - mean_q: 3.291 - mean_eps: 0.100 - ale.lives: 2.143\n",
      "\n",
      "Interval 287 (2860000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0245\n",
      "13 episodes - episode_reward: 19.692 [8.000, 30.000] - loss: 0.018 - mae: 2.760 - mean_q: 3.325 - mean_eps: 0.100 - ale.lives: 2.075\n",
      "\n",
      "Interval 288 (2870000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0234\n",
      "14 episodes - episode_reward: 15.929 [10.000, 26.000] - loss: 0.018 - mae: 2.768 - mean_q: 3.336 - mean_eps: 0.100 - ale.lives: 2.005\n",
      "\n",
      "Interval 289 (2880000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 21.909 [15.000, 30.000] - loss: 0.019 - mae: 2.764 - mean_q: 3.330 - mean_eps: 0.100 - ale.lives: 2.161\n",
      "\n",
      "Interval 290 (2890000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0230\n",
      "14 episodes - episode_reward: 16.786 [3.000, 33.000] - loss: 0.018 - mae: 2.760 - mean_q: 3.326 - mean_eps: 0.100 - ale.lives: 2.027\n",
      "\n",
      "Interval 291 (2900000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 20.455 [9.000, 32.000] - loss: 0.018 - mae: 2.751 - mean_q: 3.316 - mean_eps: 0.100 - ale.lives: 2.105\n",
      "\n",
      "Interval 292 (2910000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0249\n",
      "11 episodes - episode_reward: 21.182 [7.000, 33.000] - loss: 0.018 - mae: 2.722 - mean_q: 3.280 - mean_eps: 0.100 - ale.lives: 2.184\n",
      "\n",
      "Interval 293 (2920000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0233\n",
      "14 episodes - episode_reward: 17.500 [8.000, 26.000] - loss: 0.018 - mae: 2.740 - mean_q: 3.301 - mean_eps: 0.100 - ale.lives: 2.210\n",
      "\n",
      "Interval 294 (2930000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0223\n",
      "12 episodes - episode_reward: 18.417 [10.000, 30.000] - loss: 0.018 - mae: 2.732 - mean_q: 3.291 - mean_eps: 0.100 - ale.lives: 2.025\n",
      "\n",
      "Interval 295 (2940000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0233\n",
      "13 episodes - episode_reward: 18.692 [8.000, 28.000] - loss: 0.017 - mae: 2.727 - mean_q: 3.285 - mean_eps: 0.100 - ale.lives: 2.053\n",
      "\n",
      "Interval 296 (2950000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0213\n",
      "11 episodes - episode_reward: 16.818 [6.000, 27.000] - loss: 0.018 - mae: 2.718 - mean_q: 3.274 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 297 (2960000 steps performed)\n",
      "10000/10000 [==============================] - 315s 32ms/step - reward: 0.0222\n",
      "14 episodes - episode_reward: 17.500 [6.000, 31.000] - loss: 0.018 - mae: 2.710 - mean_q: 3.264 - mean_eps: 0.100 - ale.lives: 2.111\n",
      "\n",
      "Interval 298 (2970000 steps performed)\n",
      "10000/10000 [==============================] - 316s 32ms/step - reward: 0.0217\n",
      "11 episodes - episode_reward: 19.364 [11.000, 34.000] - loss: 0.018 - mae: 2.693 - mean_q: 3.244 - mean_eps: 0.100 - ale.lives: 2.071\n",
      "\n",
      "Interval 299 (2980000 steps performed)\n",
      "10000/10000 [==============================] - 316s 32ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 17.231 [5.000, 34.000] - loss: 0.017 - mae: 2.708 - mean_q: 3.263 - mean_eps: 0.100 - ale.lives: 2.097\n",
      "\n",
      "Interval 300 (2990000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0224\n",
      "done, took 84873.622 seconds\n",
      "\n",
      "=== ENTRENAMIENTO COMPLETADO ===\n",
      "Pesos guardados en: dqn_SpaceInvaders-v0_weights.h5f\n"
     ]
    }
   ],
   "source": [
    "# Configuración de callbacks para guardar progreso\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "\n",
    "callbacks = [\n",
    "    ModelIntervalCheckpoint(checkpoint_weights_filename, interval=500000),\n",
    "    FileLogger(log_filename, interval=100)\n",
    "]\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"\\n=== INICIANDO ENTRENAMIENTO ===\")\n",
    "print(f\"Pasos totales: 3,000,000\")\n",
    "print(f\"Tiempo estimado: 12-24 horas en GPU\")\n",
    "print(f\"Checkpoints se guardarán cada 500,000 pasos\")\n",
    "print(\"\\nPara continuar desde un checkpoint:\")\n",
    "print(\"  dqn.load_weights('dqn_SpaceInvaders-v0_weights_XXXXX.h5f')\")\n",
    "print(\"\\nIniciando...\\n\")\n",
    "\n",
    "history = dqn.fit(\n",
    "    env,\n",
    "    callbacks=callbacks,\n",
    "    nb_steps=3000000,\n",
    "    log_interval=10000,\n",
    "    visualize=False\n",
    ")\n",
    "\n",
    "# Guardar pesos finales\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "print(f\"\\n=== ENTRENAMIENTO COMPLETADO ===\")\n",
    "print(f\"Pesos guardados en: {weights_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoadWeightsSection"
   },
   "source": [
    "---\n",
    "### **Cargar pesos pre-entrenados**\n",
    "\n",
    "Al tener el modelo entrenado, permite cargar los pesos aquí en lugar de entrenar desde cero.\n",
    "Esto es útil para:\n",
    "- Continuar el entrenamiento desde un checkpoint\n",
    "- Evaluar un modelo ya entrenado\n",
    "- Comparar diferentes versiones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoadWeightsCell"
   },
   "outputs": [],
   "source": [
    "# Descomentar para cargar pesos pre-entrenados\n",
    "weights_filename = 'dqn_SpaceInvaders-v0_weights.h5f'\n",
    "dqn.load_weights(weights_filename)\n",
    "print(f\"Pesos cargados desde: {weights_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TestingSection"
   },
   "source": [
    "---\n",
    "### **Evaluación del modelo (Testing)**\n",
    "\n",
    "Esta celda evalúa el rendimiento del agente entrenado en 100 episodios de test.\n",
    "\n",
    "**Objetivo:** Alcanzar **más de 20 puntos durante más de 100 episodios consecutivos**\n",
    "\n",
    "**Criterio de éxito:**\n",
    "- Ejecutar 100 episodios de evaluación\n",
    "- Contar la racha máxima de episodios consecutivos con recompensa >20\n",
    "- Si racha_máxima >= 100 → Objetivo alcanzado ✅\n",
    "\n",
    "**Interpretación de resultados:**\n",
    "- < 50 consecutivos: El agente necesita más entrenamiento\n",
    "- 50-99 consecutivos: El agente está cerca, continuar entrenamiento\n",
    "- >= 100 consecutivos: ¡Objetivo conseguido!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OHYryKd1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUACIÓN DEL MODELO ===\n",
      "Ejecutando 100 episodios de test...\n",
      "\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 11.000, steps: 631\n",
      "Episode 2: reward: 19.000, steps: 725\n",
      "Episode 3: reward: 21.000, steps: 820\n",
      "Episode 4: reward: 21.000, steps: 697\n",
      "Episode 5: reward: 14.000, steps: 620\n",
      "Episode 6: reward: 28.000, steps: 1093\n",
      "Episode 7: reward: 19.000, steps: 814\n",
      "Episode 8: reward: 21.000, steps: 672\n",
      "Episode 9: reward: 25.000, steps: 1086\n",
      "Episode 10: reward: 12.000, steps: 492\n",
      "Episode 11: reward: 20.000, steps: 743\n",
      "Episode 12: reward: 14.000, steps: 622\n",
      "Episode 13: reward: 27.000, steps: 1144\n",
      "Episode 14: reward: 21.000, steps: 768\n",
      "Episode 15: reward: 20.000, steps: 929\n",
      "Episode 16: reward: 12.000, steps: 532\n",
      "Episode 17: reward: 26.000, steps: 1121\n",
      "Episode 18: reward: 20.000, steps: 742\n",
      "Episode 19: reward: 27.000, steps: 1140\n",
      "Episode 20: reward: 18.000, steps: 921\n",
      "Episode 21: reward: 16.000, steps: 619\n",
      "Episode 22: reward: 15.000, steps: 532\n",
      "Episode 23: reward: 27.000, steps: 1210\n",
      "Episode 24: reward: 26.000, steps: 1090\n",
      "Episode 25: reward: 16.000, steps: 679\n",
      "Episode 26: reward: 14.000, steps: 684\n",
      "Episode 27: reward: 9.000, steps: 507\n",
      "Episode 28: reward: 19.000, steps: 589\n",
      "Episode 29: reward: 30.000, steps: 1126\n",
      "Episode 30: reward: 17.000, steps: 804\n",
      "Episode 31: reward: 19.000, steps: 954\n",
      "Episode 32: reward: 22.000, steps: 1040\n",
      "Episode 33: reward: 22.000, steps: 804\n",
      "Episode 34: reward: 17.000, steps: 563\n",
      "Episode 35: reward: 14.000, steps: 520\n",
      "Episode 36: reward: 19.000, steps: 870\n",
      "Episode 37: reward: 11.000, steps: 485\n",
      "Episode 38: reward: 15.000, steps: 703\n",
      "Episode 39: reward: 20.000, steps: 1011\n",
      "Episode 40: reward: 10.000, steps: 449\n",
      "Episode 41: reward: 19.000, steps: 591\n",
      "Episode 42: reward: 27.000, steps: 1139\n",
      "Episode 43: reward: 21.000, steps: 887\n",
      "Episode 44: reward: 14.000, steps: 559\n",
      "Episode 45: reward: 29.000, steps: 1129\n",
      "Episode 46: reward: 11.000, steps: 667\n",
      "Episode 47: reward: 16.000, steps: 621\n",
      "Episode 48: reward: 19.000, steps: 848\n",
      "Episode 49: reward: 9.000, steps: 454\n",
      "Episode 50: reward: 11.000, steps: 515\n",
      "Episode 51: reward: 22.000, steps: 811\n",
      "Episode 52: reward: 15.000, steps: 780\n",
      "Episode 53: reward: 26.000, steps: 1085\n",
      "Episode 54: reward: 16.000, steps: 566\n",
      "Episode 55: reward: 15.000, steps: 697\n",
      "Episode 56: reward: 19.000, steps: 694\n",
      "Episode 57: reward: 22.000, steps: 798\n",
      "Episode 58: reward: 25.000, steps: 871\n",
      "Episode 59: reward: 24.000, steps: 890\n",
      "Episode 60: reward: 22.000, steps: 778\n",
      "Episode 61: reward: 18.000, steps: 870\n",
      "Episode 62: reward: 28.000, steps: 1050\n",
      "Episode 63: reward: 27.000, steps: 977\n",
      "Episode 64: reward: 28.000, steps: 1120\n",
      "Episode 65: reward: 21.000, steps: 799\n",
      "Episode 66: reward: 24.000, steps: 1021\n",
      "Episode 67: reward: 28.000, steps: 1046\n",
      "Episode 68: reward: 19.000, steps: 781\n",
      "Episode 69: reward: 22.000, steps: 846\n",
      "Episode 70: reward: 29.000, steps: 1121\n",
      "Episode 71: reward: 32.000, steps: 1367\n",
      "Episode 72: reward: 11.000, steps: 457\n",
      "Episode 73: reward: 22.000, steps: 964\n",
      "Episode 74: reward: 29.000, steps: 1075\n",
      "Episode 75: reward: 20.000, steps: 647\n",
      "Episode 76: reward: 30.000, steps: 1082\n",
      "Episode 77: reward: 11.000, steps: 487\n",
      "Episode 78: reward: 18.000, steps: 744\n",
      "Episode 79: reward: 22.000, steps: 776\n",
      "Episode 80: reward: 23.000, steps: 1047\n",
      "Episode 81: reward: 27.000, steps: 947\n",
      "Episode 82: reward: 14.000, steps: 568\n",
      "Episode 83: reward: 14.000, steps: 610\n",
      "Episode 84: reward: 15.000, steps: 532\n",
      "Episode 85: reward: 22.000, steps: 735\n",
      "Episode 86: reward: 9.000, steps: 536\n",
      "Episode 87: reward: 11.000, steps: 493\n",
      "Episode 88: reward: 16.000, steps: 696\n",
      "Episode 89: reward: 22.000, steps: 824\n",
      "Episode 90: reward: 19.000, steps: 792\n",
      "Episode 91: reward: 8.000, steps: 483\n",
      "Episode 92: reward: 16.000, steps: 521\n",
      "Episode 93: reward: 13.000, steps: 628\n",
      "Episode 94: reward: 27.000, steps: 1051\n",
      "Episode 95: reward: 18.000, steps: 706\n",
      "Episode 96: reward: 28.000, steps: 1210\n",
      "Episode 97: reward: 13.000, steps: 522\n",
      "Episode 98: reward: 23.000, steps: 789\n",
      "Episode 99: reward: 15.000, steps: 692\n",
      "Episode 100: reward: 10.000, steps: 513\n",
      "\n",
      "=== RESULTADOS ===\n",
      "Recompensa media: 19.38\n",
      "Recompensa mínima: 8.00\n",
      "Recompensa máxima: 32.00\n",
      "Desviación estándar: 5.92\n",
      "\n",
      "Episodios con >20.0 puntos: 43/100\n",
      "Máximo de episodios consecutivos >20.0: 6\n",
      "\n",
      "==================================================\n",
      "❌ OBJETIVO NO ALCANZADO\n",
      "Máximo consecutivo: 6/100 episodios\n",
      "\n",
      "Sugerencias:\n",
      "  - Entrenar por más pasos (recomendado: 4-5M)\n",
      "  - Continuar entrenamiento desde checkpoint actual\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EVALUACIÓN DEL MODELO ===\")\n",
    "print(\"Ejecutando 100 episodios de test...\\n\")\n",
    "\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "test_results = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "\n",
    "rewards = test_results.history['episode_reward']\n",
    "mean_reward = np.mean(rewards)\n",
    "min_reward = np.min(rewards)\n",
    "max_reward = np.max(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "threshold = 20.0\n",
    "consecutive = 0\n",
    "max_consecutive = 0\n",
    "\n",
    "for reward in rewards:\n",
    "    if reward > threshold:\n",
    "        consecutive += 1\n",
    "        max_consecutive = max(max_consecutive, consecutive)\n",
    "    else:\n",
    "        consecutive = 0\n",
    "\n",
    "episodios_exitosos = sum(r > threshold for r in rewards)\n",
    "\n",
    "print(\"\\n=== RESULTADOS ===\")\n",
    "print(f\"Recompensa media: {mean_reward:.2f}\")\n",
    "print(f\"Recompensa mínima: {min_reward:.2f}\")\n",
    "print(f\"Recompensa máxima: {max_reward:.2f}\")\n",
    "print(f\"Desviación estándar: {std_reward:.2f}\")\n",
    "print(f\"\\nEpisodios con >{threshold} puntos: {episodios_exitosos}/100\")\n",
    "print(f\"Máximo de episodios consecutivos >{threshold}: {max_consecutive}\")\n",
    "\n",
    "objetivo_alcanzado = max_consecutive >= 100\n",
    "print(f\"\\n{'='*50}\")\n",
    "if objetivo_alcanzado:\n",
    "    print(f\"✅ OBJETIVO ALCANZADO\")\n",
    "    print(f\"El agente logró >20 puntos durante {max_consecutive} episodios consecutivos\")\n",
    "else:\n",
    "    print(f\"❌ OBJETIVO NO ALCANZADO\")\n",
    "    print(f\"Máximo consecutivo: {max_consecutive}/100 episodios\")\n",
    "    print(f\"\\nSugerencias:\")\n",
    "    print(f\"  - Entrenar por más pasos (recomendado: 4-5M)\")\n",
    "    print(f\"  - Continuar entrenamiento desde checkpoint actual\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VisualizationSection"
   },
   "source": [
    "---\n",
    "### **Visualización del agente (solo local)**\n",
    "\n",
    "Esta celda permite visualizar el agente jugando.\n",
    "**Nota:** Solo funciona en entorno local (puesto en False dado que al ejecutar el Kernel se reinicializa), no en Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VisualizationCell"
   },
   "outputs": [],
   "source": [
    "# Visualización (solo funciona en local)\n",
    "if not IN_COLAB:\n",
    "    print(\"Visualizando 3 episodios...\")\n",
    "    dqn.test(env, nb_episodes=3, visualize=False)\n",
    "else:\n",
    "    print(\"La visualización no está disponible en Google Colab.\")\n",
    "    print(\"Para ver al agente jugar, ejecuta este notebook en local.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "---\n",
    "### **3. Justificación de los parámetros seleccionados y de los resultados obtenidos**\n",
    "\n",
    "#### **3.1. Arquitectura de la Red Neuronal**\n",
    "\n",
    "**Decisión:** Se utilizó la arquitectura CNN descrita en el paper seminal de DQN (Mnih et al., 2015).\n",
    "\n",
    "**Justificación:**\n",
    "- **Capas Convolucionales:** Son ideales para procesar imágenes, ya que preservan la estructura espacial y detectan patrones locales como bordes, objetos y movimientos.\n",
    "- **Estructura progresiva:** Los filtros van de 32→64→64, permitiendo detectar características desde simples (bordes) hasta complejas (naves, disparos).\n",
    "- **Strides decrecientes:** (4→2→1) capturan información a diferentes escalas, desde una vista general hasta detalles finos.\n",
    "- **Capa densa de 512:** Suficientemente grande para combinar todas las características extraídas sin sobreajustar.\n",
    "\n",
    "**Alternativas consideradas:**\n",
    "- Redes más profundas (ResNet): Descartadas por mayor coste computacional sin mejora significativa en Atari.\n",
    "- Menos filtros: Podría reducir capacidad de aprendizaje.\n",
    "- Más filtros: Aumentaría tiempo de entrenamiento sin beneficio claro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MetricsSection"
   },
   "source": [
    "---\n",
    "## **ANEXO: Análisis de Métricas del Entrenamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar este código para visualizar el progreso del test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Guardar\u001b[39;00m\n\u001b[0;32m     99\u001b[0m fig\u001b[38;5;241m.\u001b[39mwrite_html(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_results.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 100\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_results.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Visualización guardada:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym_gpu\\lib\\site-packages\\plotly\\basedatatypes.py:3835\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3775\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3776\u001b[0m \u001b[38;5;124;03mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[0;32m   3777\u001b[0m \u001b[38;5;124;03mobject\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[0;32m   3832\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3833\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m-> 3835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym_gpu\\lib\\site-packages\\plotly\\io\\_kaleido.py:266\u001b[0m, in \u001b[0;36mwrite_image\u001b[1;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03mCannot infer image type from output path '{file}'.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 )\n\u001b[0;32m    261\u001b[0m             )\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# Request image\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;66;03m# -------------\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# Do this first so we don't create a file if image conversion fails\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     img_data \u001b[38;5;241m=\u001b[39m \u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# Open file\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# ---------\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;66;03m# We previously failed to make sense of `file` as a pathlib object.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;66;03m# Attempt to write to `file` as an open file descriptor.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym_gpu\\lib\\site-packages\\plotly\\io\\_kaleido.py:132\u001b[0m, in \u001b[0;36mto_image\u001b[1;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03mwhich can be installed using pip:\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    $ pip install -U kaleido\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m         )\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Validate figure\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# ---------------\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[1;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Extraer datos disponibles\n",
    "episodes = list(range(1, len(test_results.history['episode_reward']) + 1))\n",
    "rewards = test_results.history['episode_reward']\n",
    "steps = test_results.history['nb_steps']\n",
    "\n",
    "# Calcular estadísticas\n",
    "episodios_exitosos = sum(r > 20 for r in rewards)\n",
    "tasa_exito = (episodios_exitosos / len(rewards)) * 100\n",
    "\n",
    "# Crear subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Recompensas por Episodio (Test)', \n",
    "                    'Steps por Episodio',\n",
    "                    'Distribución de Recompensas', \n",
    "                    'Estadísticas del Test'),\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"histogram\"}, {\"type\": \"table\"}]]  # ← CAMBIO: tabla en lugar de scatter\n",
    ")\n",
    "\n",
    "# 1. Recompensas por episodio\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=episodes, y=rewards, mode='lines+markers', \n",
    "               name='Recompensa', line=dict(color='blue', width=2),\n",
    "               marker=dict(size=10)),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=20, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=\"Objetivo (20)\", row=1, col=1)\n",
    "fig.add_hline(y=np.mean(rewards), line_dash=\"dot\", line_color=\"green\",\n",
    "              annotation_text=f\"Media ({np.mean(rewards):.2f})\", row=1, col=1)\n",
    "\n",
    "# 2. Steps por episodio\n",
    "fig.add_trace(\n",
    "    go.Bar(x=episodes, y=steps, name='Steps', marker_color='orange',\n",
    "           showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Distribución (histograma)\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rewards, nbinsx=8, name='Distribución',\n",
    "                 marker_color='purple', showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Tabla de estadísticas (SOLUCIÓN)\n",
    "fig.add_trace(\n",
    "    go.Table(\n",
    "        header=dict(\n",
    "            values=['<b>Métrica</b>', '<b>Valor</b>'],\n",
    "            fill_color='lightblue',\n",
    "            align='left',\n",
    "            font=dict(size=14, color='black')\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                ['Episodios evaluados', 'Recompensa media', 'Recompensa máxima', \n",
    "                 'Recompensa mínima', 'Desviación estándar', 'Mediana',\n",
    "                 'Steps promedio', '', 'Episodios >20 pts', 'Tasa de éxito', \n",
    "                 '', '<b>Objetivo (μ>20)</b>'],\n",
    "                [f'{len(rewards)}', f'{np.mean(rewards):.2f}', f'{np.max(rewards):.2f}',\n",
    "                 f'{np.min(rewards):.2f}', f'{np.std(rewards):.2f}', f'{np.median(rewards):.2f}',\n",
    "                 f'{np.mean(steps):.0f}', '', f'{episodios_exitosos}/{len(rewards)}', \n",
    "                 f'{tasa_exito:.0f}%', '',\n",
    "                 f'<b>{\"✅ ALCANZADO\" if np.mean(rewards) > 20 else \"❌ NO ALCANZADO\"}</b>']\n",
    "            ],\n",
    "            fill_color=[['white', 'lightgray']*6],\n",
    "            align='left',\n",
    "            font=dict(size=12),\n",
    "            height=30\n",
    "        )\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Actualizar layouts\n",
    "fig.update_xaxes(title_text=\"Episodio\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Episodio\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Recompensa\", row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Recompensa\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Steps\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Frecuencia\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, width=1200,\n",
    "    showlegend=False,\n",
    "    title_text=\"<b>Resultados del Test - DQN SpaceInvaders</b>\",\n",
    "    title_font_size=20\n",
    ")\n",
    "\n",
    "# Guardar\n",
    "fig.write_html('test_results.html')\n",
    "fig.write_image('test_results.png', width=1200, height=800)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n✓ Visualización guardada:\")\n",
    "print(\"  - test_results.html (interactivo)\")\n",
    "print(\"  - test_results.png (imagen)\")\n",
    "\n",
    "# Resumen en consola\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANÁLISIS DETALLADO DEL TEST\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n Recompensas por episodio:\")\n",
    "for i, r in enumerate(rewards, 1):\n",
    "    emoji = \"🟢\" if r > 20 else \"🟡\" if r > 10 else \"🔴\"\n",
    "    print(f\"  {emoji} Episodio {i:2d}: {r:5.2f} puntos\")\n",
    "\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\"Estadísticas agregadas:\")\n",
    "print(f\"{'─'*60}\")\n",
    "print(f\"  Recompensa media:      {np.mean(rewards):6.2f}\")\n",
    "print(f\"  Recompensa mediana:    {np.median(rewards):6.2f}\")\n",
    "print(f\"  Recompensa máxima:     {np.max(rewards):6.2f}\")\n",
    "print(f\"  Recompensa mínima:     {np.min(rewards):6.2f}\")\n",
    "print(f\"  Desviación estándar:   {np.std(rewards):6.2f}\")\n",
    "print(f\"  Steps promedio:        {np.mean(steps):6.0f}\")\n",
    "\n",
    "print(f\"\\n{'─'*60}\")\n",
    "print(f\" Evaluación de objetivo:\")\n",
    "print(f\"{'─'*60}\")\n",
    "print(f\"  Episodios >20 puntos:  {episodios_exitosos}/{len(rewards)} ({tasa_exito:.0f}%)\")\n",
    "print(f\"  Media vs objetivo:     {np.mean(rewards):.2f} vs 20.00\")\n",
    "print(f\"  Estado:                {'✅ ALCANZADO' if np.mean(rewards) > 20 else '❌ NO ALCANZADO'}\")\n",
    "\n",
    "if np.mean(rewards) < 20:\n",
    "    deficit = 20 - np.mean(rewards)\n",
    "    print(f\"\\n El modelo necesita mejorar {deficit:.2f} puntos para alcanzar el objetivo\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_results.png\" alt=\"Estadisticas del modelo\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuar Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el modelo con los últimos pesos\n",
    "dqn.load_weights('dqn_SpaceInvaders-v0_weights.h5f')\n",
    "print(\" Pesos cargados\")\n",
    "\n",
    "# 2. Continuar entrenando más steps\n",
    "dqn.fit(\n",
    "    env, \n",
    "    nb_steps=500000,  # 500k steps adicionales\n",
    "    visualize=False, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 3. Los nuevos episodios se añadirán automáticamente al log\n",
    "print(\" Entrenamiento continuado completado!\")\n",
    "\n",
    "# 4. Evaluar\n",
    "test_results = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(f\"Media: {np.mean(test_results.history['episode_reward']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (gym)",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
